{
  "hash": "caf4576be53fd3e226fdbad63b0475fd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Timetable Metrics\" \n---\n\n\n\n\nSo far, we have explored the complexities and challenges of university timetabling, the pros and cons of graph databases and investigated different graph data models.  We have also built and implemented a data engineering solution which extracts, transforms and loads data from SQL RDMS to a graph database.  \n\nIn this section, we delve deeper into the concept of timetable quality metrics and explore how graph databases can help us quantify and measure the quality of timetables. \n\n\n### Defining Timetable Quality\n\nAs discussed in the introduction, the inherent complexity of timetabling, with its competing objectives and subjective evaluations, makes it difficult to objectively assess the quality of a timetable. There is no universally agreed-upon definition of a \"good\" timetable, as it is often a balancing act between satisfying hard constraints (e.g. avoiding clashes) and optimising for softer constraints (e.g. minimising travel time).\n\nSo how can we move beyond anecdotal evidence and subjective opinions to a more data-driven understanding of timetable quality?\n\nBy leveraging the power of graph databases, I propose the development of a *timetable quality index*. \n\n### Towards a Quantifiable Measure\n\nA *Timetable Quality Index* is a quantifiable and measurable score reflecting the overall \"goodness\" of a timetable, both at individual and aggregate levels. This score would be based on a flexible and adaptable system of penalties and rewards tied to specific metrics, allowing institutions to tailor the index to their unique needs and priorities.\n\nSome key use cases for the timetable quality index include:\n\n#### Benchmarking and comparison\n\nThe index allows institutions to compare timetable quality across different programmess, departments, or even years, facilitating the identification of best practices and areas for improvement.\n\n#### Resource optimisation \n\nInsights from the index can help institutions allocate resources, such as lecture rooms and teaching staff, more effectively by identifying underutilised or overbooked facilities.\n\n#### Student experience enhancement\n\nBy prioritising metrics related to student well-being, such as travel time and consecutive teaching hours, institutions can enhance the overall student experience and satisfaction.\n\n#### Data-driven decision making\n\nHistorical timetable quality data can inform future planning and course scheduling, allowing institutions to anticipate and address potential issues proactively.\n\n#### Stakeholder communication\n\nThe timetable quality index can serve as a transparent and data-driven tool for communicating the performance and challenges of the timetabling process to various stakeholders, including faculty, students, and administrative staff.\n\n### Implemented Metrics\n\nThe foundation of this quality index lies in defining and calculating specific metrics that capture various aspects of timetable quality. \n\nHere are some examples of how this could work:\n\n#### Constraint or preference violations\n\nIndividual timetables have certain qualities which can be measured - the shape and feel of the timetable.  Often, hese can be summarised into rules or constraints which can be measured - either desirable qualities to strive for or undesirable qualities to avoid.  They can be 'hard' - must not be violated, or 'soft' - *should* not be violated. \n\nThe presence or absence of these qualities on an individual's timetable can be measured and scored in the form of a reward or penalty.\n\nExamples include:\n\n* maximum hours per day, e.g. no more than 6 hours of teaching per day\n* maximum consecutive hours, e.g. no more than 3 hours of teaching without a break\n* minimum hours per day, e.g. at least 2 hours of teaching per day\n* no lunch break, e.g. must have a break between 12-2pm\n* minimal idle time, e.g. no more than a 4 hour gap between activities\n* preferred timeblocks, e.g. bonus points for activities scheduled in the morning\n\n#### Distance-based metrics\n\nBy incorporating room location data, we can calculate travel distances and times between activities. Long travel times or back-to-back activities in distant locations would incur penalties.\n\n#### Resource Utilisation \n\nMetrics related to room utilisation, such as occupancy rates and frequency of use, can provide insights into the efficient allocation of space. Low utilisation rates could be penalised, encouraging more effective use of resources.\n\n#### Activity Characteristics\n\nFactors like activity clashes, oversubscription rates, and room suitability (size, type) can impact student experience. Penalties can be assigned based on the severity of these issues.  \n\nThe flexibility of the graph database allows for experimentation in terms of which metrics are most relevant, how they should be calculated, where they should be stored, and how they should be weighted in the overall quality score.\n\n\n### Aggregation Methods\n\nWith the individual metrics calculated, the next step is to aggregate these into meaningful scores at different levels.  This could be at the student level, programme level, department level, or even at the room level.  \n\nThe metrics used and their weightings will depend on the use-case and the priorities of the institution.  For example, a student-level score could be used to identify students with particularly poor timetables, while a programme-level score could be used to compare the quality of timetables across different programmes, and a room-level score could be used to identify rooms that are underutilised or overbooked or are otherwise unsuitable.\n\nThis allows for a more nuanced understanding of timetable quality and can help identify areas for improvement.\n\n#### Student-level \n\nEach student node can have a quality score reflecting their individual timetable experience based on assigned activities and associated penalties.\n\n#### Programme-level \n\nBy aggregating student scores within a programme, we gain insights into the overall quality experienced by students in that programme.\n\n#### Other groupings\n Scores can be aggregated at various levels, such as by department, room type, or time slot, to identify potential areas for improvement.\n\n### Cypher Queries for Metric Calculation\n\nPrototype queries have been identified to identify constraint violations.  Several of these queries are quite complex but their final form will dependent on the use-case as well as the graph data model.  \n\nAs a simple example, the following query identifies students with back-to-back activities in different buildings, highlighting a potential travel time issue:\n\n\n\n\n```{cypher}\n// Identify students with back-to-back activities in different buildings\nMATCH (s:Student)-[:ATTENDS]->(a1:Activity)-[:NEXT]->(a2:Activity)\nWHERE a1.endTime = a2.startTime AND a1.building <> a2.building\nRETURN s.name, a1.name, a2.name, a1.building, a2.building\n```\n\n\n\n\n![Travel time between activities](./images/cypher-b2b-travel.png)\n\n\n\nSee  [Cypher Queries - Hard Constraints](appendix-cypher5.qmd) and [Cypher Queries - Soft Constraints](appendix-cypher6.qmd) for more examples. \n\n\n### Penalty and Reward System\n\nOne way of implementing this is to store the quality score as a property on the relevant node (student, programme, room, etc.).  Starting with a baseline score, the quality score is dynamically updated by subtracting penalties and adding rewards based on the specific metrics calculated. The weighting of these penalties and rewards can be adjusted to reflect institutional priorities. \n\nUsing the back-to-back activities example above, we can imagine using either `distance` or `walkingTimeSeconds` and a sliding scale to calculate a penalty.  For example, if the walking time is greater than 5 minutes, a penalty of -3 points could be applied to that student's timetable quality score.\n\nFurther examples: \n\n**No lunch break**: -5 points\n\n**Back-to-back activities 5+ minutes apart**: -3 points per instance\n\n**Activity clash**: -10 points\n\n**Room at full capacity**: -2 points\n\n**High room utilisation rate:** +2 points\n\n### Visualisation of Results\n\nThe calculated scores and underlying metrics can be effectively visualised using various techniques:\n\n**Bloom visualisations in Neo4j**: These can provide an intuitive overview of timetable quality across different programmes, time slots, or other groupings.  They enable users to explore hierarchical relationships, identify patterns and outliers, and drill down into specific data points.\n\n**Charts and dashboards**: Bar charts, line graphs, and heatmaps can be used to display and compare scores, identify trends, and track changes over time.  Interactive dashboards can be built to provide a various views of timetable quality metrics and enable stakeholders to explore the data, identify trends, and make informed decisions.\n\n### Potential Challenges\n\nWhile the concept of a timetable quality index offers many benefits, there are several challenges to acknowledge:\n\n**Data quality and availability**: As with any analysis, accuracy and completeness of timetable data is crucial to calculate reliable quality scores. Inconsistent or missing data can lead to inaccurate results and skewed conclusions.\n\n**Complexity of metrics**: Defining and calculating meaningful metrics that capture the nuances of timetable quality can be challenging and time-consuming.\n\n**Metric Definition and Weighting**: Ironically, the very attempt to quantify quality is based on subjective judgements on which metrics to include, how to calculate them and how to weight them.\n\n### Benefits and Future Development\n\nA timetable quality index is a potentially a powerful mechanism which can help universities gain a more quantifiable and data-driven understanding of their timetabling function.  \n\nThe flexibility of graph allows for rapid prototyping, experimentation and deployment of new metrics and scoring systems.  This can help institutions to identify areas for improvement, allocate resources more effectively, and enhance the overall student experience.\n\nFuture developments could include:\n\n* Identifying additional metrics that capture the quality of timetables more comprehensively.\n* Refining the weighting system for penalties and rewards based on stakeholder feedback and institutional priorities.\n* Incorporating additional datasets, such as student preferences or transportation schedules, to enhance the accuracy and granularity of the quality index.\n* Incorporating additional constraints and preferences, such as room suitability, staff availability, student preferences and any reasonable adjustments.\n* Developing interactive dashboards that allow users to explore timetable data, simulate changes, and assess their impact on the quality score.\n\n",
    "supporting": [
      "04-timetable-metrics_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}