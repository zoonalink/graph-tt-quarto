---
title: Background and Motivation
---

::: callout-warning
## TODO

-   review - reduce?
:::

My journey into the world of university timetabling began several years ago, first as a scheduler grappling with the complexities of timetable generation, and later transitioning into the role of a timetabling data manager. These experiences exposed me not only to the intricate data and systems involved but also to the intense pressures faced by timetabling teams. The constant scrutiny, stakeholder demands, and the near-impossibility of achieving universal satisfaction left an indelible mark, highlighting the critical need for better tools and metrics to understand and assess timetable quality – a factor often overshadowed by the pursuit of mere feasibility.

## Research Gap: Bridging Theory and Practice

Much of the current research in university course timetabling (UCTTP) centres on optimisation algorithms, sophisticated techniques designed to efficiently generate feasible solutions given a set of constraints. This computationally-driven approach stems from the inherent difficulty of the UCTTP, often categorised as NP-hard[^1], meaning finding the absolute "best" timetable is exceptionally challenging (Babaei, Karimpour and Hadidi, 2015; Herres and Schmitz, 2021; Wikipedia contributors, 2024). Consequently, significant effort has been dedicated to developing algorithms like constraint programming (Holm et al., 2022) and local search techniques such as Tabu Search and simulated annealing (Oude Vrielink et al., 2019), aiming to create workable timetables within reasonable timeframes.

However, this emphasis on computational optimisation often unfolds within the controlled environment of standardised datasets and predefined constraints. While crucial for advancing algorithmic development, these idealised scenarios may not fully capture the dynamic complexity of real-world university timetabling. Universities grapple with constantly shifting demands: fluctuating student popoulations, evolving school preferences, resource limitations, and the ever-present need to balance diverse stakeholder needs. These complexities extend beyond simply finding a feasible solution – they necessitate tools to understand the trade-offs inherent in any timetable, enabling informed decisions about which "good" to prioritise (Lindahl, 2017). This is where I believe graph data structures offer unique potential. Timetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Graph databases excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable.

While algorithms excel at generating solutions, there remains a significant gap in post-generation analysis – the ability to delve into a timetable's nuanced impacts on student and staff experience. Despite the acknowledged importance of factors like room allocation, teaching period distribution, and their effect on overall timetable quality, traditional optimisation-focused approaches often lack the tools to explore these relationships in depth (Ceschia, Di Gaspero and Schaerf, 2023; Lindahl, 2017; Rudová, Müller and Murray, 2011). 

## Motivation

**This is where I believe graph data structures offer unique potential.** Timetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Traditional data analysis methods often struggle to capture the richness of these interconnected elements. Graph databases, on the other hand, are specifically designed to excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable. This potential for deeper analysis, coupled with the limitations of optimisation-centric approaches, motivates this exploration of graph data structures for enhancing timetable understanding and, ultimately, improving timetable quality for all stakeholders.

[^1]: "In [computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory "Computational complexity theory"), a computational problem *H* is called **NP-hard** if, for every problem *L* which can be solved in [non-deterministic polynomial-time](https://en.wikipedia.org/wiki/NP_(complexity) "NP (complexity)"), there is a [polynomial-time reduction](https://en.wikipedia.org/wiki/Polynomial_time_reduction "Polynomial time reduction") from *L* to *H*." (Wikipedia contributors, 2024)

