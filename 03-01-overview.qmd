---
title: "Data Engineering Overview"
---

A main objective of my project is the development of a data pipeline which efficiently and securely transfers selected university timetabling data from a relational database ([MS SQL](https://www.microsoft.com/en-us/sql-server/sql-server-2019)) to a graph database ([Neo4j](https://neo4j.com/)).

This section provides an overview of the pipeline architecture, fundamental design principles, implementation approach and key learning takeaways.

## High-level Architecture

The data pipeline consists of these core stages:

1. **Extraction**: Data is extracted from the SQL database and saved into CSV files.
2. **Transformation**: CSV files are processed, cleaned, transformed, merged, and anonymised using Python.
3. **Intermediate Storage**: Processed CSVs are uploaded to Google Drive (required for [Neo4j Aura](https://neo4j.com/cloud/platform/aura-graph-database/) free instance).
4. **Loading**: Clean data is processed and loaded into Neo4j.

:::{.figure #fig-pipeline fig-align="center"}
```{dot}
#| fig-cap: "Data Pipeline Overview" 

digraph Overview {

  // graph layout - td
  rankdir = TD;
  nodesep = 1.0; 
  node [shape=box, style="filled,rounded", fillcolor="#f0f0f0", fontname="Arial"]; 
  edge [fontname="Arial"]; 

  // Nodes 
  A [label="SQL Database", shape=cylinder, fillcolor="#ffc0cb"];  //  pink 
  B [label="CSV Files"];
  C [label="Processed CSV Files"];
  
  // subgraph for vertical positioning
  subgraph cluster_D_E { 
      style=invis;  //  subgraph invisible
      D [label="Google Drive", shape=folder]; 
      E [label="Neo4j Aura DB", style="filled", fillcolor="#e0f0e0", shape=cylinder];
      
      // invisible edge to force positioning
      D -> E [style=invis]; 
  }

  // Edges and labels 
  A -> B [label=" 1. ùóòùó´ùóßùó•ùóîùóñùóß: Filter"];
  B -> C [label=" 2. ùóßùó•ùóîùó°ùó¶ùóôùó¢ùó•ùó†: Validate, Process & Anonymise"];
  C -> D [label=" 3. ùêîùêèùêãùêéùêÄùêÉ"];
  
  // arrows from Google Drive (left and right)
  D:w -> E [label=" 4. (Optional) Load Schema", style=dashed];   
  D:e-> E [label=" 5. ùóüùó¢ùóîùóó: Load & Validate Data"]; 
}
```
<figcaption>Data Pipeline Overview</figcaption>
:::


## Design Principles

Several "best practices" in data handling, processing, and database management were incorporated in developing this ETL. The data pipeline is built on several core design principles: 

![Design Principles](./images/design-principles-graph.png){.small-image}

I started with a strong sense of what I wanted to build - a modular, scalable, secure and configurable design - however, what *exactly* this meant was only discovered during the development process.  

Given project constraints - deadline, word-limits, resources, data, technology - it is fair to say that compromises were made.  That said, it was important that the final artefact is one that can be developed further for specific business use-cases. 

### Security and Data Protection

![](./images/designSecure.png){.smaller-image fig-alt="Security, Access, Anonymisation"}

* Secure access controls
* [Data anonymisation](appendix-anonymise.qmd)
* Controlled handling of personally identifiable information

### Modularity, Scalability and Automation

![](./images/designModular.png){.smaller-image fig-alt="Modularity, Scalability, Validity, Automation"}

* Distinct, interoperable modules (extract, process, upload, load)
* Ability to handle increased data volume and complexity
* Automation, where possible
* Configurable data processing options (e.g., data chunking, row processing)
* Optimised, where possible

### Error Handling and Logging

![](./images/designLog.png){.smaller-image fig-alt="Logging and Error Handling"}

* Robust error handling
* Comprehensive logging for troubleshooting and auditing

### User configurable

![](./images/designConfig.png){.smaller-image fig-alt="Configurability"}

* Flexible configuration options for data filtering, directory controls, and schema handling

## Implementation Approach

The pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement. 

Crucial aspects of the implementation include:

* **Technology Stack**: Python for data processing, MS SQL for source data, Neo4j for the target graph database.  See [Technology Stack](appendix-tech-stack.qmd) for more details.
* **Cloud Integration**: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.
* **Validation**: Implemented at various stages to ensure data integrity and fitness for processing.
* **Testing**: Continuous simulated unit testing to ensure that components are behaving as expected.

## Upcoming Sections

The following sections will delve into specific implementation details of each stage, demonstrating how these principles are put into practice, before reflecting on lessons learned and potential future enhancements.
