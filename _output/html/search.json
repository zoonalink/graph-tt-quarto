[
  {
    "objectID": "appendix-cypher6.html",
    "href": "appendix-cypher6.html",
    "title": "Soft Constraints",
    "section": "",
    "text": "Soft constraints in a timetabling context are (strong) preferences. They should be generally met and only violated when absolutely necessary, although there is an argument for a sliding scale of soft constraint adherence.\nFor example, a member of staff may be unavailable on Fridays, generally, but at a push can be available. Other examples might include ensuring that students have an opportunity to eat lunch by ensuring at least 30 minutes free time between 12:00-14:00 or minimising travel between activities..\nThis page contains cypher queries that can be used to identify where a timetabling soft constraint has been violated.\nExample soft constraints include:\n\nMinimal Idle Time (aka no large gaps): Minimise gaps in staff and student schedules (within reason).\nSpread Activities (aka maximum consecutive hours): Avoid clumping all activities for a student or staff member on one day.\nPreferred Times: Consider staff and student preferences for morning, afternoon, or evening classes\nTravel Time: Minimise the time students need to travel between consecutive classes (especially on large campuses), e.g.¬†between building blocks or by lat/long\nLunch Breaks: Ensure students have sufficient time for lunch breaks.\n\n\nMinimal idle time\nIdentifying time gaps between scheduled activities is very complex and requires several steps, clauses and comprehensions within a single query:\n\ngrouping and sorting - activities are grouped by student and date, and sorted within groups to establish the sequence\ngap calculation - time difference between the end of one activity and the start of the next is calculated for consecutive pairs of activities within a day\nfiltering and aggregation - gaps are filtered based on threshold (e.g.¬†6 hours) and then the maximum gap for each day is identified\ndata restructuring - output is restructured.\n\n\nCypher logic for identifying gaps\nThe below is the logic for identifying gaps between activities, using an example student:\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWHERE s.stuID_anon = \"stu-10085720\"\nAND a.actStartDate = date(\"2022-10-03\")\nWITH s, a\nORDER BY a.actStartTime\n\n// Collecting the start and end times of the activities\nWITH s, collect({start: a.actStartTime, end: a.actEndTime}) AS times\n\n// Calculating the gaps in minutes between consecutive activities\nWITH s, times, \n     [i IN range(0, size(times)-2) | \n      duration.between(times[i].end, times[i+1].start).minutes / 60.0] AS gaps\n\n// Finding the maximum gap\nRETURN s.stuID_anon AS student, times, gaps, reduce(maxGap = 0.0, gap IN gaps | CASE WHEN gap &gt; maxGap THEN gap ELSE maxGap END) AS maxGap\n\n\n\nExample student with gaps between activities\n\n\n\n\nPython code on graph\nThe below code cell returns the first 5 rows where a 6 hour maximum gap has been violated.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\n// students with gaps between activities\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a\nORDER BY s.stuFirstName_anon, a.actStartDate, a.actStartTime\n// Group activities by student and date\nWITH s, a.actStartDate AS date, collect({start: a.actStartTime, end: a.actEndTime, activity: a}) AS times\n// calculating the gaps in hours between consecutive activities\nWITH s, date, times, \n     [i IN range(0, size(times)-2) | \n      {gap: duration.between(times[i].end, times[i+1].start).minutes / 60.0, \n       firstActivity: times[i].activity, \n       secondActivity: times[i+1].activity}] AS gaps\n// filtering gaps based on a threshold of 6 hours\nWITH s, date, gaps\nWHERE any(gapRecord IN gaps WHERE gapRecord.gap &gt; 6.0)\n// Finding the maximum gap that exceeds the threshold\nWITH s, date, reduce(maxGap = {gap: 0.0, firstActivity: null, secondActivity: null}, gapRecord IN gaps | \n    CASE WHEN gapRecord.gap &gt; maxGap.gap THEN gapRecord ELSE maxGap END) AS maxGapRecord\n// group by student to remove duplications\nWITH s.stuID_anon AS student, \n     collect({date: date, \n              activity1: maxGapRecord.firstActivity.actName,\n              activity1_time: maxGapRecord.firstActivity.actStartTime + \"-\" + maxGapRecord.firstActivity.actEndTime,\n              activity2: maxGapRecord.secondActivity.actName,\n              activity2_time: maxGapRecord.secondActivity.actStartTime + \"-\" + maxGapRecord.secondActivity.actEndTime,\n              maxGapInHours: maxGapRecord.gap}) AS gapRecords\n// Unwind the collected records\nUNWIND gapRecords AS record\n// Returning the result\nRETURN student, \n       record.date AS date,\n       record.activity1 AS activity1, \n       record.activity1_time AS activity1_time,\n       record.activity2 AS activity2,\n       record.activity2_time AS activity2_time,\n       record.maxGapInHours AS maxGapInHours\nORDER BY student, date\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"activity1\", \"activity1_time\", \n                                   \"activity2\", \"activity2_time\", \"maxGapInHours\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000002661E8B76D0&gt;\nRunning query...\n\n        student        date                 activity1     activity1_time  \\\n0  stu-10270089  2023-01-30  UFCFGS-15-1 L_oc/01 &lt;29&gt;  09:00:00-10:00:00   \n1  stu-10270089  2023-02-06  UFCFGS-15-1 L_oc/01 &lt;30&gt;  09:00:00-10:00:00   \n2  stu-10270089  2023-02-13  UFCFGS-15-1 L_oc/01 &lt;31&gt;  09:00:00-10:00:00   \n3  stu-10270089  2023-02-20  UFCFGS-15-1 L_oc/01 &lt;32&gt;  09:00:00-10:00:00   \n4  stu-10270089  2023-02-27  UFCFGS-15-1 L_oc/01 &lt;33&gt;  09:00:00-10:00:00   \n\n                   activity2     activity2_time  maxGapInHours  \n0  UFCFES-30-1 L2_oc/01 &lt;29&gt;  17:30:00-19:00:00            7.5  \n1  UFCFES-30-1 L2_oc/01 &lt;30&gt;  17:30:00-19:00:00            7.5  \n2  UFCFES-30-1 L2_oc/01 &lt;31&gt;  17:30:00-19:00:00            7.5  \n3  UFCFES-30-1 L2_oc/01 &lt;32&gt;  17:30:00-19:00:00            7.5  \n4  UFCFES-30-1 L2_oc/01 &lt;33&gt;  17:30:00-19:00:00            7.5  \n\n\n\n\nPython to return total hours and block hours\nAlternatively, we can amend the query to return, for each date and student combination, their total scheduled hours, maximum consecutive block hours and the number of activities within the continuous block.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\n// calculates - total hours, max block hours, max block activities per day \n// to be used for max block hours and max block activities per day\n// logic - example\n\nMATCH (s:student {stuID_anon:\"stu-10085720\"})-[:ATTENDS]-&gt;(a:activity)\nWITH s, a ORDER BY a.actStartDate, a.actStartTime\nWITH s, a.actStartDate AS date, \n     SUM(a.actDurationInMinutes) / 60.0 AS totalHours,\n     REDUCE(\n        blockInfo = [],\n        activity IN COLLECT(a)\n        | CASE\n            WHEN blockInfo = [] THEN [[activity]]\n            ELSE CASE\n                   WHEN head(last(blockInfo)).actEndTime &gt;= activity.actStartTime\n                     THEN blockInfo[..-1] + [last(blockInfo) + activity]\n                   ELSE blockInfo + [[activity]]\n                 END\n          END\n     ) AS blocks\nUNWIND blocks AS block\nWITH s, date, totalHours, blocks,\n     REDUCE(blockHours = 0.0, activity IN block | blockHours + activity.actDurationInMinutes) / 60.0 AS blockHours,\n     SIZE(block) AS blockActivities\nRETURN s.stuFullName_anon AS student, date, totalHours, \n       MAX(blockHours) AS blockHours,\n       MAX(blockActivities) AS blockActivities\nORDER BY date;\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"totalHours\", \"blockHours\", \"blockActivities\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000002661ED434D0&gt;\nRunning query...\n\n       student        date  totalHours  blockHours  blockActivities\n0  Aaron Evans  2022-09-22         1.0         1.0                1\n1  Aaron Evans  2022-09-23         2.0         2.0                1\n2  Aaron Evans  2022-09-27         2.0         2.0                2\n3  Aaron Evans  2022-09-30         4.0         2.0                2\n4  Aaron Evans  2022-10-03         4.5         2.5                2\n\n\n\nExplanation\n\nMatch and Sort Activities:\n\nMATCH (s:student {stuID_anon:‚Äústu-10085720‚Äù})-[:ATTENDS]-&gt;(a:activity): Matches the specified student and all their attended activities.\nWITH s, a ORDER BY a.actStartDate, a.actStartTime: Sorts the activities by their start date and then by their start time within each date.\n\nCalculate Total Hours and Group Activities into Blocks:\n\nWITH s, a.actStartDate AS Date, SUM(a.actDurationInMinutes) / 60.0 AS totalHours, ‚Ä¶: Calculates the total hours spent on activities for each date by summing the durations of all activities on that date and converting minutes to hours.\nREDUCE(blockInfo = [], activity IN COLLECT(a) | ‚Ä¶): Groups activities into blocks based on time overlaps using a REDUCE function and a CASE expression.\n\nInitialises an empty list blockInfo to store the blocks.\nIterates over the collected activities (COLLECT(a)).\nFor each activity:\n\nIf blockInfo is empty (first activity), it creates a new block with the activity.\nOtherwise, it checks if the current activity overlaps with the last activity in the last block of blockInfo.\n\nIf there‚Äôs an overlap, it adds the current activity to the last block.\nIf there‚Äôs no overlap, it creates a new block with the current activity.\n\n\n\n\nCalculate Block Hours and Number of Activities:\n\nUNWIND blocks AS block: Unwinds the list of blocks, processing each block individually.\nWITH s, Date, totalHours, blocks, REDUCE(blockHours = 0.0, activity IN block | blockHours + activity.actDurationInMinutes) / 60.0 AS blockHours, SIZE(block) AS blockActivities: For each block, it calculates the total duration in hours (blockHours) by summing the durations of activities within the block and converting minutes to hours. It also calculates the number of activities in the block (blockActivities).\n\nReturn Aggregated Results:\n\nRETURN s.stuFullName_anon AS Student, Date, totalHours, MAX(blockHours) AS blockHours, MAX(blockActivities) AS blockActivities ORDER BY Date;: Returns the student‚Äôs full name, the date, the total hours for the day, the maximum block hours across all blocks for that day, and the maximum number of activities within a single block for that day. The results are ordered by date.\n\n\n\n\n\nExample Use case - identifying students with 5+ hours in a single block\nThis query returns the first 5 rows where a student has more than 5 consecutive scheduled hours on a date.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)&lt;-[:TEACHES]-(:staff) // Filter for teaching activities\nWITH s, a ORDER BY a.actStartDate, a.actStartTime\nWITH s, a.actStartDate AS date, \n     SUM(a.actDurationInMinutes) / 60.0 AS totalHours,\n     REDUCE(\n         blockInfo = [],\n         activity IN COLLECT(a)\n         | CASE\n             WHEN blockInfo = [] THEN [[activity]]\n             ELSE CASE\n                     WHEN head(last(blockInfo)).actEndTime &gt;= activity.actStartTime\n                         THEN blockInfo[..-1] + [last(blockInfo) + activity]\n                     ELSE blockInfo + [[activity]]\n                 END\n         END\n     ) AS blocks\nUNWIND blocks AS block\nWITH s, date, totalHours, blocks,\n     REDUCE(blockHours = 0.0, activity IN block | blockHours + activity.actDurationInMinutes) / 60.0 AS blockHours,\n     SIZE(block) AS blockActivities\nWHERE blockHours &gt; 5 // Filter for blocks with more than 5 hours\nRETURN s.stuFullName_anon AS student, date, totalHours, \n       blockHours,\n       blockActivities\nORDER BY date;\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"totalHours\", \"blockHours\", \"blockActivities\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000002661E9179D0&gt;\nRunning query...\n\n             student        date  totalHours  blockHours  blockActivities\n0        Jacob Jones  2022-07-19         5.5         5.5                2\n1      Rachael Moore  2022-07-19         5.5         5.5                2\n2        Kayla Sharp  2022-07-19         5.5         5.5                2\n3         David Rose  2022-07-19         5.5         5.5                2\n4  Francisco Holland  2022-07-19         5.5         5.5                2\n\n\n\n\nTotal hours per day\nIn contrast, calculating simple total hours per day is done as follows:\nMATCH (s:student )-[:ATTENDS]-&gt;(a:activity)\nWITH s, a.actStartDate AS Date, SUM(a.actDurationInMinutes) / 60.0 AS totalHours\nRETURN s.stuFullName_anon AS Student, Date, totalHours\nORDER BY Date;\n\n\n\nTotal hours per day\n\n\n\n\nLongest consecutive block of activities per day\nWe can use the earlier cypher logic to identify the longest consecutive block of activities for a student, or student on a day, etc.\n\nMATCH (s:student {stuFullName_anon: \"Susan Lopez\"})-[:ATTENDS]-&gt;(a:activity {actStartDate: date(\"2022-09-27\")})\nWITH s, a \nORDER BY a.actStartTime\nWITH s, COLLECT(a) AS activities\nWITH s, activities,\n     REDUCE(\n       state = {currentBlock: {duration: 0, start: null, end: null}, longestBlock: {duration: 0, start: null, end: null}},\n       activity IN activities |\n         CASE\n           WHEN state.currentBlock.end IS NULL OR \n                activity.actStartTime &gt; state.currentBlock.end\n           THEN {\n             currentBlock: {\n               duration: activity.actDurationInMinutes,\n               start: activity.actStartTime,\n               end: activity.actEndTime\n             },\n             longestBlock: \n               CASE\n                 WHEN activity.actDurationInMinutes &gt; state.longestBlock.duration\n                 THEN {\n                   duration: activity.actDurationInMinutes,\n                   start: activity.actStartTime,\n                   end: activity.actEndTime\n                 }\n                 ELSE state.longestBlock\n               END\n           }\n           ELSE {\n             currentBlock: {\n               duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                         (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n               start: state.currentBlock.start,\n               end: activity.actEndTime\n             },\n             longestBlock: \n               CASE\n                 WHEN ((activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                       (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute)) &gt; state.longestBlock.duration\n                 THEN {\n                   duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                             (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n                   start: state.currentBlock.start,\n                   end: activity.actEndTime\n                 }\n                 ELSE state.longestBlock\n               END\n           }\n         END\n     ) AS finalState\nRETURN\n  s.stuFullName_anon AS stuName,\n  activities[0].actStartDate AS date,\n  finalState.longestBlock.duration AS longestConsecutiveBlockDuration,\n  finalState.longestBlock.start AS blockStartTime,\n  finalState.longestBlock.end AS blockEndTime\n\n\nExample - longest consecutive block for ‚ÄòSusan Lopez‚Äô on 2022-09-27\nThe below finds the longest consecutive block in a day for a student:\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\nMATCH (s:student {stuFullName_anon: \"Susan Lopez\"})-[:ATTENDS]-&gt;(a:activity{actStartDate: date(\"2022-09-27\")})\nWITH s, a \nORDER BY a.actStartTime\nWITH s, COLLECT(a) AS activities\nWITH s, activities,\n   REDUCE(\n     state = {currentBlock: {duration: 0, start: null, end: null}, longestBlock: {duration: 0, start: null, end: null}},\n     activity IN activities |\n       CASE\n         WHEN state.currentBlock.end IS NULL OR \n              activity.actStartTime &gt; state.currentBlock.end\n         THEN {\n           currentBlock: {\n             duration: activity.actDurationInMinutes,\n             start: activity.actStartTime,\n             end: activity.actEndTime\n           },\n           longestBlock: \n             CASE\n               WHEN activity.actDurationInMinutes &gt; state.longestBlock.duration\n               THEN {\n                 duration: activity.actDurationInMinutes,\n                 start: activity.actStartTime,\n                 end: activity.actEndTime\n               }\n               ELSE state.longestBlock\n             END\n         }\n         ELSE {\n           currentBlock: {\n             duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                       (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n             start: state.currentBlock.start,\n             end: activity.actEndTime\n           },\n           longestBlock: \n             CASE\n               WHEN ((activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                     (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute)) &gt; state.longestBlock.duration\n               THEN {\n                 duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                           (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n                 start: state.currentBlock.start,\n                 end: activity.actEndTime\n               }\n               ELSE state.longestBlock\n             END\n         }\n       END\n   ) AS finalState\nRETURN\ns.stuFullName_anon AS student,\nactivities[0].actStartDate AS date,\nfinalState.longestBlock.duration AS longestConsecutiveBlockDuration,\nfinalState.longestBlock.start AS blockStartTime,\nfinalState.longestBlock.end AS blockEndTime\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n  records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"longestConsecutiveBlockDuration\", \"blockStartTime\", \"blockEndTime\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000002661E942FD0&gt;\nRunning query...\n\n       student        date  longestConsecutiveBlockDuration  \\\n0  Susan Lopez  2022-09-27                              300   \n\n       blockStartTime        blockEndTime  \n0  13:00:00.000000000  18:00:00.000000000  \n\n\n\n\n\nMax hours in a day\nThis query calculates the total scheduled hours for each student on a day and returns the results ordered by date. This examples filters for students who have more than 7 hours of activities in a day.\n// sum of activity durations\n// does not account for simultaneous activities (clashes) - so could be inflated, e.g. 12.5 hour students\n\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a.actStartDate AS Date, SUM(a.actDurationInMinutes) / 60.0 AS totalHours\nWHERE totalHours &gt; 7 // Set  maximum here\nRETURN s.stuFullName_anon AS Student, Date, totalHours\nORDER BY Date;\n\n\n\nMax hours in a day\n\n\n\n\nLunch breaks\nIt might be expected that a student (or staff) has a lunch break. The cypher below calculates free and booked tie within a window, in this case 12:00 and 14:00. It can be used to find students who do not have a lunch break or count the number of days that a student does not have a lunch break.\n\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a\nORDER BY a.actStartDate, a.actStartTime\nWITH s, COLLECT(a) AS activities\nUNWIND activities AS activity\nWITH s.stuFullName_anon AS Student, activity.actStartDate AS Date, time(activity.actStartTime) AS StartTime, time(activity.actEndTime) AS EndTime, duration.between(time('12:00'), time('14:00')).minutes AS BreakWindow_12_14\nWITH Student, Date, BreakWindow_12_14, COLLECT([StartTime, EndTime]) AS Activities\nUNWIND Activities AS activity\nWITH Student, Date, BreakWindow_12_14, activity[0] AS StartTime, activity[1] AS EndTime\nWITH Student, Date, BreakWindow_12_14,\n     CASE\n       WHEN StartTime &gt;= time('14:00') OR EndTime &lt;= time('12:00') THEN 0\n       WHEN StartTime &lt; time('12:00') AND EndTime &gt; time('14:00') THEN BreakWindow_12_14\n       WHEN StartTime &gt;= time('12:00') AND StartTime &lt; time('14:00') THEN duration.between(StartTime, time('14:00')).minutes\n       WHEN EndTime &gt; time('12:00') AND EndTime &lt;= time('14:00') THEN duration.between(time('12:00'), EndTime).minutes\n     END AS BookedDurationMinutes\nRETURN Student, Date, BreakWindow_12_14, BreakWindow_12_14 - SUM(BookedDurationMinutes) AS FreeTimeMinutes, SUM(BookedDurationMinutes) AS BookedTimeMinutes\nORDER BY Date\n\n\n\nLunch breaks\n\n\nInterestingly, Michael Johnson has a negative lunch break! A quick look showed that there are actually two Michael Johnsons attending this class and they both have 30 minutes free time in the 2-hour lunch break window.\nBecause the query was written using student name, it is incorrectly aggregating the two students into one person as follows:\n\\[\n2 \\text{ hour lunch window} - (1.5 \\text{ hours/class} \\times 2 \\text{ students}) = -1 \\text{ hour}\n\\]\nTo remedy this, the query can be updated to use student ID or a different unique identifier. I would also like to update the anonymisation function in the ETL so that it does not duplicate names in the output.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Soft Constraints"
    ]
  },
  {
    "objectID": "03-04-extract.html",
    "href": "03-04-extract.html",
    "title": "Extraction",
    "section": "",
    "text": "extract\n\n\n\nA\n\n\nSQL Database\n(students, staff, programmes,\nactivities, rooms, etc.)\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nA-&gt;keyring\n\n\nRequires\n\n\n\nB\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nextract\n\nùóòùó´ùóßùó•ùóîùóñùóß\n\n\n\nextract-&gt;B\n\n\n\n\n\nkeyring-&gt;extract\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;extract\n\n\nSQL Scripts\n\n\n\nconfig-&gt;keyring\n\n\nCredentials\n\n\n\n\n\n\n\n\n\nExtract\n\n\nEXTRACT starts by securely connecting to the specified SQL database using encrypted credentials stored with keyring. The combination of configuration and SQL scripts determine which data will be extracted by filtering based on programme(s) of study and specifying which nodes, relationships and properties to extract. Additional options include specifying chunk size if extracting signficicant amounts of data, for example.\nThe process performs basic validation at every step ensuring secure connection before running SQL SELECT statements and storing extracted data as local csv files.\n\nSQL example\nSELECT DISTINCT a.[Id] AS actSplusID,\n     CONCAT(a.[Id], '-', adt.[Week], '-', adt.[Day]) AS actGraphID,\n     a.[Name] AS actName,\n     a.[Description] AS actDescription,\n     a.[DepartmentId] AS actDeptSPlusID,\n     adt.[StartDateTime] AS actStartDateTime,\n     adt.[EndDateTime] AS actEndDateTime,\n     adt.[Week] AS actWeekNum,\n     adt.[Occurrence] AS actOccurrence,\n     a.[ModuleId] AS actModSplusID,\n     a.[ScheduledDay] AS actScheduledDay,\n     a.[StartDate] AS actFirstActivityDate,\n     a.[EndDate] AS actLastActivityDate,\n     a.[PlannedSize] AS actPlannedSize,\n     a.[RealSize] AS actRealSize,\n     a.[Duration] AS actDuration,\n     a.[DurationInMinutes] AS actDurationInMinutes,\n     a.[NumberOfOccurrences] AS actNumberOfOccurrences,\n     a.[WeekPattern] AS actWeekPattern,\n     a.[ActivityTypeId] AS actActivityTypeSplusID,\n     a.[WhenScheduled] AS actWhenScheduled,\n     a.[IsJtaParent],\n     a.[IsJtaChild],\n     a.[IsVariantParent],\n     a.[IsVariantChild]\nFROM ##TempActivity a\nINNER JOIN ##TempActivityDateTime adt ON a.[Id] = adt.[ActivityID];\n\n\nSnippet: extract_data.py\n#| eval: false\n# extract_main.py\nfrom logger_config import extract_logger\nfrom extract_data import main as extract_main\nfrom config import EXTRACT_DIR, HOSTKEYS, CHUNK_SIZE\nfrom utils import execution_times\n\ndef run_extraction():\n    extract_logger.info(\"Starting data extraction process\")\n    extract_logger.info(f\"Output Directory: {EXTRACT_DIR}\")\n    extract_logger.info(f\"Hostkeys: {HOSTKEYS}\")\n    extract_logger.info(f\"Chunksize: {CHUNK_SIZE}\")\n\n    try:\n        extract_main()\n    except Exception as e:\n        extract_logger.exception(\"An error occurred during data extraction:\")\n    finally:\n        extract_logger.info(\"Data extraction completed.\")\n\n   \n    # Log the execution times\n    extract_logger.info(\"Extraction Time Summary:\")\n    for func_name, exec_time in execution_times.items():\n        extract_logger.info(f\"Function {func_name} took {exec_time:.2f} seconds\")\n\n\nif __name__ == \"__main__\":\n    run_extraction()",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Extract"
    ]
  },
  {
    "objectID": "02-02b-early.html",
    "href": "02-02b-early.html",
    "title": "Early Insights",
    "section": "",
    "text": "Even with this basic model, we can easily extract valuable insights, for example:\n\nActivity Load: Identify staff with the highest number of teaching activities or total teaching hours.\nStudent Timetable Profiles: Calculate average hours per student or per programme to understand workload distribution.\nResource utilisation: Determine the busiest teaching locations or times on campus.\nAnomaly detection: Identify students who have unexpected profiles or unusual combinations.\n\n\n\nBusiest locations overall\n{cypher .scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WITH r, sum(a.actDuration)/60 AS totalDurationInHours RETURN r.roomName AS Room, r.roomCapacity AS Capacity, r.roomType AS Type, totalDurationInHours ORDER BY totalDurationInHours DESC LIMIT 3\n\n\n\nRoom\nCapacity\nType\ntotalDurationInHours\n\n\n\n\n2Q12 FR\n25\nPC LAB\n21\n\n\n4Q69 FR\n36\nPC LAB\n19\n\n\n3E11 FR\n48\nTEACHING\n18\n\n\n\nBusiest location for a specific time\nMATCH (r:room)&lt;-[:OCCUPIES]-(a:activity)\nWHERE a.actStartTime = localtime({hour:9, minute:0, second:0}) \nWITH r, count(a) AS Count, a.actStartTime AS StartTime\nRETURN r.roomName AS Room, Count, StartTime\nORDER BY Count DESC\nLIMIT 3\n\n\n\nRoom\nCount\nStartTime\n\n\n\n\n2Q12 FR\n86\n09:00:00\n\n\n3E28 FR\n50\n09:00:00\n\n\n3E11 FR\n49\n09:00:00\n\n\n\nStudents with below/above average hours\nThis query returns students and whether they have more or less scheduled time on their timetable compared to the programme cohort average. The cut-off is based on average +/- 10% but can also be written using standard deviation.\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s.stuProgName AS progName, s.stuID_anon AS studentID, SUM(a.actDurationInMinutes) AS studentTotalDuration\nWITH progName, \n     AVG(studentTotalDuration) / 60 AS progAverageHoursPerStudent, \n     collect({studentID: studentID, studentTotalHours: studentTotalDuration / 60}) AS studentsData\nUNWIND studentsData AS studentData\nRETURN progName, \n       progAverageHoursPerStudent,\n       studentData.studentID AS studentID, \n       studentData.studentTotalHours AS studentTotalHours, \n       CASE \n           WHEN studentData.studentTotalHours &lt; (progAverageHoursPerStudent * 0.9) THEN 'Below Average'\n           WHEN studentData.studentTotalHours &gt; (progAverageHoursPerStudent * 1.1) THEN 'Above Average'\n           ELSE 'Average'\n       END AS compare\n\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-23442558‚Äù\n361\n‚ÄúAbove Average‚Äù\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-91911371‚Äù\n126\n‚ÄúBelow Average‚Äù\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-75224499‚Äù\n251\n‚ÄúAverage‚Äù",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights"
    ]
  },
  {
    "objectID": "02-02b-early.html#unveiling-basic-patterns",
    "href": "02-02b-early.html#unveiling-basic-patterns",
    "title": "Early Insights",
    "section": "",
    "text": "Even with this basic model, we can easily extract valuable insights, for example:\n\nActivity Load: Identify staff with the highest number of teaching activities or total teaching hours.\nStudent Timetable Profiles: Calculate average hours per student or per programme to understand workload distribution.\nResource utilisation: Determine the busiest teaching locations or times on campus.\nAnomaly detection: Identify students who have unexpected profiles or unusual combinations.\n\n\n\nBusiest locations overall\n{cypher .scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WITH r, sum(a.actDuration)/60 AS totalDurationInHours RETURN r.roomName AS Room, r.roomCapacity AS Capacity, r.roomType AS Type, totalDurationInHours ORDER BY totalDurationInHours DESC LIMIT 3\n\n\n\nRoom\nCapacity\nType\ntotalDurationInHours\n\n\n\n\n2Q12 FR\n25\nPC LAB\n21\n\n\n4Q69 FR\n36\nPC LAB\n19\n\n\n3E11 FR\n48\nTEACHING\n18\n\n\n\nBusiest location for a specific time\nMATCH (r:room)&lt;-[:OCCUPIES]-(a:activity)\nWHERE a.actStartTime = localtime({hour:9, minute:0, second:0}) \nWITH r, count(a) AS Count, a.actStartTime AS StartTime\nRETURN r.roomName AS Room, Count, StartTime\nORDER BY Count DESC\nLIMIT 3\n\n\n\nRoom\nCount\nStartTime\n\n\n\n\n2Q12 FR\n86\n09:00:00\n\n\n3E28 FR\n50\n09:00:00\n\n\n3E11 FR\n49\n09:00:00\n\n\n\nStudents with below/above average hours\nThis query returns students and whether they have more or less scheduled time on their timetable compared to the programme cohort average. The cut-off is based on average +/- 10% but can also be written using standard deviation.\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s.stuProgName AS progName, s.stuID_anon AS studentID, SUM(a.actDurationInMinutes) AS studentTotalDuration\nWITH progName, \n     AVG(studentTotalDuration) / 60 AS progAverageHoursPerStudent, \n     collect({studentID: studentID, studentTotalHours: studentTotalDuration / 60}) AS studentsData\nUNWIND studentsData AS studentData\nRETURN progName, \n       progAverageHoursPerStudent,\n       studentData.studentID AS studentID, \n       studentData.studentTotalHours AS studentTotalHours, \n       CASE \n           WHEN studentData.studentTotalHours &lt; (progAverageHoursPerStudent * 0.9) THEN 'Below Average'\n           WHEN studentData.studentTotalHours &gt; (progAverageHoursPerStudent * 1.1) THEN 'Above Average'\n           ELSE 'Average'\n       END AS compare\n\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-23442558‚Äù\n361\n‚ÄúAbove Average‚Äù\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-91911371‚Äù\n126\n‚ÄúBelow Average‚Äù\n\n\n‚ÄúMaths NS‚Äù\n274.7692307692308\n‚Äústu-75224499‚Äù\n251\n‚ÄúAverage‚Äù",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights"
    ]
  },
  {
    "objectID": "appendix-cypher4.html",
    "href": "appendix-cypher4.html",
    "title": "Count Queries",
    "section": "",
    "text": "This page contains a selection of count queries used to explore the graph database. The queries are designed to provide insights into the data and relationships between nodes. They can be considered starter queries which can be amended depending on the requirements.\n\nCount all nodes - by label\nBelow are two queries returning the same results - counts of nodes by node label.\n// Count of nodes - row per node\n\nUNWIND [\"student\", \"staff\", \"room\", \"activity\"] AS label\nMATCH (n)\nWHERE label IN labels(n)\nRETURN label, count(n) AS count\n\n\n\nCount All Nodes\n\n\n// Count of nodes - single row\n\nMATCH (n:student)\nWITH count(n) AS studentCount\nMATCH (n:staff)\nWITH studentCount, count(n) AS lecturerCount\nMATCH (n:room)\nWITH studentCount, lecturerCount, count(n) AS roomCount\nMATCH (n:activity)\nRETURN studentCount, lecturerCount, roomCount, count(n) AS activityCount\n\n\n\nCount All Nodes\n\n\n\n\nCount all relationships - by type\nThe query below returns counts of relationships. We can see that there are a significant amount of (student)-[]-&gt;(activity) relationships due to how we structured activity in the graph - that is, a separate node for each instance.\n// Count of relationships\n\nMATCH ()-[r:ATTENDS]-&gt;()\nWITH count(r) AS attendsCount\nMATCH ()-[r:TEACHES]-&gt;()\nWITH attendsCount, count(r) AS teachesCount\nMATCH ()-[r:OCCUPIES]-&gt;() \nWITH attendsCount, teachesCount, count(r) AS occupiesCount\nMATCH ()-[r:BELONGS_TO]-&gt;()\nRETURN attendsCount, teachesCount, occupiesCount, count(r) AS belongsCount\n\n\n\nCount All Relationships\n\n\n\n\nActivity counts\nIn this graph, an activity is an instance of an activity, that is, a unique combination of name, date, start, end, location, staff. It means a lot of activities!\n// Count of activities\n\nMATCH (a:activity)\nRETURN count(a) AS totalActivities;\n// Count of activities on a day\n\nMATCH (a:activity)\nWHERE a.actDayName = \"Wednesday\"\nRETURN DISTINCT count(a) AS wednesdayActivities\n\nMATCH (a:activity)\nRETURN DISTINCT a.actDayName AS dayName, count(a) AS activityCount\n\n\n\n\n\n\nActivity counts by time\nThe query below connects to the graph via python and returns the result - that is, the number of activities which start at 17:00.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\n// Activity count by time (start)\n\nMATCH (a:activity)\nWHERE a.actStartTime = localtime(\"17:00:00\")\n//AND a.actDayName = \"Wednesday\"\nRETURN count(a) AS activitiesStartingAt5pm\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\nfor record in result:\n    print(record)\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000001971C754790&gt;\nRunning query...\n\n&lt;Record activitiesStartingAt5pm=172&gt;\n\n\n\n\nStaff activity count\nThis query returns the first 5 rows of the query which counts activities by member of staff.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\n// Staff activity count\n\nMATCH (st:staff)-[r:TEACHES]-&gt;(a:activity)\nRETURN st.staffFullName_anon AS staffName, count(a) AS activityCount\nORDER BY activityCount DESC\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df of first 5 records\ndf = pd.DataFrame(records[:5], columns=[\"staffName\", \"activityCount\"])\n\n# print\nprint(df)\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x00000197372BE610&gt;\nRunning query...\n\n        staffName  activityCount\n0  Debbie Nichols            145\n1  Marc Hernandez            127\n2    Eileen Allen            126\n3    Steven Perez            124\n4  Justin Alvarez            112",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Count Queries"
    ]
  },
  {
    "objectID": "appendix-random-graph.html",
    "href": "appendix-random-graph.html",
    "title": "Random Graph Generator",
    "section": "",
    "text": "The function below generates a random graph (dot file) using Graphviz.\nTo render, ensure that graphviz is installed or save to file and render within documents using Quarto or similar.\n\n\nClick to show code\nimport graphviz\nimport random\nimport string\nfrom collections import defaultdict\n\ndef generate_random_graph(num_nodes=50, num_edges=100, num_clusters=5, colors=None):\n    \"\"\"Generates a random Graphviz graph with clusters and random colours.\n\n    Args:\n        num_nodes: Number of nodes in the graph.\n        num_edges: Number of edges in the graph.\n        num_clusters: Number of clusters to create.\n        colors: List of colours to use for clusters (optional). If not provided, random colours will be used.\n    \"\"\"\n\n    dot = graphviz.Digraph(\"G\")\n    dot.attr(fontname=\"Helvetica,Arial,sans-serif\")\n    dot.attr(layout=\"neato\")\n    dot.attr(start=\"random\")\n    dot.attr(overlap=\"false\")\n    dot.attr(splines=\"true\")\n    dot.attr(size=\"8,8\")\n    #dot.attr(dpi=\"300\")\n\n    # nodes to clusters, random colours if not provided\n    cluster_assignments = {}\n    if colors is None:\n        colors = [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(num_clusters)] \n\n    for i in range(num_nodes):\n        cluster_assignments[i] = random.randint(0, num_clusters - 1)\n\n    # random node names, colouur assignment\n    nodes = []\n    for i in range(num_nodes):\n        node_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n        nodes.append(node_name)\n        cluster_id = cluster_assignments[i]\n        color = colors[cluster_id]\n        dot.node(node_name, label=\"\", shape=\"circle\", height=\"0.12\", width=\"0.12\", fontsize=\"1\", fillcolor=color, style=\"filled\")\n        \n\n    # random edges (with a higher probability of staying within clusters)\n    edges = []\n    for _ in range(num_edges):\n        src_cluster = random.randint(0, num_clusters - 1)\n        dst_cluster = src_cluster if random.random() &lt; 0.8 else random.randint(0, num_clusters - 1)  # 80% chance of staying in cluster\n        src_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == src_cluster])\n        dst_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == dst_cluster])\n        edges.append((src_node, dst_node))\n\n    #  edges to the graph\n    for edge in edges:\n        dot.edge(*edge)\n\n    return dot",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Random Graph Generator"
    ]
  },
  {
    "objectID": "appendix-anonymise.html",
    "href": "appendix-anonymise.html",
    "title": "Anonymisation",
    "section": "",
    "text": "The following code snippet is shows how I anonymised personal data in a DataFrame using the Faker library.\nThe code generates fake names, emails, and IDs for staff or student data based on the unique IDs in the extract DataFrame. The anonymised data is then merged back with the original DataFrame, and the original columns are removed.\n\n\n\nPre-anonymisation Extract\n\n\n\n\n\n\nPost-anonymisation Extract\n\n\n \n\n\nClick to show code\nimport random\nimport hashlib\nfrom faker import Faker\nimport pandas as pd\n\n\ndef anonymise_data(df):\n    \"\"\"\n    anonymises cols in df by generating fake names, emails, and IDs.\n    \"\"\"\n    process_logger.info(\"Starting anonymisation\")\n    process_logger.info(f\"Columns in dataframe: {df.columns.tolist()}\")\n    \n    # staff or student data\n    if 'staffSplusID' in df.columns:\n        process_logger.info(\"Processing staff data\")\n        id_col = 'staffID'\n        prefix = 'staff'\n        columns_to_remove = ['staffFullName', 'staffLastName', 'staffForenames', 'staffID']\n    elif 'stuSplusID' in df.columns:\n        process_logger.info(\"Processing student data\")\n        id_col = 'studentID'\n        prefix = 'stu'\n        columns_to_remove = ['stuFullName', 'stuLastName', 'stuForenames', 'studentID']\n    else:\n        process_logger.error(\"Neither 'staffSplusID' nor 'stuSplusID' found in columns.\")\n        return df  # Return original dataframe if required columns are missing\n\n    # dictionary to store anonymised data\n    anon_data = {}\n    \n    # generate anonymised data for each unique ID\n    for unique_id in df[id_col].unique():\n        # create a seed based on the unique_id\n        seed = int(hashlib.md5(str(unique_id).encode()).hexdigest(), 16) & 0xFFFFFFFF\n        fake = Faker()\n        fake.seed_instance(seed)\n        random.seed(seed)\n\n        first_name = fake.first_name()\n        last_name = fake.last_name()\n        full_name = f\"{first_name} {last_name}\"\n        email = f\"{first_name.lower()}.{last_name.lower()}@fakemail.ac.uk\"\n        anon_id = f\"{prefix}-{random.randint(10000000, 99999999):08d}\"\n        \n        anon_data[unique_id] = {\n            f'{prefix}FirstName_anon': first_name,\n            f'{prefix}LastName_anon': last_name,\n            f'{prefix}FullName_anon': full_name,\n            f'{prefix}Email_anon': email,\n            f'{prefix}ID_anon': anon_id\n        }\n    \n    # create a new df with anonymised data\n    df_anon = pd.DataFrame.from_dict(anon_data, orient='index')\n    \n    # reset the index and rename it to match the original ID column\n    df_anon = df_anon.reset_index().rename(columns={'index': id_col})\n    \n    try:\n        # Merge anonymised data with the original DataFrame\n        df_result = pd.merge(df, df_anon, on=id_col)\n        \n        # Rmove columns that should be anonymised\n        columns_to_remove = [col for col in columns_to_remove if col in df_result.columns]\n        df_result = df_result.drop(columns=columns_to_remove)\n        \n        process_logger.info(\"Anonymisation completed successfully\")\n        return df_result\n\n    except Exception as e:\n        process_logger.error(f\"Error during anonymisation: {str(e)}\")\n        return df  # return original df if error",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Anonymisation"
    ]
  },
  {
    "objectID": "03-06a-load.html",
    "href": "03-06a-load.html",
    "title": "Google Load",
    "section": "",
    "text": "As I am using a free instance of Neo4j‚Äôs graph database - called Neo4j Aura - I needed to overcome some limitations which are not relevant to local installations of Neo4j or paid-for cloud instances.\nIn order to load from csv files, Neo4j Aura requires that the csv files are stored in public cloud storage like Google Drive or Dropbox. Therefore, my project requires this intermediary step.\n\n\n\n\n\n\n\ngoogle_drive_storage\n\n\n\nsource_files\n\nLocal Files\n(./{hostkeys}/process)\n\n\n\ngdrive_api\n\n\n\nGoogle Drive API\n\n\n\nsource_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nget_folder\n\nGet Folder Details\n\n\n\ngdrive_api-&gt;get_folder\n\n\n\n\n\ncreate_folder\n\nCreate Folder if Needed\n\n\n\nget_folder-&gt;create_folder\n\n\n\n\n\nupload_nodes\n\nUpload Files\nMatching Node Pattern\nto './{hostkeys}/node'\n\n\n\ncreate_folder-&gt;upload_nodes\n\n\n\n\n\nupload_relationships\n\nUpload Files\nMatching Relationship Pattern\nto './{hostkeys}/relationships'\n\n\n\ncreate_folder-&gt;upload_relationships\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nupload_nodes-&gt;process_done\n\n\n\n\n\nupload_relationships-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nConfiguration settings determine where processed node and relationship files are stored. I created one publicly shared folder in Google drive which contains all project csvs:\n\nroot Google Drive folder\n\nhostkeys (automatically created, unless override)\n\nnodes\nrelationships\n\n\n\n\n\n\nScreenshot of Google Drive",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Google Drive Load"
    ]
  },
  {
    "objectID": "03-01-overview.html",
    "href": "03-01-overview.html",
    "title": "Data Engineering Overview",
    "section": "",
    "text": "A main objective of my project is the development of a data pipeline which efficiently and securely transfers selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j).\nThis section provides an overview of the pipeline architecture, fundamental design principles, implementation approach and key learning takeaways.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#high-level-architecture",
    "href": "03-01-overview.html#high-level-architecture",
    "title": "Data Engineering Overview",
    "section": "High-level Architecture",
    "text": "High-level Architecture\nThe data pipeline consists of these core stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: CSV files are processed, cleaned, transformed, merged, and anonymised using Python.\nIntermediate Storage: Processed CSVs are uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. ùóòùó´ùóßùó•ùóîùóñùóß: Filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. ùóßùó•ùóîùó°ùó¶ùóôùó¢ùó•ùó†: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. ùêîùêèùêãùêéùêÄùêÉ\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. (Optional) Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. ùóüùó¢ùóîùóó: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure¬†1",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#design-principles",
    "href": "03-01-overview.html#design-principles",
    "title": "Data Engineering Overview",
    "section": "Design Principles",
    "text": "Design Principles\nSeveral ‚Äúbest practices‚Äù in data handling, processing, and database management were incorporated in developing this ETL. The data pipeline is built on several core design principles:\n\n\n\nDesign Principles\n\n\nI started with a strong sense of what I wanted to build - a modular, scalable, secure and configurable design - however, what exactly this meant was only discovered during the development process.\nGiven project constraints - deadline, word-limits, resources, data, technology - it is fair to say that compromises were made. That said, it was important that the final artefact is one that can be developed further for specific business use-cases.\n\nSecurity and Data Protection\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\nModularity, Scalability and Automation\n\n\nDistinct, interoperable modules (extract, process, upload, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\nError Handling and Logging\n\n\nRobust error handling\nComprehensive logging for troubleshooting and auditing\n\n\n\nUser configurable\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#implementation-approach",
    "href": "03-01-overview.html#implementation-approach",
    "title": "Data Engineering Overview",
    "section": "Implementation Approach",
    "text": "Implementation Approach\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nCrucial aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. See Technology Stack for more details.\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that components are behaving as expected.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#upcoming-sections",
    "href": "03-01-overview.html#upcoming-sections",
    "title": "Data Engineering Overview",
    "section": "Upcoming Sections",
    "text": "Upcoming Sections\nThe following sections will delve into specific implementation details of each stage, demonstrating how these principles are put into practice, before reflecting on lessons learned and potential future enhancements.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Graph Data Models for Timetabling Insights",
    "section": "",
    "text": "Supervisor: Xiaodong Li\nProgramme: MSc Data Science\nWord Count: [add when finished - currently ~6100 words (excluding code, tables, references)]\n[insert Loom video]",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "appendix-supervision3.html",
    "href": "appendix-supervision3.html",
    "title": "Weekly Update - 2024-07-08",
    "section": "",
    "text": "This week has been primarily based on getting the ETL pipeline working.\nSome thought and planning about the overall project",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#summary",
    "href": "appendix-supervision3.html#summary",
    "title": "Weekly Update - 2024-07-08",
    "section": "",
    "text": "This week has been primarily based on getting the ETL pipeline working.\nSome thought and planning about the overall project",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#accomplishments",
    "href": "appendix-supervision3.html#accomplishments",
    "title": "Weekly Update - 2024-07-08",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nProject Management\n\nPlanned next 6 weeks - high level\n\nData Collection:\n\nN/A\n\nAnalysis / Wrangling:\n\nN/A\n\nModel Development:\n\nThe ETL is (hopefully) production ready.\n\nResults:\n\nFunctioning ETL which:\n\nExtracts -&gt; Filters -&gt; Cleans -&gt; Transforms -&gt; Anonymises -&gt; Uploads to Google Drive -&gt; Loads to Neo4j\nConfigurable, scalable, modular\nLogging, Error-handling",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#next-steps",
    "href": "appendix-supervision3.html#next-steps",
    "title": "Weekly Update - 2024-07-08",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goal: What is goal of week?\n\nProject Management\n\nPlan next six weeks - with tasks\nsee project-structure\n\nData:\n\nClear out database and repopulate\nRun ETL for some programme - e.g.¬†UG maths, PG Data Sci, PG AI, PG cyber\nConsider Unit Tests\n\nAnalysis / Wrangling:\n\nDevelop and explore Cypher quality and violation queries\nsee queries\n\nModeling:\n\nConsider different model of time as nodes (but probably will not develop)\n\nWriting\n\nAfter ETL confirmation - fully documentation of code\n\nAlso write up into QMD doc\nVisualiation - Data Flow Diagram?\n\nModel section write up\n\nSQL data model\nGraph data model\nVisualisations\nsee code-critique\n\nDraft Intro\n\nProof of concept\nResearch question\nDomain",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#issuesblockers",
    "href": "appendix-supervision3.html#issuesblockers",
    "title": "Weekly Update - 2024-07-08",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nMain limitations are free Neo4j Aura software and other restrictions\n\nMethodological:\n\nNeed to think about what I want to deliver and how it looks",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#post-meeting-notes",
    "href": "appendix-supervision3.html#post-meeting-notes",
    "title": "Weekly Update - 2024-07-08",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\n\nKey Decisions: What were the main takeaways from meeting?\n\nI need to focus on making progress and think about what the final produce will be.\n\nAction Items: What tasks do you need to complete?\n\nsee above - write up sections, even if draft",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#additional-notes",
    "href": "appendix-supervision3.html#additional-notes",
    "title": "Weekly Update - 2024-07-08",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nLiterature Review: Relevant papers read.\nExperiments: If applicable, describe any experiments conducted.\nCode: Embedding example snippets of code.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision1.html",
    "href": "appendix-supervision1.html",
    "title": "Weekly Update - 10 June 2024",
    "section": "",
    "text": "This week has been about getting back on track after an extended period away from the dissertation. It has mainly revolved around refocusing, rescoping, reacquainting and rekindling motivation.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#summary",
    "href": "appendix-supervision1.html#summary",
    "title": "Weekly Update - 10 June 2024",
    "section": "",
    "text": "This week has been about getting back on track after an extended period away from the dissertation. It has mainly revolved around refocusing, rescoping, reacquainting and rekindling motivation.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#accomplishments",
    "href": "appendix-supervision1.html#accomplishments",
    "title": "Weekly Update - 10 June 2024",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nResults:\n\nrecreated single student graph timetable - see poc-1-basic\ncreated SQL to csv pipeline\ncreated cypher queries\n\nProject Management\n\nnew github repo (currently private) https://github.com/zoonalink/graph-project\n\ndraft readme\nfolder/file structure\n\nweekly update template\n\nData Collection:\n\nSQL to CSV files for single student\nraw SQL data tables\n\nAnalysis / Wrangling:\n\nTransformation in SQL query\n\nModel Development:\nInvestigation\n\npipelines to automate data flow - prefect\nhow to represent time in graph",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#issuesblockers",
    "href": "appendix-supervision1.html#issuesblockers",
    "title": "Weekly Update - 10 June 2024",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nOriginal servers were deleted and access to new servers was gone\nSome original work no longer works\n\nMethodological:\n\nI need to ensure that scope does not creep.\nI need to stay focused and not start solving problems which are out of scope or are interesting.\n\nData-Related:\n\ndevelop a robust anonymisation process\nrepresenting time - see representing time",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#next-steps",
    "href": "appendix-supervision1.html#next-steps",
    "title": "Weekly Update - 10 June 2024",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goals\n\nBigger cohort of data (MSc DataScience?)\nExplore time in Graph\nData pipeline documentation and steps (anonymisation)\n\n\nProject Management Project tasks - planning, admin.\n\nadd supervisor to github repo\n\nData:\n\nAnonymisation or generation\nMSc Data Science cohort isolated into separate csv files\nsplitting data into single rows\nPipeline steps plotted\n\nAnalysis / Wrangling:\n\nMore advanced cypher queries\n\nModeling:\n\nComparing different representations of time\n\nValidation:\nDeadlines:",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#post-meeting-notes",
    "href": "appendix-supervision1.html#post-meeting-notes",
    "title": "Weekly Update - 10 June 2024",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\nWe met on 13 June 2024 for approximatly 45 minutes. I showed Xiaodong what I have been up to referencing my working files, poc files, and some code. I also showed what I currently have in my free instance of neo4j aura - which was MSc DS students activities and rooms. Staff and Students to be loaded but I want to ensure my anonymisation function is tested first.\nWe discussed what my aims are with this project, clarifying that it is not an building a timetable schedule based on graph structure; it is also not a timetable optimiser. Instead it is a data engineering project that ultimately explores whether representing timetables in graph format can bring opportunities for reporting and insight which is currently difficult to produce using the current system.\nWe agreed to meet in two weeks time. I will schedule a meeting for us.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#additional-notes",
    "href": "appendix-supervision1.html#additional-notes",
    "title": "Weekly Update - 10 June 2024",
    "section": "Additional Notes",
    "text": "Additional Notes\nSummary from intial meeting\nI met with Xiaodong Li my supervisor on Wednesday 01 May 2024 where we introduced ourselves and our backgrounds. We discussed my proposal which Xiaodong had read in advance of our meeting.\nI showed Xiaodong my work so far, most of it completed in January which included proof-of-concept data engineering steps to extract, transform and load data from a relational database to a graph structure. I showed the steps I had taken, the challenges encountered and the possible opportunities of graph data structures which I am hoping to explore in more detail.\nWe discussed project scope and outcomes and recognised that I need to ensure that I keep within scope.\nThe next few weeks will require attention on other matters (work, taught modules, etc.) but I will look to pick up project work soon.\nIf possible, Xiaodong was going to investigate Neo4j and graph to get a bit of context.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Weekly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-cypher7.html",
    "href": "appendix-cypher7.html",
    "title": "Rooms and Spaces",
    "section": "",
    "text": "Start from rooms in cypher-query-notes\nutilisation most popular rooms mapping rooms room types",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Rooms and Spaces"
    ]
  },
  {
    "objectID": "appendix-cypher2.html",
    "href": "appendix-cypher2.html",
    "title": "Deleting Nodes and Relationships",
    "section": "",
    "text": "Deleting nodes and relationships, using the DELETE clause in Cypher, is an important operation in managing the graph database.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher2.html#deleting-nodes",
    "href": "appendix-cypher2.html#deleting-nodes",
    "title": "Deleting Nodes and Relationships",
    "section": "Deleting Nodes",
    "text": "Deleting Nodes\nThe general syntax for deleting a node is:\nMATCH (n:NodeLabel {propertyName: propertyValue})\nDELETE n\nWhere:\n\nn is the node variable\nNodeLabel is the label assigned to the node\npropertyName is the property name\npropertyValue is the value assigned to the property\nDELETE is used to delete the node\nMATCH is used to find the node to delete\n... represents additional properties\n\n\nExample: Deleting a Student Node\nMATCH (s:Student {studentID: '123456'})\nDELETE s",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher2.html#deleting-relationships",
    "href": "appendix-cypher2.html#deleting-relationships",
    "title": "Deleting Nodes and Relationships",
    "section": "Deleting Relationships",
    "text": "Deleting Relationships\nThe general syntax for deleting a relationship is:\nMATCH (n1:NodeLabel1)-[r:RELATIONSHIP_TYPE]-&gt;(n2:NodeLabel2)\nDELETE r\nWhere:\n\nn1 and n2 are the node variables\nNodeLabel1 and NodeLabel2 are the labels assigned to the nodes\nr is the relationship variable\nRELATIONSHIP_TYPE is the type of relationship\nDELETE is used to delete the relationship\nMATCH is used to find the relationship to delete\n... represents additional properties\n\n\nExample: Deleting a Relationship Between a Student and an Activity\nMATCH (s:Student {studentID: '123456'})-[r:ATTENDS]-&gt;(a:Activity {activityID: '789'})\nDELETE r",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-config.html",
    "href": "appendix-config.html",
    "title": "Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\"",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Configuration"
    ]
  },
  {
    "objectID": "appendix-code5-transform.html",
    "href": "appendix-code5-transform.html",
    "title": "Process",
    "section": "",
    "text": "Graph Timetable - Quarto - Process\n\nprocess_utils\nprocess_node\nprocess_main",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Transform"
    ]
  },
  {
    "objectID": "appendix-code3-extract.html",
    "href": "appendix-code3-extract.html",
    "title": "Extract",
    "section": "",
    "text": "Graph Timetable - Quarto - Extract\n\ntemp_table_loader\nextract_sql_file\nextract_data\nextract_main",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Extract"
    ]
  },
  {
    "objectID": "appendix-code1-config.html",
    "href": "appendix-code1-config.html",
    "title": "Config and Misc",
    "section": "",
    "text": "Graph Timetable - Quarto - Config and Misc\n\nconfig.py\nlogger_config.py\nneo4j_schema.py\nutils.py\nconnect_to_neo4j.py\nconnect_to_rdb.py",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Config and Misc"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgement",
    "section": "",
    "text": "First, I would like to express my gratitude to my supervisor, Dr.¬†Xiaodong Li, for their guidance, support and feedback throughout this project. They were particular helpful in keeping me focused and staying on topic, allowing me to incrementally achieve my goals.\nI extend my sincere thanks to the faculty and staff of the MSc Data Science programme for providing the environment and resources necessary for my project:\n\nDr Paul Matthews - Course lead for MSc Data Science\nPrakash Chatterjee - Data Management Fundamentals\n\nReintroduced the concept of graph databases and noSQL.\n\nDr David Wyatt - Programming for Data Science\n\nProvided foundations of Python programming, Git version control, markdown.\n\nDr Hisham Ihshaish - Machine Learning and Predictive Analytics\n\nIntroduced me to Machine Learning, marrying programming and statistics.\n\nDr Deirdre Toher - Advanced Statistics\n\nPulled me through challenging statistitics, introduced me to Quarto (in an R context) and was generally very supportive over the years. Now working for Central Statistics Office in Ireland.\n\nDr Jason Anquandah - Interdisciplinary Group Project\n\nSupported me through the challenges of a group project.\n\nDr Mahmoud Elbattah - Interdisciplinary Group Project\n\n\nSupported me through the challenges of a group project.\n\n\n\nI am indebted to my colleagues in SDS for their support, camaraderie, intellectual discussions and willingness to share knowledge, expertise and time. I am especially grateful to my mentor, colleague, manager and friend, Esther Williams, who has supported me throughout this journey and others, always magnanimously sharing her wisdom and unwavering encouragement.\nFinally, I would like to thank my family and friends for their unwavering love and support throughout my graduate studies. Their encouragement and understanding have been invaluable in helping me navigate the challenges and celebrate the successes along the way.\nIn the spirit of graphs‚Ä¶\n\n\n\nAcknowledgement graph",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Acknowledgements"
    ]
  },
  {
    "objectID": "05-future-opportunities.html",
    "href": "05-future-opportunities.html",
    "title": "Future Opportunities",
    "section": "",
    "text": "Caution\n\n\n\nNOTES, repetition",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#opportunities",
    "href": "05-future-opportunities.html#opportunities",
    "title": "Future Opportunities",
    "section": "Opportunities",
    "text": "Opportunities\nUnveiling Hidden Patterns & Improving Student Experience: Problem: Timetable inefficiencies often remain hidden in relational data, impacting student experience. Graph Solution: Graph analysis can uncover patterns like students with excessive travel time between classes, those lacking adequate breaks, or those facing scheduling conflicts due to part-time work. This empowers universities to optimize timetables for improved student well-being and academic performance. Stakeholder-Centric Analysis & Enhanced Decision Making: Problem: Traditional timetabling often prioritizes one factor (e.g., room utilisation) over others, neglecting holistic needs. Graph Solution: Graphs allow simultaneous modeling of student preferences (class times, travel distance), faculty constraints, and institutional priorities (resource allocation). This enables data-driven decisions that balance stakeholder needs and improve overall satisfaction. What-If Scenarios & Agile Timetable Management: Problem: Evaluating the impact of timetable changes in relational systems is cumbersome, hindering proactive planning. Graph Solution: Graph databases excel at simulating ‚Äúwhat-if‚Äù scenarios. Adding hypothetical courses, adjusting room capacities, or modifying faculty availability becomes straightforward. This agility allows for rapid evaluation of multiple scenarios, enabling institutions to anticipate challenges and adapt timetables dynamically. Visual Exploration & Fostering Collaboration: Problem: Communicating complex timetable data to diverse stakeholders (students, faculty, administrators) is challenging. Graph Solution: Graph visualizations make complex relationships intuitive and accessible, fostering shared understanding. This transparency promotes collaboration, reduces misunderstandings, and facilitates informed decision-making.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#challenges",
    "href": "05-future-opportunities.html#challenges",
    "title": "Future Opportunities",
    "section": "Challenges",
    "text": "Challenges\nResources, commitment to exploration. Data Migration & Integration: Challenge: Migrating from existing relational systems to a graph database requires careful planning and data transformation. Mitigation: Employing robust ETL (Extract, Transform, Load) processes and leveraging graph database import tools can streamline the migration process. Prioritizing incremental migration, starting with core entities, can minimize disruption. Tooling and Expertise: Bridging the Skills Gap: Challenge: The graph database ecosystem, while maturing, might require specialised skills compared to traditional SQL. Mitigation: Investing in staff training, collaborating with experts, and leveraging online resources can address the skills gap. Open-source graph databases like Neo4j offer ample learning material and community support. Performance at Scale: Ensuring Responsiveness with Large Datasets: Challenge: Graph databases, while generally performant for connected data, might face challenges with extremely large universities and complex queries. Mitigation: Employing performance tuning techniques like indexing, caching, and query optimization can enhance scalability. Exploring specialised graph database solutions designed for high-volume transactional systems might be necessary in extreme cases. ‚ÄúSoft‚Äù Constraint Modeling: Quantifying Subjective Preferences: Challenge: Graphs excel at explicit relationships but struggle with subjective preferences (e.g., student aversion to late classes). Mitigation: Combine graph analysis with techniques like sentiment analysis on student feedback or preference elicitation surveys. This hybrid approach allows incorporating both explicit relationships and quantified subjective factors.\n2.4 Data Augmentation Opportunities\nData Augmentation Opportunities: You touch on this briefly; expanding this section could be very compelling. Example: Integrating room location data (latitude/longitude) with student address data could allow for powerful analyses of commute patterns and potential inequities -&gt; EDI, planning, Business intelligence, predictive analysis Ethical Considerations: mention the importance of data privacy, anonymisation, and responsible use of insights.\n2.5 Challenges and Considerations\nPotential limitations of the graph approach Data migration considerations Performance considerations for large-scale timetabling systems",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#future-opportunities-and-potential-insights",
    "href": "05-future-opportunities.html#future-opportunities-and-potential-insights",
    "title": "Future Opportunities",
    "section": "Future Opportunities and Potential Insights",
    "text": "Future Opportunities and Potential Insights\n\nDiscussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#exploring-time",
    "href": "05-future-opportunities.html#exploring-time",
    "title": "Future Opportunities",
    "section": "Exploring time",
    "text": "Exploring time\nFuture work could include evaluating the performance and scalability of different time modeling options, particularly:\n\nDynamic Node Creation (Option 2 or 3): This approach would create Time or TimeBlock nodes only when needed, potentially offering a good balance between flexibility and performance.\nDirect Performance Comparisons: Conducting benchmarks against specific use cases and datasets will provide valuable insights for choosing the optimal approach for large-scale deployments.\n\nModeling time effectively is crucial for unlocking the full potential of a graph database for university timetabling analysis. This section has outlined the challenges, explored potential solutions, and documented the chosen approach for this proof of concept.\nFurther exploration and optimisation of time modeling will be essential for developing robust, scalable, and insightful graph-based timetabling solutions. The next section will delve into the data engineering pipeline required to populate and maintain this model, bridging the gap between raw data and insightful analysis.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "03-07-reflections.html",
    "href": "03-07-reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "From the outset, I was wary that this data engineering project is too ambitious in both scale and scope. However, the reality of its magnitude became increasingly apparent as development progressed.\nYet, despite my initial awareness, I found myself continually expanding the project‚Äôs boundaries, often pushing for a ‚Äúgold-plated‚Äù solution rather than acknowledging when certain aspects were ‚Äúgood enough.‚Äù This tendency towards scope creep, while driven by a desire for excellence, has significantly increased the project‚Äôs complexity and time requirements.\nThe learning curve has been exceptionally steep. I‚Äôve had to rapidly acquire proficiency in a diverse range of technologies and tools: Python, Neo4j, Google APIs, Quarto, and GraphViz. This intensive learning process, while challenging, has also been incredibly rewarding. My technical toolkit has expanded far beyond my initial expectations - but this also contributed to the continuously expanding scope, as each new skill opened possibilities for further enhancement and the necessitry for on-the-fly troubleshooting.\nUnexpected challenges have been a constant companion throughout this process. From deleted servers and access issues to discrepancies between development environments (such as missing certificates), I‚Äôve encountered a wide array of unforeseen obstacles. These issues have necessitated the development of strong troubleshooting skills and a flexible approach to problem-solving.\nWhile often frustrating, these challenges have also provided valuable learning opportunities, pushing me to deepen my understanding of the systems and technologies I‚Äôm working with.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-07-reflections.html#lessons-learned",
    "href": "03-07-reflections.html#lessons-learned",
    "title": "Reflections",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nScope management is crucial: Work on recognising when a solution is ‚Äúgood enough‚Äù and resist the urge to continually expand scope. Set clear boundaries at the start and be prepared to reassess and adjust plans when necessary.\nEmbrace modularisation from the beginning: Avoid the temptation to create oversized code blocks. Maintain a list of ‚Äúfuture enhancements‚Äù to prevent immediate implementation of every idea.\nBalance documentation with development: Document sufficiently during the development process, but save comprehensive documentation for appropriate milestones. This approach maintains progress while ensuring proper record-keeping.\nView obstacles as learning opportunities: Embrace continuous learning and see challenges as chances to grow. Invest time in understanding the right technologies and approaches, particularly focusing on modularisation.\nCelebrate incremental progress: Recognise and appreciate small achievements throughout the development process. This helps maintain motivation and provides a clearer sense of overall progress.\n\nThe next section will start looking at the newly transferred data in the graph database.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-02-approach.html",
    "href": "03-02-approach.html",
    "title": "Data Engineering Approach",
    "section": "",
    "text": "I followed an interative, agile-inspired approach despite being a team of one. This method allowed for flexibility, continuous improvement and the opportunity to adapt to new insights during the process (Beck, K., et al.¬†2001).\nThe bulk of my effort was spent prototyping, testing and reviewing with each iteration resulting in a new challenge, issue, or opportunity.\n\n\n\nIterative Development Approach\n\n\n\nInitial Planning and Requirements Gathering\nThe development cycle began with initial high-level planning and requirements gathering, where I imagined how each stage should work, trying to bear in mind future-proofing and repeatability principles.\nI defined core functionality for each module (extraction, transformation, loading) and outlined initial technical requirements and constraints. The planning documentation was maintained in Quarto and markdown files in a centralised repository for project information.\n\n\nPrototyping\nFollowing initial planning, rapid prototyping was undertaken for each module:\n\nSQL prototyping for data extraction queries\nPython prototyping for data transformation and processing logic\nNeo4j prototyping for graph database schema and loading procedures\n\nThis stage allowed for quick exploration of different approaches and early identification of potential challenges as well as giving me the confidence to continue with my exploration.\n\n\nComponent-Based Development and Testing\n\nEach module (extraction, transformation, loading) was developed separately with a view to distinct ‚Äúhandovers‚Äù\nAn iterative, component-based testing approach was employed\nWhile formal unit tests were not always created, each component was thoroughly tested for functionality\n\nThis approach allowed for continuous progress while maintaining a focus on component-level quality. It was during this phase that I started expanding configuration, logging and error-handling options - and I am glad I did!\n\n\nIntegration -&gt; Review -&gt; Demo -&gt; Feedback -&gt; Repeat\nAs components reached a (more) stable state, they were integrated and reviewed:\n\nComponents were combined to form larger functional units\nIntegrated functionality was occasionally demonstrated to subject matter experts (e.g.¬†data manager)\nFeedback was gathered on functionality, usability, and alignment with requirements\n\nInsights gained from reviews, demonstrations and ongoing development were continuously fed back into the process. New requirements or modifications were documented, for example updates to SQL SELECT statements and data model interpretations.\nEach change required decisions - but I did not always make the right ones!\n\n\nVersion Validation and Documentation\nAt pivotal junctures, e.g., when a stable version was achieved:\n\nEnd-to-end validation of the entire pipeline was performed.\nResults were documented in notebooks, including opportunities for improvement.\nBugs and opportunities were logged for future iterations.\n\n\n\nContinuous Learning and Adaptation\nLearning and adaptation became central to the project. Each iteration brought new insights, often through trial and error and certainly through unintended consequences or unforeseen complications. Early challenges included the need to modularise components before they became unmanageable and resisting the temptation to make overly ambitious changes. With practice, I became better at recognising when refactoring was necessary.\nDeveloping the ETL was not a linear journey. There were many moments of frustration, periods of seemingly endless, painstaking troubleshooting, and a constant battle against the urge to over-deliver. Yet, with each stumble, the process itself became more refined, transforming into a powerful tool for identifying and resolving issues.\nWhile core MVP (minimum viable product) requirements remained relatively stable (I set them after all!), iterating allowed me to seize opportunities for enhancement. Each chance to modularise, parameterise, or fine-tune sparked an almost compulsive drive for improvement, pushing the pipeline beyond its initial scope.\nUltimately it all resulted in a robust, flexible solution that can adapt (relatively) gracefully to unforeseen challenges and serve as the starting point for future opportunities.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Approach"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html",
    "href": "02-02-graph-timetable.html",
    "title": "Graph Data Model for Timebling",
    "section": "",
    "text": "Having discussed advantages of graph databases for representing interconnected data, this section delves into the specifics of a proposed graph data model tailored for university timetabling.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#an-iterative-approach",
    "href": "02-02-graph-timetable.html#an-iterative-approach",
    "title": "Graph Data Model for Timebling",
    "section": "An Iterative Approach",
    "text": "An Iterative Approach\nDue to flexibility, creating graph data models is an iterative process: design -&gt; build -&gt; test -&gt; review -&gt; revise -&gt; ‚Ä¶and repeat.\nMy first model was small in scope, incorporating minimal nodes and properties in an MVP1 approach. Eventually, my expanded model was created in a cloud-instance of Neo4j Aura.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "href": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "title": "Graph Data Model for Timebling",
    "section": "Core Nodes - Building Blocks",
    "text": "Core Nodes - Building Blocks\nAt its core, the timetable model revolves around four key entities represented as nodes:\n\n\n\n\nNode\nProperty\nDescription\nData Type\n\n\n\n\nStudent\nfirstName\nLegal first name\nstring\n\n\n\nlastName\nLegal last name\nstring\n\n\n\nstudentID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nLecturer\nfirstName\nFirst name\nstring\n\n\n\nlastName\nLast name\nstring\n\n\n\nstaffID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nRoom\nname\nRoom name\nstring\n\n\n\nsplusID\nTimetable URN\ninteger\n\n\nActivity\nname\nActivity name\nstring\n\n\n\ndescription\nActivity description\nstring\n\n\n\nstartTime\nScheduled start time\ndatetime\n\n\n\nendTime\nScheduled end time\ndatetime\n\n\n\ndate\nDate of activity\ndate",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "href": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "title": "Graph Data Model for Timebling",
    "section": "Relationships - Connecting the Dots",
    "text": "Relationships - Connecting the Dots\nThe core nodes are interconnected through relationships that reflect the dynamics of a timetable:\n\n(Student)-[IS_ALLOCATED_TO]-&gt;(Activity)\n(Staff)-[TEACHES_ON]-&gt;(Activity)\n(Activity)-[TAKES_PLACE_IN]-&gt;(Room)",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#mvp-model",
    "href": "02-02-graph-timetable.html#mvp-model",
    "title": "Graph Data Model for Timebling",
    "section": "MVP model",
    "text": "MVP model\n\n\n\nCore Nodes and Properties\n\n\n\n\n\nNeo4j Interface showing basic nodes and properties",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#footnotes",
    "href": "02-02-graph-timetable.html#footnotes",
    "title": "Graph Data Model for Timebling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMinimum Viable Product: ‚ÄúFirst, a definition: the minimum viable product is that version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort or in other words building the most minimum version of their product that will still allow them to learn.‚Äù (Ries, 2024)‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "01-02-background.html",
    "href": "01-02-background.html",
    "title": "Background and Motivation",
    "section": "",
    "text": "Many years ago I grappled with the complexities of timetable generation and optimisation, and battled with trying to balance competing, but conflicting demands like maximising room utilisation and adhering to staff working patterns and producing a ‚Äòdecent‚Äô timetable for the students. It is an unwinnable battle.\nThese experiences and challenges left an indelible mark - highlighting the need for robust tools and metrics to understand and assess timetable quality - a factor which is often overshadowed by the pursuit of mere feasibility.\nThis project is the result of a deliberate clash of my professional experiences and data science learning where I aim to deliver a practical solution to a real-world problem.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#personal",
    "href": "01-02-background.html#personal",
    "title": "Background and Motivation",
    "section": "",
    "text": "Many years ago I grappled with the complexities of timetable generation and optimisation, and battled with trying to balance competing, but conflicting demands like maximising room utilisation and adhering to staff working patterns and producing a ‚Äòdecent‚Äô timetable for the students. It is an unwinnable battle.\nThese experiences and challenges left an indelible mark - highlighting the need for robust tools and metrics to understand and assess timetable quality - a factor which is often overshadowed by the pursuit of mere feasibility.\nThis project is the result of a deliberate clash of my professional experiences and data science learning where I aim to deliver a practical solution to a real-world problem.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "href": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "title": "Background and Motivation",
    "section": "Research Gap: Bridging Theory and Practice",
    "text": "Research Gap: Bridging Theory and Practice\nMuch current research into university timetabling centres on combinatorial optimisation (Chen et al., 2021), that is using various sophisticated techniques designed to efficiently generate feasible solutions given a set of constraints. This computationally-driven optimisation research is often referred to as the university course timetabing problem (UCTTP) and is categorised as NP-hard1, meaning finding the absolute ‚Äúbest‚Äù timetable is exceptionally challenging (Babaei, Karimpour and Hadidi, 2015; Herres and Schmitz, 2021; Wikipedia contributors, 2024).\nConsequently, significant effort has been dedicated to developing algorithms like constraint programming (Holm et al., 2022) and local search techniques such as Tabu Search and simulated annealing (Oude Vrielink et al., 2019), aiming to create workable timetables within reasonable timeframes. While crucial for advancing algorithmic development, these idealised scenarios2 do not fully capture the dynamic complexity of real-world university timetabling.\nUniversities grapple with constantly shifting demands: fluctuating student populations, evolving institutional preferences, resource limitations, and the ever-present need to balance diverse stakeholder needs. These complexities extend beyond simply finding a feasible solution ‚Äì they necessitate tools to understand the trade-offs inherent in any timetable, enabling informed decisions about which ‚Äúgood‚Äù outcomes to prioritise (Lindahl, 2017).",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#the-hypothesisenter-graphs",
    "href": "01-02-background.html#the-hypothesisenter-graphs",
    "title": "Background and Motivation",
    "section": "The hypothesis‚Ä¶Enter Graphs",
    "text": "The hypothesis‚Ä¶Enter Graphs\nThis is where I believe graph data structures could offer unique potential.\nTimetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Graph databases excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable.\nWhile algorithms excel at generating optimised solutions, there remains a gap in post-generation analysis ‚Äì e.g.¬†the ability to delve into a timetable‚Äôs nuanced impacts on student and staff experience. Despite the acknowledged importance of factors like room allocation and teaching period distribution, traditional optimisation-focused approaches lack the tools to explore these relationships in depth (Ceschia, Di Gaspero and Schaerf, 2023; Lindahl, 2017; Rudov√°, M√ºller and Murray, 2011), particularly in a real-world scenario.\nThis potential for deeper analysis motivates this exploration of graph data structures for enhancing timetable understanding and, ultimately, improving timetable quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#footnotes",
    "href": "01-02-background.html#footnotes",
    "title": "Background and Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúIn¬†computational complexity theory, a computational problem¬†H¬†is called¬†NP-hard¬†if, for every problem¬†L¬†which can be solved in¬†non-deterministic polynomial-time, there is a¬†polynomial-time reduction¬†from¬†L¬†to¬†H.‚Äù (Wikipedia contributors, 2024)‚Ü©Ô∏é\nResearch on computational optimisation often makes use of standardised datasets and predefined constraints in order to facilitate comparison, repeatability and evaluation.‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": " ",
    "section": "",
    "text": "Abdipoor, S., Yaakob, R., Goh, S.L. and Abdullah, S. (2023) Meta-heuristic approaches for the University Course Timetabling Problem. Intelligent Systems with Applications [online]. 19, p.¬†200253. Available from: https://www.sciencedirect.com/science/article/pii/S2667305323000789 [Accessed 25 July 2024].\nAnon. (no date) CPB Projects [online]. Available from: https://www.cpbprojects.co.uk/solutions/timetabling-and-teaching-space [Accessed 25 July 2024a].\nBabaei, H., Karimpour, J. and Hadidi, A. (2015)‚ÄòA survey of approaches for university course timetabling problem‚Äô Computers & Industrial EngineeringApplications of Computational Intelligence and Fuzzy Logic to Manufacturing and Service Systems [online]. 86, pp.¬†43‚Äì59. Available from: https://www.sciencedirect.com/science/article/pii/S0360835214003714 [Accessed 28 July 2024].\nBeck, K., et al.¬†(2001) The Agile Manifesto. Agile Alliance. http://agilemanifesto.org/\nBellio, R., Ceschia, S., Di Gaspero, L., Schaerf, A. and Urli, T. (2016) Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem. Computers & Operations Research [online]. 65, pp.¬†83‚Äì92. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0305054815001690 [Accessed 26 January 2024].\nBonutti, A., De Cesco, F., Di Gaspero, L. and Schaerf, A. (2012) Benchmarking curriculum-based course timetabling: formulations, data formats, instances, validation, visualization, and results. Annals of Operations Research [online]. 194 (1), pp.¬†59‚Äì70. Available from: https://doi.org/10.1007/s10479-010-0707-0 [Accessed 3 February 2024].\nBruggen, R. van (2014)‚ÄôLearning Neo4j: run blazingly fast queries on complex graph datasets with the power of the Neo4j graph database‚ÄôCommunity Experience Distilled. 1st edition. Birmingham, England: Packt Publishing.\nBurke, E., Mccollum, B., Meisels, A., Petrovic, S. and Qu, R. (2007) A graph-based hyper-heuristic for educational timetabling problems. European Journal of Operational Research [online]. 176, pp.¬†177‚Äì192.\nCeschia, S., Di Gaspero, L. and Schaerf, A. (2023) Educational timetabling: Problems, benchmarks, and state-of-the-art results. European Journal of Operational Research [online]. 308 (1), pp.¬†1‚Äì18. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0377221722005641 [Accessed 25 January 2024].\nChen, M., Sze, S., Goh, S.L., Sabar, N. and Kendall, G. (2021) A Survey of University Course Timetabling Problem: Perspectives, Trends and Opportunities. IEEE Access [online]. PP, pp.¬†1‚Äì1.\nChicken, S., Fogg Rogers, L., Hobbs, L., Hunt-Fraisse, T. and Lewis, D. (2023) Amplifying the voices of neurodivergent students in relation to higher education assessment at UWE Bristol. [online]. Available from: https://uwe-repository.worktribe.com/output/10879555 [Accessed 25 July 2024].\nDammak, A., Elloumi, A. and Kamoun, H. (2007) An enterprise system component based on graph colouring for exam timetabling: A case study in a Tunisian university. Transforming Government: People, Process and Policy [online]. 1 (3), pp.¬†255‚Äì270. Available from: https://www.emerald.com/insight/content/doi/10.1108/17506160710778095/full/html [Accessed 19 February 2024].\nde Werra, D. (1997) The combinatorics of timetabling. European Journal of Operational Research [online]. 96 (3), pp.¬†504‚Äì513.\nDon State Technical University, Rostov-on-Don, Russian Federation and Al-Gabri, W.M. (2017) Literature review for the topic of automation of scheduling classes and exams in higher education institutions. Vestnik of Don State Technical University [online]. 17 (1), pp.¬†132‚Äì143. Available from: https://vestnik.donstu.ru/jour/article/view/255 [Accessed 25 January 2024].\nDowland, D. (2018) Rubik‚Äôs cube or Battenburg? The university timetable Wonkhe. 11 January 2018 [online]. Available from: https://wonkhe.com/blogs/rubiks-cube-or-battenburg-the-university-timetable/ [Accessed 25 July 2024].\nFoung, D. and Chen, J. (2019) Discovering disciplinary differences: blending data sources to explore the student online behaviors in a University English course. Information Discovery and Delivery [online]. 47 (2), pp.¬†106‚Äì114. Available from: https://www.emerald.com/insight/content/doi/10.1108/IDD-10-2018-0053/full/html [Accessed 19 February 2024].\nhelenclu (2024) Database normalization description - Microsoft 365 Apps. 6 June 2024 [online]. Available from: https://learn.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description [Accessed 11 August 2024].\nHerres, B. and Schmitz, H. (2021) Decomposition of university course timetabling: A systematic study of subproblems and their complexities. Annals of Operations Research [online]. 302 (2), pp.¬†405‚Äì423. Available from: https://ezproxy.uwe.ac.uk/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&AN=150973572&site=ehost-live [Accessed 28 July 2024].\nHolm, D.S., Mikkelsen, R.√ò., S√∏rensen, M. and Stidsen, T.J.R. (2022) A graph-based MIP formulation of the International Timetabling Competition 2019. Journal of Scheduling [online]. 25 (4), pp.¬†405‚Äì428. Available from: https://doi.org/10.1007/s10951-022-00724-y [Accessed 2 November 2023].\nHutson, G. and Jackson, M. (2023) Graph Data Modeling in Python [online]. Packt Publishing. [Accessed 24 December 2023].\nJohnson, D. (1993) A Database Approach to Course Timetabling. The Journal of the Operational Research Society [online]. 44 (5), pp.¬†425‚Äì433. Available from: https://www.jstor.org.ezproxy.uwe.ac.uk/stable/2583909 [Accessed 28 July 2024].\nKhan, W., Kumar, T., Zhang, C., Raj, K., Roy, A.M. and Luo, B. (2023) SQL and NoSQL Database Software Architecture Performance Analysis and Assessments‚ÄîA Systematic Literature Review. Big Data and Cognitive Computing [online]. 7 (2), p.¬†97. Available from: https://www.mdpi.com/2504-2289/7/2/97 [Accessed 28 July 2024].\nLee, V., Nguyen, P.K. and Thomas, A. (2023) Graph-Powered Analytics and Machine Learning with TigerGraph [online]. [Accessed 4 November 2023].\nLemos, A., Melo, F.S., Monteiro, P.T. and Lynce, I. (2019) Room usage optimization in timetabling: A case study at Universidade de Lisboa. Operations Research Perspectives [online]. 6, p.¬†100092. Available from: https://linkinghub.elsevier.com/retrieve/pii/S2214716018301696 [Accessed 26 January 2024].\nLindahl, M., Mason, A.J., Stidsen, T. and S√∏rensen, M. (2018) A strategic view of University timetabling. European Journal of Operational Research [online]. 266 (1), pp.¬†35‚Äì45. Available from: https://www.sciencedirect.com/science/article/pii/S0377221717308433 [Accessed 28 July 2024].\nMandal, A.K. (2020) Development of an Interactive Tool based on Combining Graph Heuristic with Local Search for Examination Timetable Problem. International Journal of Advanced Computer Science and Applications [online]. 11 (3). Available from: https://www.proquest.com/docview/2655156280/abstract/33CF4A9244324D32PQ/1 [Accessed 18 July 2024].\nMirHassani, S.A. and Habibi, F. (2013) Solution approaches to the course timetabling problem. Artificial Intelligence Review [online]. 39 (2), pp.¬†133‚Äì149. Available from: http://link.springer.com/10.1007/s10462-011-9262-6 [Accessed 26 January 2024].\nM√ºhlenthaler, M. and Wanka, R. (2016) Fairness in academic course timetabling. Annals of Operations Research [online]. 239 (1), pp.¬†171‚Äì188. Available from: https://doi.org/10.1007/s10479-014-1553-2 [Accessed 3 February 2024].\nM√ºller, T. and Murray, K. (2010) Comprehensive approach to student sectioning. Annals of Operations Research [online]. 181 (1), pp.¬†249‚Äì269. Available from: http://link.springer.com/10.1007/s10479-010-0735-9 [Accessed 26 January 2024].\nNan 1, Z., Bai, X. 1 1 C. of I. and Economics, T.Y. (2019) The study on data migration from relational database to graph database. [online]. Available from: https://www.proquest.com/docview/2568058349?pq-origsite=primo [Accessed 4 November 2023].\nNegro, A. (2021) Graph-Powered Machine Learning [online]. O‚ÄôReilly Media, Inc.¬†[Accessed 4 November 2023].\nNeo4j (2023) The Neo4j Cypher Manual v5.\nNguyen, V.D. and Nguyen, T. (2021) An SHO-based approach to timetable scheduling: a case study. Journal of Information and Telecommunication [online]. 5 (4), pp.¬†421‚Äì439. Available from: https://doi.org/10.1080/24751839.2021.1935644 [Accessed 2 November 2023].\nNorman, R. and Williams, E. (no date) PSP Board Pack 220804 v1.3.pptx [online]. Available from: https://uweacuk-my.sharepoint.com/:p:/g/personal/richard2_norman_uwe_ac_uk/EWKNTqInQuRDoSaPmHCOp20B72Dp_2vYpJ__GbCzaY5tiA?email=Petter.Lovehagen%40uwe.ac.uk&e=4%3AwEUjpu&fromShare=true&at=31&CID=256ab09c-758c-97f4-07dd-ff61808257cf [Accessed 19 February 2024].\nOude Vrielink, R.A., Jansen, E.A., Hans, E.W. and Van Hillegersberg, J. (2019) Practices in timetabling in higher education institutions: a systematic review. Annals of Operations Research [online]. 275 (1), pp.¬†145‚Äì160. Available from: http://link.springer.com/10.1007/s10479-017-2688-8 [Accessed 2 November 2023].\nRies, E. (2024) What Is an MVP? Eric Ries Explains Lean Startup Co.¬†28 February 2024 [online]. Available from: https://leanstartup.co/resources/articles/what-is-an-mvp/ [Accessed 11 August 2024].\nRudov√°, H., M√ºller, T. and Murray, K. (2011) Complex university course timetabling. Journal of Scheduling [online]. 14 (2), pp.¬†187‚Äì207. Available from: http://link.springer.com/10.1007/s10951-010-0171-3 [Accessed 26 January 2024].\nSanchez, C.A. (2015) An analytics based architecture and methodology for collaborative timetabling in higher education - ProQuest [online]. Available from: https://www.proquest.com/docview/1779550151?pq-origsite=primo&parentSessionId=GLad2hOIbVrF%2F0eiOKHGi%2BO%2BFOyV9GXuQTQCxSfgWNw%3D&sourcetype=Dissertations%20&%20Theses [Accessed 25 January 2024].\nScifo, E. (2023) Graph Data Science with Neo4j [online]. [Accessed 4 November 2023].\nSokolova, Marina V., Francisco J. G√≥mez, and Larisa N. Borisoglebskaya. ‚ÄòMigration from an SQL to a Hybrid SQL/NoSQL Data Model‚Äô. Journal of Management Analytics 7, no. 1 (March 2020): 1‚Äì11. https://doi.org/10.1080/23270012.2019.1700401.\nThomas, J.J., Khader, A.T. and Belaton, B. (2009) Visualization Techniques on the Examination Timetabling Pre-processing Data. In: Imaging and Visualization 2009 Sixth International Conference on Computer Graphics [online]Imaging and Visualization 2009 Sixth International Conference on Computer Graphics. pp.¬†454‚Äì458. Available from: https://ieeexplore.ieee.org/document/5298764 [Accessed 25 January 2024].\nWebber, J., Eifrem, E. and Robinson, I. (2013) Graph Databases [online]. [Accessed 4 November 2023].\nWikipedia contributors (2024) Cypher (query language) ‚Äî Wikipedia, the free encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=Cypher_(query_language)&oldid=1219736900.\nWikipedia contributors (2024) NP-hardness ‚Äî Wikipedia, the free encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=NP-hardness&oldid=1236371945.\nWikipedia contributors (2024) SQL Wikipedia, The Free Encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=SQL&oldid=1238737606 [Accessed 11 August 2024].",
    "crumbs": [
      "Home",
      "Appendix & References",
      "References"
    ]
  },
  {
    "objectID": "01-01a-introduction.html",
    "href": "01-01a-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "University timetabling is the process of scheduling resources within the constraints of an academic institution and calendar. At its core, it involves collecting and combining time slots, rooms, students, and other resources while satisfying a multitude of constraints and preferences to achieve a viable outcome.\nHowever, the reality of timetabling is far more complicated than this simple definition suggests.\nTimetablers must juggle numerous hard constraints (e.g., room capacities, pre-assigned times) and soft constraints (e.g., staff preferences, student travel times) to reach a workable solution. The scale of this task, combined with interdependencies between scheduling decisions, makes university timetabling one of the most challenging administrative tasks in higher education (de Werra, 1997).\nTimetables can make or break a university - they shape the daily experiences of students and staff, influence resource utilisation, and play a significant role in institutional efficiency. The complexity of timetabling stems from various factors:\n\nScale: Tens of thousands of students and activities, and limited resources create a logistical nightmare.\nConstraints: Juggling hard limits (room capacities) and soft preferences (College desires) is a constant balancing act.\nInterdependencies: Changes in one part of the schedule can have cascading effects throughout the entire timetable.\nDiversity of Needs: Different organisational units (colleges, faculties, schools, departments) have varying requirements and preferences.\nOptimisation Goals: Timetablers must balance efficiency, fairness, and quality of education.\n\nWhile traditional studies on ‚Äútimetabling‚Äù focus heavily on generating or optimising feasible timetables (Bonutti et al., 2012; Ceschia, Di Gaspero and Schaerf, 2023; Rudov√°, M√ºller and Murray, 2011) ‚Äì ensuring no clashes or rule violations ‚Äì this project explores a different facet: how analysing scheduled timetables can lead to deeper insights and ultimately, improved quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html",
    "href": "01-03-good-timetable.html",
    "title": "What is a ‚Äúgood‚Äù timetable?",
    "section": "",
    "text": "One of the most challenging aspects of university timetabling is defining what constitutes a ‚Äúgood‚Äù timetable. Despite best efforts, it is virtually impossible to deliver universal satisfaction from a university timetable. The quality of a timetable is inherently subjective and varies among stakeholders depending on their preferences and the demands on their time.\nBased on surveys across various institutions, students typically prioritise (Dowland, 2018; Norman, 2022):\nThe above are easy-to-measure deliverables but they do not address what a ‚Äògood‚Äô timetable should look like; individual stakeholders often have conflicting priorities:\nThis divergence in preferences and the complex interplay of constraints make it challenging to define and achieve a universally ‚Äúgood‚Äù timetable (Lindahl et al., 2018). It is this complexity that sets the stage for this project.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html#consider-these-timetables",
    "href": "01-03-good-timetable.html#consider-these-timetables",
    "title": "What is a ‚Äúgood‚Äù timetable?",
    "section": "Consider these timetables:",
    "text": "Consider these timetables:\n\nThe first timetable is evenly spread over five days.\nThe second timetable has two days free of activities.\nThe third timetable has activities on five days, with gaps.\n\nWhich timetable is better? Is any of them ‚Äògood‚Äô? The answer is it depends! or none of them!\n(Click to enlarge)\n\n\n\n\n\n\n\nEvenly spread over 5 days. ¬†Tuesday afternoons are heavily scheduled; activities take place over lunch\n\n\nTwo days free of activity (Wednesday and Thursday). ¬†Monday has a single activity at 18:00-19:00\n\n\nActivities on five days. ¬†There are large gaps between activities on Tuesday and Wednesday.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "02-02c-expand.html",
    "href": "02-02c-expand.html",
    "title": "Model Expansion",
    "section": "",
    "text": "The true power of the graph model lies in its extensibility. Introducing additional nodes and properties allows for a more comprehensive representation and enables more sophisticated analysis. The resulting graph model will depend on desired use cases and performance requirements but the following are some potential expansions to the basic model:\n\nPotential Expansions:\n\nOrganisational Units: Include departments, colleges, or schools to analyse timetabling within organisational structures.\nCurriculum Data: Incorporate modules and programmes to understand the interconnectedness of courses and student enrolment patterns.\nActivity Types: Differentiate between lectures, seminars, labs, etc., for a more granular analysis of teaching and learning activities.\nActivity Delivery: Understand teaching delivery (virtual, in-person, hybrid, drop-in).\nStudent Attributes: Add properties like ‚Äúinternational student‚Äù, ‚Äúreasonable adjustment flag‚Äù, ‚Äúfirst-year student‚Äù.\n\nThe below image (click to enlarge) shows a graph model augmented with additional data contained within the timetable database. It is much richer and therefore more complex, but this allows for richer analysis.\n\n\n\nExample of Expanded Timetable Graph Model",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Model Expansion"
    ]
  },
  {
    "objectID": "02-02c-expand.html#towards-richer-insights",
    "href": "02-02c-expand.html#towards-richer-insights",
    "title": "Model Expansion",
    "section": "",
    "text": "The true power of the graph model lies in its extensibility. Introducing additional nodes and properties allows for a more comprehensive representation and enables more sophisticated analysis. The graph model will depend on desired use cases and performance requirements.\n\n\n\nOrganisational Units: Include departments, colleges, or schools to analyse timetabling within organisational structures.\nCurriculum Data: Incorporate modules and programmes to understand the interconnectedness of courses and student enrolment patterns.\nActivity Types: Differentiate between lectures, seminars, labs, etc., for a more granular analysis of teaching and learning activities.\nActivity Delivery: Understand teaching delivery (virtual, in-person, hybrid, drop-in).\nStudent Attributes: Add properties like ‚Äúinternational student‚Äù or ‚Äúfirst-year student‚Äù to explore potential student clusters.\n\nThe below image (click to enlarge) shows a graph model augmented with additional data contained within the timetable database. It is much richer and therefore more complex, but this allows for richer analysis.\n\n\n\nExample of Expanded Timetable Graph Model",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Model Expansion"
    ]
  },
  {
    "objectID": "03-03-config.html",
    "href": "03-03-config.html",
    "title": "Configuration and Logging",
    "section": "",
    "text": "Configuration and logging are essential components of the ETL pipeline. Config allows the user to manage different aspects of the ETL pipeline, while logging provides a record of the pipeline‚Äôs execution. They emerged from initial design and from discovering during development.\n\nMain Configuration options\n\nConfiguration parameters are centralised in Python scripts.\nThe design primarily aims for automatic and dynamic operation with well-structured data, but includes override options.\nA YAML file (Appendix-config) holds configuration options, including general settings for filtering data extraction, dynamic folder/filepath creation, and secure credential storage.\nConfig also controls options for validation, data augementation and graph structures (nodes, relationships) to be created.\n\n\n\nLogging\n\nEach module has its own log file with customisable log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\nTiming function which tracks and stores various execution and elapsed times with a view to optimising performance or identifying bottlenecks.\n\n\nExample Extract Log\nSetting up logger: extract\nLogger extract setup completed with 1 handlers.\nSetting up logger: process\nLogger process setup completed with 1 handlers.\nSetting up logger: load\nLogger load setup completed with 1 handlers.\n2024-07-04 11:26:01,124 - INFO - extract_main - Starting data extraction process\n2024-07-04 11:26:01,124 - INFO - extract_main - Output Directory: C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract\n2024-07-04 11:26:01,124 - INFO - extract_main - Hostkeys: ['INB112']\n2024-07-04 11:26:01,124 - INFO - extract_main - Chunksize: 20000\n2024-07-04 11:26:01,124 - INFO - extract_data - Starting data extraction process...\n2024-07-04 11:26:01,284 - INFO - Connected to SQL Server database.\n2024-07-04 11:26:01,284 - INFO - temp_table_loader - Creating global temporary tables...\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Processing SQL file: node-activity-by-pos-temp.sql\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Extracting data for node-activity-by-pos-temp with hostkey: INB112\nC:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\extract_sql_file.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  for chunk in pd.read_sql_query(query, conn, chunksize=CHUNK_SIZE):\n2024-07-04 11:26:07,960 - INFO - extract_sql_file - Saved 631 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activity-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Query for node-activity-by-pos-temp, INB112 took 0.09 seconds\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Processing SQL file: node-activityType-by-pos-temp.sql\n2024-07-04 11:26:07,968 - INFO - extract_sql_file - Extracting data for node-activityType-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Saved 16 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activityType-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Query for node-activityType-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Processing SQL file: node-dept-all.sql\n2024-07-04 11:26:07,987 - INFO - extract_sql_file - Extracting data for node-dept-all with hostkey: INB112\n2024-07-04 11:26:07,990 - INFO - extract_sql_file - Saved 24 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-dept-all_INB112_1.csv\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Query for node-dept-all, INB112 took 0.01 seconds\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Processing SQL file: node-module-by-pos-temp.sql\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Extracting data for node-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Saved 42 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Query for node-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Processing SQL file: node-pos-by-pos-temp.sql\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Extracting data for node-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Saved 8 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Query for node-pos-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Processing SQL file: node-room-by-pos-temp.sql\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Extracting data for node-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Saved 44 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Query for node-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Processing SQL file: node-staff-by-pos-temp.sql\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Extracting data for node-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Saved 33 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Query for node-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Processing SQL file: node-student-by-pos-temp.sql\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Extracting data for node-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Saved 206 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Query for node-student-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Processing SQL file: rel-activity-module-by-pos-temp.sql\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Extracting data for rel-activity-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Saved 168 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Query for rel-activity-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Processing SQL file: rel-activity-room-by-pos-temp.sql\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Extracting data for rel-activity-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Saved 611 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Query for rel-activity-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Processing SQL file: rel-activity-staff-by-pos-temp.sql\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Extracting data for rel-activity-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Saved 868 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Query for rel-activity-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Processing SQL file: rel-activity-student-by-pos-temp.sql\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Extracting data for rel-activity-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Saved 13423 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Query for rel-activity-student-by-pos-temp, INB112 took 0.12 seconds\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Processing SQL file: rel-mod-pos-by-pos-temp.sql\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Extracting data for rel-mod-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Saved 82 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-mod-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Query for rel-mod-pos-by-pos-temp, INB112 took 0.00 seconds\n2024-07-04 11:26:08,265 - INFO - extract_data - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Extraction Time Summary:\n2024-07-04 11:26:08,285 - INFO - extract_main - Function load_temp_tables took 6.59 seconds\n2024-07-04 11:26:08,285 - INFO - extract_main - Function main took 7.17 seconds\n\n\n\nExample Google Drive Log\n2024-07-11 13:28:22,339 - INFO - gdrive_upload - Starting Google Drive upload process.\n2024-07-11 13:28:22,789 - INFO - gdrive_upload - Found existing folder: INB112\n2024-07-11 13:28:23,115 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:23,366 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:23,576 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:24,810 - INFO - gdrive_upload - File node-activity-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:25,046 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:26,170 - INFO - gdrive_upload - File node-activityType-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:26,394 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:27,495 - INFO - gdrive_upload - File node-department-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:27,715 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:28,970 - INFO - gdrive_upload - File node-module-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:29,206 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:30,294 - INFO - gdrive_upload - File node-programme-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:30,515 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:31,595 - INFO - gdrive_upload - File node-room-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:31,846 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:32,895 - INFO - gdrive_upload - File node-staff-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:33,128 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:34,220 - INFO - gdrive_upload - File node-student-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:34,220 - WARNING - gdrive_upload - Skipping file with unrecognized prefix: process.log\n2024-07-11 13:28:34,416 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:35,565 - INFO - gdrive_upload - File rel-activity_activityType-processed.csv uploaded to Google Drive folder ID: 1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\n\n\n\nExample Process Log\n2024-07-11 13:31:24,284 - INFO - process_utils - Processing relationship data\n2024-07-11 13:31:24,284 - INFO - process_utils - Config: {'filename_pattern': 'rel-mod-pos-by-pos-temp_INB112*.csv', 'node1_col': 'modSplusID', 'node2_col': 'posSplusID', 'relationship': 'BELONGS_TO', 'properties': ['modType']}\n2024-07-11 13:31:24,285 - INFO - process_utils - Saving processed relationship file for: module_programme\n2024-07-11 13:31:24,289 - INFO - process_main - Function process_department took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_module took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_room took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_programme took 0.01 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activityType took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_staff took 1.82 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_student took 8.76 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activity took 0.10 seconds\n\n\n\nExample Load Log\n2024-07-11 16:28:45,220 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:28:45,220 - INFO - load_main - Total execution time: 0.51 seconds\n2024-07-11 16:33:31,166 - INFO - connect_to_neo4j_db - Connected to Neo4j database successfully! Driver: \n2024-07-11 16:33:31,601 - INFO - load_relationships - Found 1 relationship files in Google Drive.\n2024-07-11 16:33:31,604 - INFO - load_relationships - Processing file: rel-activity_room-processed.csv (ID: 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v)\n2024-07-11 16:33:31,605 - INFO - google_drive_utils - Downloading CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v from Google Drive.\n2024-07-11 16:33:35,665 - INFO - google_drive_utils - Downloaded CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v.\n2024-07-11 16:33:45,428 - INFO - load_relationships - Finished loading activity_room relationships:\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Total rows processed: 611\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Relationships created: 1\n2024-07-11 16:33:45,431 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:33:45,432 - INFO - load_main - Total execution time: 14.34 seconds",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Configuration and Logging"
    ]
  },
  {
    "objectID": "04-timetable-metrics.html",
    "href": "04-timetable-metrics.html",
    "title": "Timetable Metrics",
    "section": "",
    "text": "intro / segue into timetable metrics.\nlink to cypher queries for basic analysis, basic understanding\nbut to understand quality and measurable quality, we need to be able to define what we mean\nmy idea is to develop a timetable metric, index which is quantifiable and measurable and comparable\nbut also flexible and adaptable to organisational needs and priorities - it is tweakable\ndescribe general idea of penalties and positivies to get a score - details are to be determined\nbut the general concept is that every student (or staff, or programme) - timetabling object can have a quality score/measure\nfor simplicity, we can imagine starting with a score of 100 - which is either neutral. the score can increase (with positive metrics) and decrease when penalised.\n\nmentions hard and soft constraints before - these are examples of what would be used to calculate the score\neveryone has a lunch break is either rewarded or not penalised\nno clashes is the minimum, so a clash should be a penalty\nlong days are a penalty etc.\ncan add other datasets like room locations to calculate distances and travel time (see cypher page)",
    "crumbs": [
      "Home",
      "Timetable Metrics"
    ]
  },
  {
    "objectID": "04-timetable-metrics.html#timetable-quality-metrics-and-insights",
    "href": "04-timetable-metrics.html#timetable-quality-metrics-and-insights",
    "title": "Timetable Metrics",
    "section": "Timetable Quality Metrics and Insights",
    "text": "Timetable Quality Metrics and Insights\n\n4.1 Defining Timetable Quality\n\n\n4.2 Implemented Metrics\n\nConstraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n4.3 Aggregation Methods\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n4.4 Cypher Queries for Metric Calculation\n\nExample queries with explanations\n\n\n\n4.5 Visualization of Results\n\nBloom visualisations or other relevant charts",
    "crumbs": [
      "Home",
      "Timetable Metrics"
    ]
  },
  {
    "objectID": "06-conclusion.html",
    "href": "06-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations\n\nThe iterative approach facilitated personal growth, enhanced technical skills, and improved project management capabilities.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Conclusion"
    ]
  },
  {
    "objectID": "06-conclusion.html#conclusion",
    "href": "06-conclusion.html#conclusion",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations\n\nThe iterative approach facilitated personal growth, enhanced technical skills, and improved project management capabilities.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Conclusion"
    ]
  },
  {
    "objectID": "appendix-code.html",
    "href": "appendix-code.html",
    "title": "ETL Code Gists",
    "section": "",
    "text": "Complete code for the Data Pipeline can be found in these Github gists or on the following pages.\nGraph Timetable - Quarto - Config and Misc\nGraph Timetable - Quarto - SQL Queries\nGraph Timetable - Quarto - Extract\nGraph Timetable - Quarto - Transform\nGraph Timetable - Quarto - Upload to Google Drive\nGraph Timetable - Quarto - Load",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "ETL Code"
    ]
  },
  {
    "objectID": "appendix-code2-sql.html",
    "href": "appendix-code2-sql.html",
    "title": "Extract-SQL",
    "section": "",
    "text": "Graph Timetable - Quarto - SQL Queries\n\ncreate_temp_tables.sql\nnode-activity-by-pos-temp.sql\nnode-activityType-by-pos-temp.sql\nnode-dept-all.sql\nnode-module-by-pos-temp.sql\nnode-pos-by-pos-temp.sql\nnode-room-by-pos-temp.sql\nnode-staff-by-pos-temp.sql\nnode-student-by-pos-temp.sql\nrel-activity-module-by-pos-temp.sql\nrel-activity-room-by-pos-temp.sql\nrel-module-programme-by-pos-temp.sql\nrel-staff-activity-by-pos-temp.sql\nrel-student-activity-by-pos-temp.sql",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Extract-SQL"
    ]
  },
  {
    "objectID": "appendix-code4-gdrive.html",
    "href": "appendix-code4-gdrive.html",
    "title": "Google Drive Load",
    "section": "",
    "text": "Graph Timetable - Quarto - Upload to Google Drive\n\ngdrive_upload\ngoogle_drive_utils",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Google Drive Load"
    ]
  },
  {
    "objectID": "appendix-code6-load.html",
    "href": "appendix-code6-load.html",
    "title": "Neo4j Load",
    "section": "",
    "text": "Graph Timetable - Quarto - Load\n\nload_schema\nload_utils\nload_nodes\nload_relationships\nconvert_properties\nset_display_properties\nload_main",
    "crumbs": [
      "Home",
      "Appendix & References",
      "ETL Code",
      "Neo4j Load"
    ]
  },
  {
    "objectID": "appendix-cypher1.html",
    "href": "appendix-cypher1.html",
    "title": "Creating Nodes and Relationships",
    "section": "",
    "text": "Creating nodes and relationships is the first step in building a graph database. Nodes represent entities in the graph, such as students, rooms, or activities. Relationships connect nodes and represent the connections between them.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#creating-nodes",
    "href": "appendix-cypher1.html#creating-nodes",
    "title": "Creating Nodes and Relationships",
    "section": "Creating Nodes",
    "text": "Creating Nodes\nNodes are created using the CREATE clause in Cypher. The general syntax for creating a node is:\nCREATE (n:NodeLabel {propertyName: propertyValue, ...})\nWhere:\n\nn is the node variable\nNodeLabel is the label assigned to the node\npropertyName is the property name\npropertyValue is the value assigned to the property\n... represents additional properties\n\n\nExample: Creating a Student Node\nCREATE (s:Student {studentID: '123456', studentName: 'Alice'...})",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#creating-relationships",
    "href": "appendix-cypher1.html#creating-relationships",
    "title": "Creating Nodes and Relationships",
    "section": "Creating Relationships",
    "text": "Creating Relationships\nRelationships are created using the CREATE clause in Cypher. The general syntax for creating a relationship is:\nMATCH (n1:NodeLabel1), (n2:NodeLabel2)\nWHERE n1.propertyName = propertyValue1 AND n2.propertyName = propertyValue2\nCREATE (n1)-[r:RELATIONSHIP_TYPE]-&gt;(n2)\nWhere:\n\nn1 and n2 are the node variables\nNodeLabel1 and NodeLabel2 are the labels assigned to the nodes\npropertyName is the property name\npropertyValue is the value assigned to the property\nr is the relationship variable\nRELATIONSHIP_TYPE is the type of relationship\n-&gt; represents the direction of the relationship\nMATCH is used to find the nodes to connect, optional\nWHERE is used to filter the nodes, optional\nCREATE is used to create the relationship\n... represents additional properties\n\n\nExample: Creating a Relationship Between a Student and an Activity\nMATCH (s:Student {studentID: '123456'}), (a:Activity {activityID: '789'})\nCREATE (s)-[r:ATTENDS]-&gt;(a)",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#relationships-created-after-etl",
    "href": "appendix-cypher1.html#relationships-created-after-etl",
    "title": "Creating Nodes and Relationships",
    "section": "Relationships created after ETL",
    "text": "Relationships created after ETL\nThe following relationships were applied to my proof-of-concept graph after loading data using the ETL, using node properties. Delete code also supplied below.\n\n\n\n\n\n\nCaution\n\n\n\nUse code with caution, especially DELETE code. Run pattern matching and investigate results before committing to delete.\n\n\n\n(student)-[REGISTERED_ON]-&gt;(programme)\n\n// Create REGISTERED_ON relationship between student and programme nodes\n\n// Match student and programme nodes based on matching properties\nMATCH (s:student), (p:programme)\nWHERE s.stuProgSplusID = p.posSplusID\n\n// Create REGISTERED_ON relationship\nMERGE (s)-[:REGISTERED_ON]-&gt;(p)\n// Delete REGISTERED_ON relationships\nMATCH (:student)-[r:REGISTERED_ON]-&gt;(:programme)\nDELETE r\n\n\n(student)-[ENROLLED_ON]-&gt;(module)\n// Create ENROLLED_ON relationship between students and modules \n\n// Match student, activity, and module nodes based on ATTEND and BELONGS_TO relationships\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)-[:BELONGS_TO]-&gt;(m:module)\n\n// Create ENROLLED_ON relationship\nMERGE (s)-[:ENROLLED_ON]-&gt;(m)\n// Delete ENROLLED_ON relationships\nMATCH (:student)-[r:ENROLLED_ON]-&gt;(:module)\nDELETE r\n\n\n(activity)-[HAS_TYPE]-&gt;(activity_type)\n// Create HAS_TYPE relationship between activity and activityType nodes\n\n// Match activity and activityType nodes based on matching properties\nMATCH (a:activity), (at:activityType)\nWHERE a.actTypeName = at.actTypeDescription\n\n// Create HAS_TYPE relationship\nMERGE (a)-[:HAS_TYPE]-&gt;(at)\n// Find activities without an activityType\n\nMATCH (a:activity)\nWHERE NOT EXISTS ((a)-[:HAS_TYPE]-&gt;(:activityType))\nRETURN a\n// Delete HAS_TYPE relationships between activity and activityType nodes\n\nMATCH (a:activity)-[r:HAS_TYPE]-&gt;(at:activityType)\nDELETE r\n\n\n(module)-[HAS_OWNING_DEPT]-&gt;(department)\n// Create HAS_OWNING_DEPT relationship between module and department nodes\n\n// Match module and department nodes based on matching properties\nMATCH (m:module), (d:department)\nWHERE m.modDepartment = d.deptName\n\n// Create HAS_OWNING_DEPT relationship\nMERGE (m)-[:HAS_OWNING_DEPT]-&gt;(d)\n// Delete HAS_OWNING_DEPT relationships between module and department nodes\n\nMATCH (m:module)-[r:HAS_OWNING_DEPT]-&gt;(d:department)\nDELETE r\n\n\n(programme)-[HAS_OWNING_DEPT]-&gt;(department)\n// Create HAS_OWNING_DEPT relationship between programme and department nodes\n\n// Match programme and department nodes based on matching properties\nMATCH (p:programme), (d:department)\nWHERE p.posDepartment = d.deptName\n\n// Create HAS_OWNING_DEPT relationship\nMERGE (p)-[:HAS_OWNING_DEPT]-&gt;(d)\n// Delete HAS_OWNING_DEPT relationships between programme and department nodes\n\nMATCH (p:programme)-[r:HAS_OWNING_DEPT]-&gt;(d:department)\nDELETE r",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher3.html",
    "href": "appendix-cypher3.html",
    "title": "General Queries",
    "section": "",
    "text": "This page contains a selection general queries which can be used to explore the graph database. The queries are designed to provide insights into the data and relationships between nodes.\n\nList all nodes\nThe following query lists all nodes in the graph.\n\n\n\n\n\n\nCaution\n\n\n\nConsider size of graph and limits in settings before running this query.\n\n\nMATCH (n)\nRETURN n\n\n\n\nAll Nodes\n\n\n\n\ndatatype of property\nProperties in a graph have datatypes which will enable different operations and there for insights. The query below returns the datatype of a node property.\n/* return datatype of actStartTime on activity node */\n\nMATCH (a:activity)\nRETURN DISTINCT apoc.meta.cypher.type(a.actStartTime) as actStartTimeType\n\n\n\nDatatype of Property\n\n\n\n\nunique properties\nA node or relationship can potentially have many properties. The query below lists the properties of a node - in this case, activity.\n// List unique properties for a Node\n\nMATCH (a:activity)\nUNWIND keys(a) AS propertyKey\nRETURN COLLECT(DISTINCT propertyKey) AS propertyKeys\n//RETURN DISTINCT propertyKey as propertyKeys\n\n\n\nUnique Property Values\n\n\n\n\nnode labels without relationships\nGraph databases are all about the relationships between nodes. It can be useful identifying nodes without relationships as they could indicate a problem with the data, data loading mechanism or be the outliers you want to identify.\nFor example, in a timetabling scenario, we would expect all nodes to be related to another node. However, we can see that several node labels are orphans. In the proof-of-concept, these results are expected or deliberate, due to the source data.\n// Find node labels without relationships and their count\n\nMATCH (n)\nWHERE NOT EXISTS(()-[]-(n)) AND NOT EXISTS((n)-[]-())\nRETURN DISTINCT labels(n) AS nodeLabels, count(n) AS nodeCount\n\n\n\nNode labels without Relationships\n\n\n\n\nnodes without relationships - aka orphans\nInstead of returning a count of nodes without relationships per node label we can return the nodes as a graph or a table:\n// Find nodes without relationships\n\nMATCH (n)\nWHERE NOT EXISTS(()-[]-(n)) AND NOT EXISTS((n)-[]-())\nRETURN n\n\n\n\nNodes Without Relationships\n\n\n\n\nstudents without activities\nIn the timetabling context, we would expect students to be allocated to activities. It turns out that we have 219 students without activities. A bit more investigation indidates that they are all from a particular programme of study run.\n// Students without Activities\nMATCH (s:student)\nWHERE NOT (s)-[:ATTENDS]-&gt;()\nRETURN s\n\n\n\nStudents Without Activities\n\n\n\n\nactivityType without activity\nActivities can have a activity type - the graph model could have activity type as a property or as a relationship. The query below finds activity typewithout activity. The decision may be to delete these orphaned nodes as they may cause problems with some calculations. They would be created if they become required in the future.\n// Activities without Rooms\n\nMATCH (at:activityType)\nWHERE NOT (at)&lt;-[:HAS_TYPE]-()\nRETURN at;\n\n\n\nActivityType Without Room\n\n\n\n\nactivities without rooms\nThe graph has over 1500 activity instances without rooms. Most of these will be deliberate - online, virtual sessions - but we may want to query the graph to identify those where a room is expected.\n// Activities without Rooms\n\nMATCH (a:activity)\nWHERE NOT (a)-[]-&gt;(:room)\nRETURN a;\n\n\n\nActivities Without Rooms",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "General Queries"
    ]
  },
  {
    "objectID": "appendix-supervision.html",
    "href": "appendix-supervision.html",
    "title": "Supervision",
    "section": "",
    "text": "Project supervision was undertaken by Dr.¬†Xiaodong Li who was very supportive and made himself available to me in-person and by email, etc.\nIn general, we met for 30-60 minutes every fortnight where I presented progress, issues/blockers, planned next steps, etc. and we discussed the project in general. Dr.¬†Li was very engaged and helpful and I am indebted to his supervision. Following each meeting, I wrote up notes.\nI have shared a few examples in the following pages. Please note that links within these pages will not necessarily work as they point to working files and repositories.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Supervision"
    ]
  },
  {
    "objectID": "appendix-supervision2.html",
    "href": "appendix-supervision2.html",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "",
    "text": "Significant progress in the backend and more proof-of-concept in front end.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#summary",
    "href": "appendix-supervision2.html#summary",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "",
    "text": "Significant progress in the backend and more proof-of-concept in front end.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#accomplishments",
    "href": "appendix-supervision2.html#accomplishments",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nProject Management\n\npost supervisor meeting notes\n\nData Collection:\n\nAnonymisation function for staff/student personal data developed and tested.\nMSc Data Science cohort isolated into separate csv files\nData extraction pipeline designed, developed, tested. This allows for extracting data filtered by programme on demand.\n\nmodularised, scalable, configurable, efficient pipeline\n\nTransformation pipeline (preprocessing, anonymising)\n\nMore than half way completed - same principles\nneed to add staff, student, activity nodes and test outcomes\nneed to add relationships and test.\n\nLoading to Neo4j pipeline developed - suitable for version 1\n\nAnalysis / Wrangling:\n\nMore advanced cypher queries - constraint violation queries developed using cypher for version 1. some are very complex. more testing needed.\nlist of insights which can be derived\n\nModel Development:\n\nComparing different representations of time\n\nResults:\nPipeline development\nCypher queries for version 1\nconversation with business owners to validate work",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#next-steps",
    "href": "appendix-supervision2.html#next-steps",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goal: What is goal of next fortnight?\n\nwrite up and benchmark v1 notes\nfinish developing and testing pipeline:\n\nextract\ntransform\nload\n\ndocument pipeline - written and visual (mermaid?)\ndevelop cypher queries for version 2\ndesign (theory) timetable quality index\nconsider scaling dataset\nstart writing project notes",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#issuesblockers",
    "href": "appendix-supervision2.html#issuesblockers",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nDigital certificates on machines preventing load\nNeo4j Aura (free) limitations - loading issues\n\nMethodological: Concerns about approach or analysis?\n\nSpending a lot of time getting ETL ‚Äòright‚Äô\nNot sure about balance of project and what I will deliver at the end\n\nData-Related: Issues with data quality, access, or quantity?\n\nWorking on pipelines which will mean I can scale accordingly (within constraints of free instance)",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#post-meeting-notes",
    "href": "appendix-supervision2.html#post-meeting-notes",
    "title": "Fortnightly Update - 2024-06-24",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\nMy superviser and I spoke about where I am at in the project at the moment and next steps. We discussed what I am attempting to do and why, including graph data structures, proof-of-concept, etc. and that it is becoming a data engineering project. I stated that my timeline lookes to complete a robust data ETL pipeline by the middle of July, with a view to shifting towards more work within Neo4j from the end of July to the middle of August.\nWe agreed to meet in two weeks.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-tech-stack.html",
    "href": "appendix-tech-stack.html",
    "title": "Technology Stack",
    "section": "",
    "text": "This project used a variety of tools, applications, programming languages, and so on. Below is a high-level record of the ‚Äòtech stack‚Äô - the what and why:\n\nProgramming\n\nPython - Main programming language.\nSQL - SELECT queries to extract source data from relational database.\nCypher - Querying language for Neo4j Graph Databases.\nBatch - Windows command language to handle yaml files and multi-format rendering.\nVSCode - Main IDE (Integrated Development Environment).\n\n\n\nDocumentation\n\nQuarto - Open-source technical publishing system.\nJupyter - Open-source application for interactive notebooks.\nZotero - Open-source reference management system.\n\n\n\nVisualisation\n\nGraphviz - Open-source graph visualisation application.\nMermaid - Open-source JavaScript diagramming tool.\nArrows - Neo4j Labs diagramming tool.\n\n\n\nVersioning\n\nGithub - Web-based platform for version control and collaboration using Git.\n\n\n\nPython Libraries\nSeveral Python libraries were explored in the development of this prototype. The below libraries are the ones used in the current implementation.\n\nDirectory/File Handling\n\nos - Interacting with the operating system for tasks like creating, deleting, and navigating directories and files.\nglob - Finding files and directories based on pattern matching.\nio - Working with input/output streams for reading and writing data.\n\n\n\nData Handling\n\npandas - Handling tabular data for analysis and manipulation.\njson - Encoding and decoding JSON (JavaScript Object Notation) data.\n\n\n\nTyping and Logging\n\ntyping - Adding type hints to code for better code readability, maintainability, and static type checking.\nlogging - Configuring and managing logging for application.\ntime - Working with time-related functions, potentially used for logging timestamps.\n\n\n\nDatabase Connectivity\n\nkeyring - Securely storing and retrieving passwords and other sensitive information.\npyodbc - Connecting to and interacting with SQL databases using the Open Database Connectivity (ODBC) standard.\nneo4j - Interacting with Neo4j graph databases.\n\n\n\nGoogle API Integration\n\ngoogleapiclient - Interacting with various Google APIs.\ngoogle.oauth2 - Handling OAuth 2.0 authentication for accessing Google services securely.\n\n\n\nAnonymisation\n\nrandom - Generating random numbers and making random choices.\nhashlib - Implementing various secure hash and message digest algorithms.\nFaker - Generating fake data for testing and development purposes.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Technology Stack"
    ]
  },
  {
    "objectID": "01-04-project.html",
    "href": "01-04-project.html",
    "title": "Project Aims and Scope",
    "section": "",
    "text": "The primary aim of this project is to investigate the viability of using graph data structures for enhanced timetabling analytics and reporting.\n\nObjectives include:\n\nDesigning an extensible, system-agnostic graph data model for university timetables\nDeveloping a configurable ETL (extract, transform, load) pipeline to transition from relational to graph database representations of timetables\nDiscussing how graph-based approaches to timetabling analysis could contribute to measuring and improving timetable quality.\n\nIn order to achieve these objectives, I implement and evaluate a set of proof-of-concept analytical metrics which leverage the graph data model whilst discussing performance capabilities (and limitations) against traditional relational approaches.\nIt is important to note that this project is positioned as a proof-of-concept and exploratory study.\nI will not:\n\nreinvent a full-scale timetabling system\nattempt to optimise real-time timetable generation\n\nI will:\n\nfocus on the data engineering aspects of transitioning from relational to graph data models\ndemonstrate the potential of graph-based approaches in the analysis of university timetables\nprovide a foundation for future analytical work.\n\nLet‚Äôs graph!\n\n\n\n\n\n\n\nG\n\n\n\n4zj99lz2\n\n\n\n\n666n359b\n\n\n\n\nu0upp5hj\n\n\n\n\n666n359b-&gt;u0upp5hj\n\n\n\n\n\n4s3xzocl\n\n\n\n\nk8yaaeoi\n\n\n\n\n4s3xzocl-&gt;k8yaaeoi\n\n\n\n\n\n9bbhq0e8\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n5cymetka\n\n\n\n\ne4qukjnm\n\n\n\n\n5cymetka-&gt;e4qukjnm\n\n\n\n\n\nhx0hp6dr\n\n\n\n\nhx0hp6dr-&gt;hx0hp6dr\n\n\n\n\n\nk8yaaeoi-&gt;k8yaaeoi\n\n\n\n\n\n14ae1pfz\n\n\n\n\nk8yaaeoi-&gt;14ae1pfz\n\n\n\n\n\njrtct42d\n\n\n\n\nglv43ych\n\n\n\n\nxpzdirk1\n\n\n\n\nglv43ych-&gt;xpzdirk1\n\n\n\n\n\n06x4bmzo\n\n\n\n\nglv43ych-&gt;06x4bmzo\n\n\n\n\n\nmis1btmm\n\n\n\n\nglv43ych-&gt;mis1btmm\n\n\n\n\n\nce1516j0\n\n\n\n\nglv43ych-&gt;ce1516j0\n\n\n\n\n\ngzxrlpod\n\n\n\n\nc114j2tw\n\n\n\n\ngzxrlpod-&gt;c114j2tw\n\n\n\n\n\n8bpimk72\n\n\n\n\n8bpimk72-&gt;k8yaaeoi\n\n\n\n\n\n8bpimk72-&gt;14ae1pfz\n\n\n\n\n\nur4yo3tx\n\n\n\n\n8bpimk72-&gt;ur4yo3tx\n\n\n\n\n\nonkctu6x\n\n\n\n\nonkctu6x-&gt;onkctu6x\n\n\n\n\n\n85hi4ca2\n\n\n\n\nonkctu6x-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep\n\n\n\n\nonkctu6x-&gt;x0n5twep\n\n\n\n\n\nc1sdjw8j\n\n\n\n\nc1sdjw8j-&gt;hx0hp6dr\n\n\n\n\n\nc1sdjw8j-&gt;u0upp5hj\n\n\n\n\n\nxh1xin78\n\n\n\n\nxztqibcz\n\n\n\n\nxh1xin78-&gt;xztqibcz\n\n\n\n\n\nvr6qql5z\n\n\n\n\nvr6qql5z-&gt;666n359b\n\n\n\n\n\nwxhhelge\n\n\n\n\nvr6qql5z-&gt;wxhhelge\n\n\n\n\n\n3ytehise\n\n\n\n\nvr6qql5z-&gt;3ytehise\n\n\n\n\n\n2m5wokx2\n\n\n\n\nmfp9is99\n\n\n\n\n2m5wokx2-&gt;mfp9is99\n\n\n\n\n\nv4x5duoh\n\n\n\n\nv4x5duoh-&gt;2m5wokx2\n\n\n\n\n\nfovglxww\n\n\n\n\nv4x5duoh-&gt;fovglxww\n\n\n\n\n\nv4x5duoh-&gt;xztqibcz\n\n\n\n\n\n89w9dxkj\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n1chkeldv\n\n\n\n\n89w9dxkj-&gt;1chkeldv\n\n\n\n\n\n89w9dxkj-&gt;mis1btmm\n\n\n\n\n\nxpzdirk1-&gt;gzxrlpod\n\n\n\n\n\nfovglxww-&gt;fovglxww\n\n\n\n\n\nfsp8cdjo\n\n\n\n\nfovglxww-&gt;fsp8cdjo\n\n\n\n\n\nfovglxww-&gt;mfp9is99\n\n\n\n\n\njnazmo3s\n\n\n\n\njnazmo3s-&gt;e4qukjnm\n\n\n\n\n\n72a2d6xd\n\n\n\n\nsj35t4ss\n\n\n\n\n72a2d6xd-&gt;sj35t4ss\n\n\n\n\n\nlfwuczry\n\n\n\n\n35yrnabr\n\n\n\n\nlfwuczry-&gt;35yrnabr\n\n\n\n\n\n8cgoonso\n\n\n\n\nlfwuczry-&gt;8cgoonso\n\n\n\n\n\nhv8k4g84\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nhcqaqm1o\n\n\n\n\nhcqaqm1o-&gt;666n359b\n\n\n\n\n\nqy17mwag\n\n\n\n\nhcqaqm1o-&gt;qy17mwag\n\n\n\n\n\n53bgisfb\n\n\n\n\nhcqaqm1o-&gt;53bgisfb\n\n\n\n\n\n1k9klz8v\n\n\n\n\neacs5e9j\n\n\n\n\n1k9klz8v-&gt;eacs5e9j\n\n\n\n\n\nj812m8am\n\n\n\n\n1k9klz8v-&gt;j812m8am\n\n\n\n\n\nieb7bdce\n\n\n\n\n1k9klz8v-&gt;ieb7bdce\n\n\n\n\n\nbav7hkue\n\n\n\n\nc114j2tw-&gt;bav7hkue\n\n\n\n\n\ne7injpqb\n\n\n\n\nc114j2tw-&gt;e7injpqb\n\n\n\n\n\nhj4rycy9\n\n\n\n\ns3okac0a\n\n\n\n\nhj4rycy9-&gt;s3okac0a\n\n\n\n\n\n85hi4ca2-&gt;hj4rycy9\n\n\n\n\n\n85hi4ca2-&gt;9bbhq0e8\n\n\n\n\n\nakih91pi\n\n\n\n\n85hi4ca2-&gt;akih91pi\n\n\n\n\n\nwxhhelge-&gt;72a2d6xd\n\n\n\n\n\nwxhhelge-&gt;53bgisfb\n\n\n\n\n\niw0pfpmk\n\n\n\n\nwt9rtovp\n\n\n\n\niw0pfpmk-&gt;wt9rtovp\n\n\n\n\n\ndtmww06j\n\n\n\n\niw0pfpmk-&gt;dtmww06j\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nw084j1ko\n\n\n\n\nx0n5twep-&gt;w084j1ko\n\n\n\n\n\ny0sap20o\n\n\n\n\ny0sap20o-&gt;y0sap20o\n\n\n\n\n\ndxppp23s\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\npveq6y3l\n\n\n\n\ny0sap20o-&gt;pveq6y3l\n\n\n\n\n\nzefyvzdp\n\n\n\n\npzodwhlw\n\n\n\n\nhxejkaog\n\n\n\n\npzodwhlw-&gt;hxejkaog\n\n\n\n\n\n9bbhq0e8-&gt;dxppp23s\n\n\n\n\n\nm0ggeeei\n\n\n\n\n9bbhq0e8-&gt;m0ggeeei\n\n\n\n\n\n06x4bmzo-&gt;c114j2tw\n\n\n\n\n\nv850hpzb\n\n\n\n\n06x4bmzo-&gt;v850hpzb\n\n\n\n\n\n8bx5ks88\n\n\n\n\n06x4bmzo-&gt;8bx5ks88\n\n\n\n\n\nzyz74rqy\n\n\n\n\nzyz74rqy-&gt;hx0hp6dr\n\n\n\n\n\nzyz74rqy-&gt;c1sdjw8j\n\n\n\n\n\n2cicgpkb\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nc21k3g41\n\n\n\n\nzyz74rqy-&gt;c21k3g41\n\n\n\n\n\n6e82t0az\n\n\n\n\nsi800de6\n\n\n\n\nsi800de6-&gt;fovglxww\n\n\n\n\n\nsi800de6-&gt;si800de6\n\n\n\n\n\n4rv1fae9\n\n\n\n\n4rv1fae9-&gt;v4x5duoh\n\n\n\n\n\ndogjcfze\n\n\n\n\n4rv1fae9-&gt;dogjcfze\n\n\n\n\n\n4rv1fae9-&gt;dtmww06j\n\n\n\n\n\n4rv1fae9-&gt;3ytehise\n\n\n\n\n\nqvvjbgxm\n\n\n\n\nqy17mwag-&gt;vr6qql5z\n\n\n\n\n\nkjba91s3\n\n\n\n\nqy17mwag-&gt;kjba91s3\n\n\n\n\n\n9ifdi52l\n\n\n\n\ns3tdzgmu\n\n\n\n\n9ifdi52l-&gt;s3tdzgmu\n\n\n\n\n\n14ae1pfz-&gt;eacs5e9j\n\n\n\n\n\nhxejkaog-&gt;s3tdzgmu\n\n\n\n\n\nlijfw7my\n\n\n\n\nlijfw7my-&gt;4zj99lz2\n\n\n\n\n\neacs5e9j-&gt;k8yaaeoi\n\n\n\n\n\nur4yo3tx-&gt;hxejkaog\n\n\n\n\n\n1tdjps87\n\n\n\n\nur4yo3tx-&gt;1tdjps87\n\n\n\n\n\nl9axxrot\n\n\n\n\nur4yo3tx-&gt;l9axxrot\n\n\n\n\n\nfsp8cdjo-&gt;v4x5duoh\n\n\n\n\n\nfsp8cdjo-&gt;fovglxww\n\n\n\n\n\n9xro7toh\n\n\n\n\n4pocwaxo\n\n\n\n\nakih91pi-&gt;4pocwaxo\n\n\n\n\n\nakih91pi-&gt;3ytehise\n\n\n\n\n\nv850hpzb-&gt;89w9dxkj\n\n\n\n\n\nv850hpzb-&gt;qvvjbgxm\n\n\n\n\n\nv850hpzb-&gt;e7injpqb\n\n\n\n\n\ne4qukjnm-&gt;u0upp5hj\n\n\n\n\n\n1bkcwtuf\n\n\n\n\n1bkcwtuf-&gt;c114j2tw\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;lijfw7my\n\n\n\n\n\ndvhx7p1e\n\n\n\n\n1bkcwtuf-&gt;dvhx7p1e\n\n\n\n\n\nu0upp5hj-&gt;hcqaqm1o\n\n\n\n\n\nu0upp5hj-&gt;wxhhelge\n\n\n\n\n\nu0upp5hj-&gt;c21k3g41\n\n\n\n\n\nj33a36gy\n\n\n\n\nj33a36gy-&gt;9ifdi52l\n\n\n\n\n\nj33a36gy-&gt;j812m8am\n\n\n\n\n\newthkxdz\n\n\n\n\nefvvduxt\n\n\n\n\nefvvduxt-&gt;onkctu6x\n\n\n\n\n\nefvvduxt-&gt;4pocwaxo\n\n\n\n\n\ntwne0skc\n\n\n\n\ntwne0skc-&gt;xpzdirk1\n\n\n\n\n\ntwne0skc-&gt;1bkcwtuf\n\n\n\n\n\n2cicgpkb-&gt;l9axxrot\n\n\n\n\n\nozdu79aw\n\n\n\n\nozdu79aw-&gt;hx0hp6dr\n\n\n\n\n\nozdu79aw-&gt;1tdjps87\n\n\n\n\n\nj812m8am-&gt;j33a36gy\n\n\n\n\n\nsj35t4ss-&gt;72a2d6xd\n\n\n\n\n\nsj35t4ss-&gt;53bgisfb\n\n\n\n\n\n1tdjps87-&gt;8bpimk72\n\n\n\n\n\n1tdjps87-&gt;8cgoonso\n\n\n\n\n\nqcsg1epp\n\n\n\n\nqcsg1epp-&gt;k8yaaeoi\n\n\n\n\n\nkjba91s3-&gt;53bgisfb\n\n\n\n\n\nmis1btmm-&gt;c114j2tw\n\n\n\n\n\nmis1btmm-&gt;efvvduxt\n\n\n\n\n\n35yrnabr-&gt;4zj99lz2\n\n\n\n\n\n35yrnabr-&gt;y0sap20o\n\n\n\n\n\n35yrnabr-&gt;dvhx7p1e\n\n\n\n\n\nwmnsxqhi\n\n\n\n\nwmnsxqhi-&gt;wxhhelge\n\n\n\n\n\ntyoit3iq\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;fovglxww\n\n\n\n\n\ntyoit3iq-&gt;1chkeldv\n\n\n\n\n\n8cgoonso-&gt;j33a36gy\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\nez7k5sb5\n\n\n\n\nez7k5sb5-&gt;72a2d6xd\n\n\n\n\n\nez7k5sb5-&gt;9ifdi52l\n\n\n\n\n\nieb7bdce-&gt;xh1xin78\n\n\n\n\n\n4pocwaxo-&gt;85hi4ca2\n\n\n\n\n\n4pocwaxo-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz\n\n\n\n\n4pocwaxo-&gt;qez5iiiz\n\n\n\n\n\nmfp9is99-&gt;wt9rtovp\n\n\n\n\n\nmfp9is99-&gt;1chkeldv\n\n\n\n\n\nyezs4jbg\n\n\n\n\nyezs4jbg-&gt;tyoit3iq\n\n\n\n\n\nyezs4jbg-&gt;w084j1ko\n\n\n\n\n\nqez5iiiz-&gt;onkctu6x\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\ns3okac0a-&gt;5cymetka\n\n\n\n\n\ns3okac0a-&gt;vr6qql5z\n\n\n\n\n\ns3okac0a-&gt;72a2d6xd\n\n\n\n\n\n1pk6uryz\n\n\n\n\n1pk6uryz-&gt;xh1xin78\n\n\n\n\n\n1pk6uryz-&gt;2m5wokx2\n\n\n\n\n\n1pk6uryz-&gt;v4x5duoh\n\n\n\n\n\n1pk6uryz-&gt;fovglxww\n\n\n\n\n\n1pk6uryz-&gt;iw0pfpmk\n\n\n\n\n\n1pk6uryz-&gt;1chkeldv\n\n\n\n\n\nhv8k4g84-&gt;89w9dxkj\n\n\n\n\n\n83ayee0w\n\n\n\n\n83ayee0w-&gt;y0sap20o\n\n\n\n\n\n83ayee0w-&gt;bav7hkue\n\n\n\n\n\n83ayee0w-&gt;lijfw7my\n\n\n\n\n\ndxppp23s-&gt;glv43ych\n\n\n\n\n\ndxppp23s-&gt;m0ggeeei\n\n\n\n\n\ndogjcfze-&gt;1chkeldv\n\n\n\n\n\ndogjcfze-&gt;dtmww06j\n\n\n\n\n\n53bgisfb-&gt;666n359b\n\n\n\n\n\n53bgisfb-&gt;72a2d6xd\n\n\n\n\n\n53bgisfb-&gt;9ifdi52l\n\n\n\n\n\n53bgisfb-&gt;xztqibcz\n\n\n\n\n\nb463lw81\n\n\n\n\n53bgisfb-&gt;b463lw81\n\n\n\n\n\ne7injpqb-&gt;bav7hkue\n\n\n\n\n\ndvhx7p1e-&gt;5cymetka\n\n\n\n\n\ndvhx7p1e-&gt;8bx5ks88\n\n\n\n\n\npveq6y3l-&gt;4s3xzocl\n\n\n\n\n\npveq6y3l-&gt;pveq6y3l\n\n\n\n\n\nm0ggeeei-&gt;4s3xzocl\n\n\n\n\n\nm0ggeeei-&gt;qez5iiiz\n\n\n\n\n\nl9axxrot-&gt;pzodwhlw\n\n\n\n\n\nl9axxrot-&gt;akih91pi\n\n\n\n\n\nl9axxrot-&gt;c21k3g41\n\n\n\n\n\nc21k3g41-&gt;c1sdjw8j\n\n\n\n\n\nc21k3g41-&gt;ozdu79aw\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nb463lw81-&gt;hcqaqm1o\n\n\n\n\n\nb463lw81-&gt;wxhhelge\n\n\n\n\n\nce1516j0-&gt;4s3xzocl\n\n\n\n\n\nce1516j0-&gt;83ayee0w\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nw084j1ko-&gt;onkctu6x\n\n\n\n\n\nw084j1ko-&gt;dxppp23s\n\n\n\n\n\ns3tdzgmu-&gt;vr6qql5z\n\n\n\n\n\ns3tdzgmu-&gt;ez7k5sb5\n\n\n\n\n\ns3tdzgmu-&gt;b463lw81\n\n\n\n\n\n\n\n\n\n\nA randomly generated graph for visual purposes only.\nSee Appendix for graph generator code",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Project Aims and Scope"
    ]
  },
  {
    "objectID": "03-05-transform.html",
    "href": "03-05-transform.html",
    "title": "Transformation",
    "section": "",
    "text": "Transform picks up from where extract finished by using the extracted csv files as the source.\ntransform\n\n\n\nsource_files\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nvalidate_data\n\nValidating Data\n\n\n\nsource_files-&gt;validate_data\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;validate_data\n\n\n\n\n\nclean_data\n\nCleaning Data\n\n\n\nconfig-&gt;clean_data\n\n\n\n\n\nadd_department\n\nAdding 'Department' to Nodes\n\n\n\nconfig-&gt;add_department\n\n\n\n\n\nanonymise_data\n\nùóîùó°ùó¢ùó°ùó¨ùó†ùóúùó¶ùóúùó°ùóö\nPersonal Data\n\n\n\nconfig-&gt;anonymise_data\n\n\n\n\n\naugment_rooms\n\nAugmenting Rooms\nwith Archibus Data\n\n\n\nconfig-&gt;augment_rooms\n\n\n\n\n\ncreate_relationships\n\nCreating\nRelationship Tables\n\n\n\nconfig-&gt;create_relationships\n\n\n\n\n\nvalidate_data-&gt;clean_data\n\n\n\n\n\nclean_data-&gt;add_department\n\n\n\n\n\nadd_department-&gt;anonymise_data\n\n\n\n\n\nanonymise_data-&gt;augment_rooms\n\n\n\n\n\naugment_rooms-&gt;create_relationships\n\n\n\n\n\nprocessed_files\n\nProcessed CSV Files\n\n\n\ncreate_relationships-&gt;processed_files\nConfiguration allows the user to control which nodes and relationships are included and how they are processed. There are options to specify validation, cleaning, data linking, anonymisation and relationship details.\nIt is also possible to specify datatypes. Neo4j assumes string datatype unless it is well-formatted or pre-determined. Config allows the user to specify specific datatyples like dates, times, point, boolean, etc.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#all-data",
    "href": "03-05-transform.html#all-data",
    "title": "Transformation",
    "section": "All data",
    "text": "All data\n\nValidation - basic validation of the data is performed. Validation is extensible and can be expanded, as requirements are identified.\nCleaned - basic cleaning of all data is performed by stripping empty space and removing non-printable characters, etc. using regex. The cleaning functionality is expandable.\n\nWith clean data, the transformation proper starts:",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#nodes-and-relationships",
    "href": "03-05-transform.html#nodes-and-relationships",
    "title": "Transformation",
    "section": "Nodes and relationships",
    "text": "Nodes and relationships\n\nAdd Organisational Unit - where appropriate, the University Organisational Unit (e.g.¬†College, School, Department) is added to the node. This will be picked up as a property during load.\n\nData Augmentation - Room data is augmented with additional properties from the location master database, including latitude, longitude, square meterage, etc. Data augmentation is extensible.\nAnonymisation - Personal data is anonymised. An anonymisation function was developed to remove and replace any personally identifiable information (PII). The pipeline extracts minimal PII but this is safely anonymised. The functional also adds fake emails. See Appendix for additional details\nRelationships - Based on requirements in the configuration, relationships are extracted including optional relationship properties.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-06b-load.html",
    "href": "03-06b-load.html",
    "title": "Neo4j Load",
    "section": "",
    "text": "With accessible csv files, the final module of the ETL pipeline creates (or updates) nodes and relationships in the Neo4j instance.\n\n\n\n\n\n\n\nneo4j_load\n\n\n\ngdrive_files\n\nGoogle Drive Files\n(nodes, relationships folders)\n\n\n\ngdrive_api\n\nConnect to Google Drive API\n\n\n\ngdrive_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nneo4j_connection\n\nConnect to Neo4j\n(with credentials)\n\n\n\nkeyring-&gt;neo4j_connection\n\n\n\n\n\nlist_files\n\nList Files\n(from folders)\n\n\n\ngdrive_api-&gt;list_files\n\n\n\n\n\ndetermine_nodes\n\nDetermine Nodes\n(to create)\n\n\n\nlist_files-&gt;determine_nodes\n\n\n\n\n\ndetermine_relationships\n\nDetermine Relationships\n(to create)\n\n\n\nlist_files-&gt;determine_relationships\n\n\n\n\n\ncreate_schema\n\nCreate Schema\n(dynamic or custom)\n\n\n\nlist_files-&gt;create_schema\n\n\nOptionally Create\n\n\n\ncreate_nodes\n\nCreate Nodes\n\n\n\nneo4j_connection-&gt;create_nodes\n\n\n\n\n\ncreate_relationships\n\nCreate Relationships\n\n\n\nneo4j_connection-&gt;create_relationships\n\n\n\n\n\ndetermine_nodes-&gt;create_nodes\n\n\n\n\n\ndetermine_relationships-&gt;create_relationships\n\n\n\n\n\ncreate_schema-&gt;create_nodes\n\n\n\n\n\ncreate_schema-&gt;create_relationships\n\n\n\n\n\nset_node_properties\n\nSet Node Properties\n(with datatypes)\n\n\n\ncreate_nodes-&gt;set_node_properties\n\n\n\n\n\nset_relationship_properties\n\nSet Relationship Properties\n\n\n\ncreate_relationships-&gt;set_relationship_properties\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nset_node_properties-&gt;process_done\n\n\n\n\n\nset_relationship_properties-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nThere are two authentication requirements:\n\nGoogle Drive to get node and relationship files and data.\n\nNeo4j Aura instance is connected to with Keyring encrypted credentials.\n\nThe process automatically processes nodes and relationships based on files in the specified folders by using a file-pattern matching approach. However, this can be overridden within configuration, if desired.\nAlso in configuration is the option to create a database schema. There are three options:\n\nNo schema\nDynamic (default) - creates unique constraints based on nodes\nCustom - allows the user to specify specific constraints prior to loading.\n\nAt this point, the ETL loads data on a row-by-row basis, reading the public csv files. Columns become properties with data types cross-referenced from a data-mapping dictionary in the configuration.\nIf there have been no errors - we should have data in our Neo4j Aura instance!\n\n\n\nLoaded data for one programme - Artificial Intelligence",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Neo4j Load"
    ]
  },
  {
    "objectID": "appendix-cypher.html",
    "href": "appendix-cypher.html",
    "title": "Cypher Queries",
    "section": "",
    "text": "These pages collate Cypher1 queries used in the development of this project. The queries are grouped by the type of operation they perform, such as creating nodes, relationships, or querying the graph. The queries are presented in a format that can be copied and pasted directly into a Neo4j browser or other Cypher-compatible interface.\nThey represent a starting point for further exploration and development of the graph database. The queries are not exhaustive, and there are many more possibilities for querying and analysing the data.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#constraints",
    "href": "appendix-cypher.html#constraints",
    "title": "Cypher Queries",
    "section": "Constraints",
    "text": "Constraints\nConstraints in Neo4j are used to enforce rules on the graph data, such as ensuring that certain properties are unique or that nodes have specific properties. Constraints can be used to maintain data integrity and prevent duplicate or inconsistent data. For example, we would want to enforce uniqueness constraints on most nodes and relationships, so that we do not duplicate students or allocations, etc.\n\n\n\nConstraints",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#indexes",
    "href": "appendix-cypher.html#indexes",
    "title": "Cypher Queries",
    "section": "Indexes",
    "text": "Indexes\nIndexes in Neo4j are used to speed up queries by allowing the database to quickly locate nodes or relationships based on a property value. They will depend heavily on graph structure and specific use-cases. For example, if performing regular seaches on (activity{actStartTime}) it is probably beneficial to create an index on this property.\nSearch performance indexes include RANGE, FULLTEXT, and POINT. Range indexes are the default and support most queries. Text indexes, like FULLTEXT, are used for string based searches and optimised for queries containing operators like CONTAINS or STARTS WITH. Point indexes are used for spatial queries and are optimised for latitude and longitude properties.\nIndexes in Graph\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nSHOW INDEXES;\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\nfor record in result:\n    print(record)\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x00000208C9874690&gt;\nRunning query...\n\n&lt;Record id=7 name='activity_clash_index' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartDate', 'actStartTime', 'actEndTime'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=19 name='graphid_demoRoom_uniq' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['graphid'] indexProvider='range-1.0' owningConstraint='graphid_demoRoom_uniq' lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 24, 22, 793000000, tzinfo=&lt;UTC&gt;) readCount=8952&gt;\n&lt;Record id=4 name='index_12b79beb' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actEndTime'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=6 name='index_207d313' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['id'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=8 name='index_241bd22f' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomName'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 19, 9, 31, 24, 907000000, tzinfo=&lt;UTC&gt;) readCount=122&gt;\n&lt;Record id=9 name='index_2d375d70' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomLatitude'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 645000000, tzinfo=&lt;UTC&gt;) readCount=107&gt;\n&lt;Record id=0 name='index_343aff4e' state='ONLINE' populationPercent=100.0 type='LOOKUP' entityType='NODE' labelsOrTypes=None properties=None indexProvider='token-lookup-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 19, 16, 28, 19, 533000000, tzinfo=&lt;UTC&gt;) readCount=117374&gt;\n&lt;Record id=10 name='index_43c5c824' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomLongitude'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 646000000, tzinfo=&lt;UTC&gt;) readCount=106&gt;\n&lt;Record id=2 name='index_6acb9a84' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartDate'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 19, 9, 32, 0, 575000000, tzinfo=&lt;UTC&gt;) readCount=92&gt;\n&lt;Record id=5 name='index_7ade165f' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actModSplusID'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 638000000, tzinfo=&lt;UTC&gt;) readCount=148&gt;\n&lt;Record id=1 name='index_f7700477' state='ONLINE' populationPercent=100.0 type='LOOKUP' entityType='RELATIONSHIP' labelsOrTypes=None properties=None indexProvider='token-lookup-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 13, 53, 33, 557000000, tzinfo=&lt;UTC&gt;) readCount=134&gt;\n&lt;Record id=3 name='index_f86013e' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartTime'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 19, 14, 27, 24, 263000000, tzinfo=&lt;UTC&gt;) readCount=16&gt;\n&lt;Record id=18 name='lat_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['lat'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=17 name='lon_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['lon'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=16 name='rm_cat_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['rm_cat'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=13 name='rm_id_demoRoom_uniq' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['rm_id'] indexProvider='range-1.0' owningConstraint='rm_id_demoRoom_uniq' lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 25, 31, 243000000, tzinfo=&lt;UTC&gt;) readCount=57296&gt;\n&lt;Record id=15 name='rm_type_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['rm_type'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=11 name='room_fulltext_index' state='ONLINE' populationPercent=100.0 type='FULLTEXT' entityType='NODE' labelsOrTypes=['room'] properties=['roomName', 'roomDescription', 'roomType', 'roomCapacity'] indexProvider='fulltext-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 2, 441000000, tzinfo=&lt;UTC&gt;) readCount=87&gt;\n&lt;Record id=12 name='room_geo_index' state='ONLINE' populationPercent=100.0 type='FULLTEXT' entityType='NODE' labelsOrTypes=['room'] properties=['roomLatitude', 'roomLongitude'] indexProvider='fulltext-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 2, 457000000, tzinfo=&lt;UTC&gt;) readCount=6&gt;\n\n\n\nCreating Indexes\nThis is the general syntax for creating an INDEX and some examples.\n// General syntax\nCREATE INDEX FOR (n:NodeLabel) ON (n.propertyName)\n\n// Examples\nCREATE INDEX FOR (a:activity) ON (a.actStartDate);\nCREATE INDEX FOR (a:activity) ON (a.actStartTime);\nCREATE INDEX FOR (a:activity) ON (a.actEndTime);\nCREATE INDEX FOR (a:activity) ON (a.actModSplusID);\nCREATE INDEX FOR (a:activity) ON (a.id);\n// composite index for clashes\nCREATE INDEX activity_clash_index FOR (a:activity) ON (a.actStartDate, a.actStartTime, a.actEndTime);\n\nCREATE INDEX FOR (r:room) ON (r.roomName);\nCREATE INDEX FOR (r:room) ON (r.roomLatitude);\nCREATE INDEX FOR (r:room) ON (r.roomLongitude);\n\nCREATE SPATIAL INDEX room_location_index FOR (r:demoRoom) ON (r.location)\nCREATE SPATIAL INDEX demoroom_location_index FOR (r:room) ON (r.location)",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#footnotes",
    "href": "appendix-cypher.html#footnotes",
    "title": "Cypher Queries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúCypher¬†is a¬†declarative¬†graph query language that allows for expressive and efficient data querying in a property graph‚Äù (Wikipedia contributors, 2024)‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "02-01-comparison.html",
    "href": "02-01-comparison.html",
    "title": "Graph vs Relational Data Models",
    "section": "",
    "text": "As outlined in the project aims, my hypothesis is that graph-based approaches have the potential to offer new insights and efficiencies in timetable analysis. This section will briefly explore the theoretical underpinnings of graph data structures and their application to the domain of university timetabling.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#relational-models",
    "href": "02-01-comparison.html#relational-models",
    "title": "Graph vs Relational Data Models",
    "section": "Relational Models",
    "text": "Relational Models\n\nTables, Joins and the Limits of Interconnectedness\nRelational databases, using SQL1 as their query language, have long been the go-to for managing data, including timetabling information. They structure data into tables, where rows represent instances of entities (e.g.¬†individual rooms, staff, or students) and columns represent entity attributes (name, capacity, email, etc.).\nRelationships between these entity tables are established through foreign keys, forming links between tables. This often involves intermediary ‚Äúrelationship‚Äù tables to handle the many-to-many nature of timetabling data (e.g., a student attends many activities, and an activity has many students) (Khan et al., 2023;Sokolova, G√≥mez and Borisoglebskaya, 2020).\nWhile robust and well-understood, relational databases start to show their limitations when dealing with the highly interconnected nature of timetables:\n\nJoin Complexity: Even seemingly simple queries, like ‚Äúfind students attending a specific lecturer‚Äôs class in a particular building,‚Äù require joining multiple tables. As queries become more nuanced, the number of joins increases, often impacting performance, especially with large datasets.\nRigidity: Relational databases rely on a predefined schema, making them less adaptable to evolving needs. Adding new entities or relationships is not possible without disrupting existing queries and applications.\n\n\n\n\nExample Simple Entity Relationship Diagram",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#graph-models",
    "href": "02-01-comparison.html#graph-models",
    "title": "Graph vs Relational Data Models",
    "section": "Graph Models",
    "text": "Graph Models\n\nEmbracing interconnectedness\nIn contrast to the rigid table structure of relational databases, graph databases offer a more intuitive and flexible approach for representing interconnected data like timetables. They utilise:\n\nNodes: Represent entities. These are often the ‚Äúnouns‚Äù like activity, room, staff, student.\nEdges: Represent relationships between nodes. These are often the ‚Äúverbs‚Äù like TAUGHT_BY, ENROLLED_IN, SCHEDULED_AT, OWNED_BY.\n\n\n\n\nSimple Graph Data Model\n\n\nThis node-and-edge structure inherently reflects how timetabling elements connect. Instead of relying on cumbersome joins, relationships are directly encoded in the data model itself. This results in several advantages:\n\nNatural Representation: Graph databases visually and conceptually mirror the relationships inherent in timetables, making them easier to understand and query.\nRelationship-Centric Queries: Graph databases are optimised for traversing and analysing relationships. Queries that would require multiple joins in a relational database often become significantly simpler and faster in a graph database.\nFlexibility: The schema-less or schema-optional nature of most graph databases allows for greater flexibility in data modeling. New entities or relationships can be added effortlessly without impacting existing structures or queries (Nan and Bai, 2019; Webber, Eifrem and Robinson, 2013).",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#comparing-queries",
    "href": "02-01-comparison.html#comparing-queries",
    "title": "Graph vs Relational Data Models",
    "section": "Comparing queries",
    "text": "Comparing queries\nExample Insight: Find all students attending a specific lecturer‚Äôs class in a particular building\nRepresentative queries have been written in SQL and Cypher to find this insight. The SQL query is much longer and requires six joins, each coming at a computational cost.\n\nSQL Query\nSELECT DISTINCT ss.[FirstName], ss.[LastName], ss.[Email]\nFROM [RDB_MAIN2223].[rdowner].[V_STUDENTSET] ss\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STUDENTSET] acts ON ss.[Id] = acts.[StudentSetId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY] a ON acts.[ActivityId] = a.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_LOCATION] al ON a.[Id] = al.[ActivityId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_LOCATION] l ON al.[LocationId] = l.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_BUILDING] b ON l.[BuildingId] = b.[Id] \nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STAFF] ast ON a.[Id] = ast.[ActivityId]\nWHERE ast.[StaffId] = 'StaffID'  \n  AND b.[Name] = 'BuildingName'; \n\n\nCypher Query\nIn contrast, the Cypher query pattern is much simpler - written in one line (MATCH pattern). The query is more intuitive and easier to understand, especially for those unfamiliar with the database schema.\n\nMATCH (s:Student)-[:ATTENDS]-&gt;(a:Activity)&lt;-[:TEACHES_ON]-(st:Staff), \n      (a:Activity)-[:TAKES_PLACE_IN]-&gt;(r:Room)\nWHERE st.last_name = \"LecturerLastName\" AND r.building = \"BuildingName\"\nRETURN s.first_name, s.last_name, s.email",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#key-differences-and-implications",
    "href": "02-01-comparison.html#key-differences-and-implications",
    "title": "Graph vs Relational Data Models",
    "section": "Key Differences and Implications",
    "text": "Key Differences and Implications\n\n\n\n\n\n\n\n\n\nFeature\nRelational Model\nGraph Model\n\n\n\n\nData Structure\nTables with rows and columns\nNodes and edges\n\n\nSchema\nRigid, predefined\nFlexible, schema-less or schema-optional\n\n\nRelationship Handling\nForeign keys, joins\nDirect connections (edges)\n\n\nQuery Performance\nCan be slow for relationship-heavy queries\nOptimised for traversing relationships, potentially faster\n\n\nData Modeling\nLess intuitive for interconnected data\nNaturally represents complex relationships\n\n\nAdaptability\nLess adaptable to schema changes\nMore flexible, accommodates evolving data needs\n\n\n\n\nThese advantages position graph databases as a powerful tool for uncovering insights hidden within complex, interconnected datasets like university timetables.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#footnotes",
    "href": "02-01-comparison.html#footnotes",
    "title": "Graph vs Relational Data Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStructured Query Language (Wikipedia contributors, 2024)‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "appendix-cypher5.html",
    "href": "appendix-cypher5.html",
    "title": "Hard Constraints",
    "section": "",
    "text": "‚ÄúHard constraints‚Äù in a timetabling context are generally rules or conditions which cannot be violated. Violation would indicate non-viable timetable, e.g.¬†a lecturer being scheduled to teach in two places simultaneously. In reality, hard constraints appear in timetables and are accepted with real-world workarounds.\nThis page contains cypher queries that can be used to identify where a timetabling hard constraint has been violated.\nExample hard constraints include:\n\nAll Activities Scheduled: Every lecture, tutorial, lab, etc., must have a designated time and place.\nNo Room Conflicts (aka room clash): Two activities cannot be scheduled in the same room at the same time.\nRoom Capacity Sufficient: The room assigned to an activity must accommodate the expected number of students\nPerson clashes: People, that is staff and students, cannot be allocated to two or more activities occurring at the same time.\n\nNo Staff Conflicts (aka staff clash)\nNo Student Conflicts (aka student clash)\n\nStaff Availability Respected: Activities cannot be scheduled during a staff member‚Äôs unavailable times (e.g., research days, meetings, unavailability pattern).\nCurriculum Requirements Met: Required courses must be offered at times when students can take them\n\n\nUnscheduled activities\nUnscheduled activities can be identified as follows. This query can be tweaked to also search for matches where the property equals ‚Äô‚Äô - that is, a blank.\nMATCH (a:activity)\nWHERE a.actStartDate IS NULL \nOR a.actStartTime IS NULL \nOR a.actEndTime IS NULL\nRETURN a\n\n\nRoom clashes\nRoom or location clashes are where two or more activities are scheduled at the same datetime in the same space and this is not deliberate. These can be identified with the starter query below. The image clearly shows pairs of activities sharing one location. In reality, I suspect that these are deliberate clashes.\nMATCH (a1:activity)-[r1:OCCUPIES]-&gt;(r:room)&lt;-[r2:OCCUPIES]-(a2:activity)\nWHERE a1.actStartDate = a2.actStartDate AND a1 &lt;&gt; a2\n    AND (\n        (a1.actStartTime &lt;= a2.actStartTime AND a1.actEndTime &gt; a2.actStartTime)\n        OR \n        (a2.actStartTime &lt;= a1.actStartTime AND a2.actEndTime &gt; a1.actStartTime)\n    )\nRETURN a1, a2, r, r1, r2\n\n\n\nRoom Clashes\n\n\nThe same results have been returned as a table.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nMATCH (a1:activity)-[r1:OCCUPIES]-&gt;(r:room)&lt;-[r2:OCCUPIES]-(a2:activity)\nWHERE r.roomName IN [\"4Q50/51 FR\", \"4Q69 FR\", \"3E Maths Open Zone A\", \"3E12 FR\"] \n¬† AND a1.actStartDate = a2.actStartDate \n¬† AND a1 &lt;&gt; a2\n¬† AND (\n¬† ¬† ¬† ¬† (a1.actStartTime &lt;= a2.actStartTime AND a1.actEndTime &gt; a2.actStartTime)\n¬† ¬† ¬† ¬† OR \n¬† ¬† ¬† ¬† (a2.actStartTime &lt;= a1.actStartTime AND a2.actEndTime &gt; a1.actStartTime)\n¬† ¬† ¬† )\nRETURN a1.actName AS activity1, a2.actName AS activity2, \n       r.roomName AS room, a1.actStartDate AS date,\n       a1.actStartTime AS activity1_start, a1.actEndTime AS activity1_end,\n       a2.actStartTime AS activity2_start, a2.actEndTime AS activity2_end\n\"\"\" \n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"activity1\", \"activity2\", \"room\", \"date\", \n                                   \"activity1_start\", \"activity1_end\", \n                                   \"activity2_start\", \"activity2_end\"])\n\n# print\nprint(df)\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000001F8E9828410&gt;\nRunning query...\n\n                          activity1                         activity2  \\\n0          UFCF8P-15-M Sep W2_oc/01  UFCF8P-15-M Jan Cont W2_oc jt/01   \n1  UFCF8P-15-M Jan Cont W2_oc jt/01          UFCF8P-15-M Sep W2_oc/01   \n2          UFCF8P-15-M Sep W1_oc/01  UFCF8P-15-M Jan Cont W1_oc jt/01   \n3  UFCF8P-15-M Jan Cont W1_oc jt/01          UFCF8P-15-M Sep W1_oc/01   \n4                     Maths_E_oc/01                    Maths_E5_oc/01   \n5                    Maths_E5_oc/01                     Maths_E_oc/01   \n6                     Maths_E_oc/01              UFMFVV-30-3 DI_oc/01   \n7              UFMFVV-30-3 DI_oc/01                     Maths_E_oc/01   \n\n                   room        date     activity1_start       activity1_end  \\\n0            4Q50/51 FR  2022-12-13  16:00:00.000000000  17:30:00.000000000   \n1            4Q50/51 FR  2022-12-13  16:00:00.000000000  17:30:00.000000000   \n2               4Q69 FR  2022-12-13  14:00:00.000000000  15:30:00.000000000   \n3               4Q69 FR  2022-12-13  14:00:00.000000000  15:30:00.000000000   \n4  3E Maths Open Zone A  2022-11-09  09:00:00.000000000  17:00:00.000000000   \n5  3E Maths Open Zone A  2022-11-09  16:00:00.000000000  17:00:00.000000000   \n6               3E12 FR  2022-11-08  09:00:00.000000000  17:00:00.000000000   \n7               3E12 FR  2022-11-08  15:00:00.000000000  16:00:00.000000000   \n\n      activity2_start       activity2_end  \n0  16:00:00.000000000  17:30:00.000000000  \n1  16:00:00.000000000  17:30:00.000000000  \n2  14:00:00.000000000  15:30:00.000000000  \n3  14:00:00.000000000  15:30:00.000000000  \n4  16:00:00.000000000  17:00:00.000000000  \n5  09:00:00.000000000  17:00:00.000000000  \n6  15:00:00.000000000  16:00:00.000000000  \n7  09:00:00.000000000  17:00:00.000000000  \n\n\n\n\nRoom capacity exceeded\nThis query identifies activities where the number of students exceeds the room capacity. It includes an optional WHERE clause if looking at a specific date range.\nMATCH (r:room)&lt;-[r1:OCCUPIES]-(a:activity)&lt;-[:ATTENDS]-(s:student)\n//WHERE a.Date &gt;= date(\"2022-01-01\") AND a.Date &lt;= date(\"2022-06-30\") \nWITH r, a, count(s) as numStudents\nWHERE numStudents &gt; r.roomCapacity\nRETURN r, a.actStartDate, a.actName AS Activity, r.roomCapacity, numStudents - r.roomCapacity AS extraNeeded\nORDER BY extraNeeded DESC\nThese are the results in a dataframe:\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nMATCH (r:room)&lt;-[r1:OCCUPIES]-(a:activity)&lt;-[:ATTENDS]-(s:student)\n//WHERE a.Date &gt;= date(\"2022-01-01\") AND a.Date &lt;= date(\"2022-06-30\")¬†\nWITH r, a, count(s) as numStudents\nWHERE numStudents &gt; r.roomCapacity\nRETURN DISTINCT r.roomName, r.roomType,¬† a.actName AS Activity, \n       r.roomCapacity, numStudents - r.roomCapacity AS extraNeeded\nORDER BY extraNeeded DESC\n\"\"\" \n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"roomName\", \"roomType\", \"Activity\", \n                                   \"roomCapacity\", \"extraNeeded\"])\n\n# print\nprint(\"Printing first 5 records...\\n\")\nprint(df.head())\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000001F8843F2050&gt;\nRunning query...\n\nPrinting first 5 records...\n\n               roomName  roomType       Activity  roomCapacity  extraNeeded\n0              3E006 FR  FAC STUD  Maths_E_oc/01             4           88\n1              3E007 FR  FAC STUD  Maths_E_oc/01             4           88\n2  3E Maths Open Zone B  FAC STUD  Maths_E_oc/01            12           80\n3               3E28 FR  TEACHING  Maths_E_oc/01            24           68\n4               3E12 FR    PC LAB  Maths_E_oc/01            30           62\n\n\n\n\nStudent clashes\nThis query identifies students who are scheduled to attend two or more activities at the same time. Identifying clashes is a complex undertaking and it is one where the graph structure in terms of nodes, properties and relationships could potentially make a significant different to performance.\nThe reason for the complexity is that you need to look for overlapping times between two activities for each date, for each student. The query to achieve this and the ensuing calculations will vary significantly depending on on the structure and syntax.\nBecause of this, I explored the student clash scenario in more detail here: Student Clashes\nMATCH (s:student)-[:ATTENDS]-&gt;(a1:activity)\nWITH s, a1\nMATCH (s)-[:ATTENDS]-&gt;(a2:activity)¬†\nWHERE a1 &lt;&gt; a2¬†\n¬† AND a1.actStartDate = a2.actStartDate¬†\n¬† AND (a1.actStartTime &lt; a2.actEndTime AND a1.actEndTime &gt; a2.actStartTime)¬† \n¬† AND NOT (a1.actStartTime = a2.actEndTime OR a1.actEndTime = a2.actStartTime) \n¬† AND a1.actName &lt; a2.actName  // Ensure only one direction of the pair is returned\nRETURN s.stuFirstName_anon AS Student,¬†\n¬† ¬† ¬† ¬†a1.actStartDate AS ClashDate,¬†\n¬† ¬† ¬† ¬†a1.actName AS Activity1,¬†\n¬† ¬† ¬† ¬†a1.actStartTime + \"-\" + a1.actEndTime AS Timeslot1,¬†\n¬† ¬† ¬† ¬†a2.actName AS Activity2,¬†\n¬† ¬† ¬† ¬†a2.actStartTime + \"-\" + a2.actEndTime AS Timeslot2\nORDER BY Student, ClashDate;\n\n\n\nStudent Clashes",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Hard (timetabling) Constraints"
    ]
  },
  {
    "objectID": "02-03-graph-time.html",
    "href": "02-03-graph-time.html",
    "title": "Graphing Time",
    "section": "",
    "text": "The biggest challenge I encountered when translating timetables into graph data involved temporal elements - that is, start and end times, dates, weeks, recurrences, durations, etc. While the basic model successfully captured the core entities and relationships, it lacked the necessary detail to perform meaningful time-based analysis.\nThe flexibility of graph databases is appealing but finding the optimal balance between efficient representation, query performance, and data redundancy requires careful consideration. This section details some challenges encountered and the approach taken for the proof-of-concept.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#the-problem-of-normalised-time",
    "href": "02-03-graph-time.html#the-problem-of-normalised-time",
    "title": "Graphing Time",
    "section": "The Problem of Normalised Time",
    "text": "The Problem of Normalised Time\nTraditional relational databases often store timetable information in a highly normalised1 format, condensing recurring events into single rows with date ranges, week patterns, or lists of occurrences. While efficient for storage and basic display, this approach severely hinders analysis, especially when aiming to:\n\nIdentify Time-Based Patterns: Determining if students lack lunch breaks or experience excessive gaps between classes becomes difficult when time is fragmented across multiple fields.\nPerform Aggregations: Calculating total teaching hours for a lecturer across specific weeks or days requires complex queries and data transformations.\nModel Temporal Relationships: Representing relationships between activities based on their temporal proximity, such as students attending consecutive classes, becomes convoluted.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#exploring-potential-solutions",
    "href": "02-03-graph-time.html#exploring-potential-solutions",
    "title": "Graphing Time",
    "section": "Exploring Potential Solutions",
    "text": "Exploring Potential Solutions\nSeveral time modelling approaches were considered, each with its own trade-offs.\nTo illustrate this, let‚Äôs explore using a fictional example - Introduction to Graph Databases - focusing on the lecture:\n\nExample Source Data (Relational)\n\n\n\n\n\n\n\n\n\n\nName\nActivityType\nDay\nStartTime\nEndTime\nWeeks\n\n\n\n\nITGD\nLecture\nWednesday\n09:00\n11:00\n1-3, 5-7, 9-13\n\n\nITGD\nSeminar\nWednesday\n10:00\n13:00\n4, 7-8, 15\n\n\nITGD\nSeminar\nMonday\n13:00\n16:00\n4, 7-8, 15\n\n\n\n\nOption 0: Proof-of-concept activity\nThe basic model creates nodes for each activity exactly as they exist in the relational database. This simple approach is perfectly acceptable but makes any time based calculations difficult because each activity node can represent a different number of occurrences due to the week ranges.\nThis in turn means you cannot simply COUNT each activity ‚Äúequally‚Äù - for example, the lecture has 11 instances, each of two hours. The seminars have four instances, each of three hours. Calculating aggregations, finding clashes and similar is very challenging.\n\n\n\nBasic example of Graphing Normalised Activities\n\n\nIf we assume that each student attends the lecture and one of the seminars, some students have a clash in week 7 (Wednesday 10:00-11:00) - this is very difficult to identify and isolate in a highly normalised dataset.\n\n\nOption 1: Unique Activity Nodes\nOption 1 addresses this by creating nodes for each unique combination of name, startTime, endTime and date - this means de-normalising the relational data and deliberately introducing duplication.\n\n\n\nUnique Activity Nodes Graph\n\n\nGraph Structure:\n\n11 separate Activity nodes one for each occurrence (date)\nEach node has date, startTime, endTime properties\nOnly date is different between each node.\n\n(Activity {Name: \"ITGD\", Date: \"2024-01-03\", StartTime: \"09:00\", EndTime: \"11:00\"}) \n(Activity {Name: \"ITGD\", Date: \"2024-01-10\", StartTime: \"09:00\", EndTime: \"11:00\"}) \n... \n(Activity {Name: \"ITGD\", Date: \"2024-03-20\", StartTime: \"09:00\", EndTime: \"11:00\"})\nPros:\n\nConceptual Simplicity: Easy to understand and implement.\nDirect Time Representation: Time is directly associated with each activity instance.\n\nCons:\n\nNode Proliferation: Leads to a high volume of nodes, potentially impacting performance with large datasets.\n\nUse Case dependent\n\nTime-Based Queries: Answering questions about time patterns or conflicts requires traversing numerous nodes and relationships. Some queries will benefit - e.g.¬†identifying clashes which may only occur in a specific week, others will become more complex as de-normalised data needs to be re-aggregated.\n\n\n\nOption 2: Date and Time Nodes\nOption 2 creates a single activity node but also additional date and time nodes, as required, thus not proliferating activities.\n\n\n\nTime and Date Nodes\n\n\nGraph Structure:\n\n1 Activity node\n11 Date nodes - shared by ALL activities on those dates.\n2 Time nodes (09:00 and 11:00) - shared by ALL activities on those times!\nAdditional Relationships\n\nActivity -[:SCHEDULED_ON]-&gt; Date (11 relationships)\nActivity -[:STARTS_AT]-&gt; Time (11 relationships to 09:00)\nActivity -[:ENDS_AT]-&gt; Time (11 relationships to 11:00)\n\n\n\n(Activity {Name: \"ITGD\"})\n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"})\n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-10\"})\n    ...\n    -[:STARTS_AT]-&gt; (Time {time: \"09:00\"}) \n    -[:ENDS_AT]-&gt; (Time {time: \"11:00\"})\nKey point: Relationships encode which activity happens when.\nPros:\n\nIncreased Flexibility: Facilitates queries across time ranges and aggregations across time slots.\nReduced Redundancy: Avoids replicating time information for activities occurring on the same date and time.\nLower Node Count: Potentially fewer nodes overall compared to Option 1 as date and time nodes are shared with all activities in the database.\n\nCons:\n\nIncreased Model Complexity: Requires managing relationships between Activity, Date, and Time nodes.\nPotential Performance Overhead: Querying might involve traversing multiple relationships, impacting efficiency.\n\n\n\nOption 3: Date and Time Block Nodes\nOption 3 creates a single activity but instead of individual start and end time nodes, we use predetermined timeBlocks encompassing both. For example, if using 30-minute blocks, we would have a node for ‚Äú09:00-09:30‚Äù and another for ‚Äú09:30-10:00‚Äù, etc.\n\n\n\nTimeBlock and Date Nodes\n\n\nGraph Structure:\n\n1 Activity node\n11 Date nodes\n4 Timeblock nodes (09:00-09:30, etc.) - shared by ALL activities on those times!\nAdditional Relationships\n\nActivity -[:SCHEDULED_ON]-&gt; Date (11 relationships)\nActivity -[:TAKES_PLACE_DURING]-&gt; timeBlock 09:00-09:30 (11 relationships)\nActivity -[:TAKES_PLACE_DURING]-&gt; timeBlock 09:30-10:00 (11 relationships)\n‚Ä¶\n\n\n(Activity {Name: \"ITGD\"}) \n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"}) \n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-10\"})\n    ...\n    -[:TAKES_PLACE_DURING]-&gt; (TimeBlock {timeBlock: \"09:00-09:30\"}) \n    -[:TAKES_PLACE_DURING]-&gt; (TimeBlock {timeBlock: \"09:30-10:00\"})\n    -[:TAKES_PLACE_DURING]-&gt; (Timeblock {timeBlock: \"10:00-10:30\"}) \n    -[:TAKES_PLACE_DURING]-&gt; (Timeblock {timeBlock: \"10:30-11:00\"})\nPros:\n\nGranular Time Representation: Enables analysis at specific time intervals\nEasier Time Calculations: Duration is encoded and allows for easy calculations.\n\nCons:\n\nPotential for Data Sparsity: Some time blocks might be sparsely populated, leading to storage inefficiencies.\nPotential for High Node Codes: Lots of TimeBlocks if using small intervals\nLess flexibile: Timeblocks are not dynamic.\n\n\n\nVariations\nStartTime and Duration: This option simplifies the model by representing time using only StartTime and DurationInMinutes properties on the Activity node, omitting explicit EndTime nodes. This approach is suitable for duration based queries but it is limiting in that it is more difficult to query events occurring at specific times, overlapping time ranges or on end-times.\n{cypher .scroll-cypher} (Timeblock {name: \"09:00-11:00\", start: 09:00, end: 11:00, duration:120}) (Timeblock {name: \"10:30-11:30\", start: 10:30, end: 11:30, duration:60}) (Timeblock {name: \"11:00-12:00\", start: 11:00, end: 12:00, duration:60})\nTime Chains: This option retains date and time nodes, but instead of having relationships from activity, the nodes are chained: activity -&gt; startTime -&gt; endTime.\nTime as Relationship Property: This option stores time information as properties on the relationship between Activity and Date nodes. This approach is more compact but can be less intuitive and may limit the ability to query based on time.\nDynamic TimeBlocks: This variation does not pre-create timeblocks based on a set interval (e.g.¬†30 minutes). They are created dynamically as required by the data and what already exists. For example, activities at 09:00-11:00, 10:30-11:30 and 11:00-12:00 would require these TimeBlocks:\n\n\nSummary\n\nOption summary\n\n\n\n\n\n\n\n\n\nOption\nPros\nCons\n\n\n\n\n0\nDirect transfer (Normalised)\nSimple\nMinimal benefits (for time calculations)\n\n\n1\nUnique Activities\nSimple, direct\nHigh node count, complex time pattern queries\n\n\n2\nDate & Time\nLower activity node count, good for time-based queries\nMore complex relationships\n\n\n3\nDate & TimeBlock\nGranular, easier duration calculations\nPotentially high node count, sparsity if blocks are fine-grained\n\n\n\nGiven the proof-of-concept scope of this project, I chose to proceed with Option 1. While this approach can lead to node proliferation, it offers the most straightforward implementation for exploring fundamental time-based queries and insights. It also acts as an easy jumping off point for exploring any of the other options.2",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#footnotes",
    "href": "02-03-graph-time.html#footnotes",
    "title": "Graphing Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúNormalization is the process of organizing data in a database. It includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating redundancy and inconsistent dependency.‚Äù (helenclu, 2024)‚Ü©Ô∏é\nSome of the above variations are explored in Appendix-Student Clashes‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html",
    "href": "appendix-cypher5a.html",
    "title": "Student Clashes - Deeper Dive",
    "section": "",
    "text": "This page explore different graph data structures and queries for the purposes of identifying student clashes. It illustrates the inherent flexiblity of graph databases and that thorough modelling and profiling of the data can lead to more efficient and effective queries.\nUse-case is king when it comes to optimised databases and performant queries.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#scenario",
    "href": "appendix-cypher5a.html#scenario",
    "title": "Student Clashes - Deeper Dive",
    "section": "Scenario",
    "text": "Scenario\nEach model below will use the same basic scenario:\n\nTwo students - Alice and Bob\nThree activities:\n\nITGD - Introduction to Graph Databases\nNeo4j - Neo4j for Beginners\nTigerDB - TigerGraph for Data Scientists\nEach activity has a start and end time\nEach activity is scheduled for several weeks\n\nThere are deliberate clashes between the activities to illustrate the concept of a student clash",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-1---activity-occurrence",
    "href": "appendix-cypher5a.html#model-1---activity-occurrence",
    "title": "Student Clashes - Deeper Dive",
    "section": "Model 1 - Activity Occurrence",
    "text": "Model 1 - Activity Occurrence\nEach ‚Äòoccurrence‚Äô of an activity is a separate node. This model is simple and easy to understand, but proliferates nodes which lead to inefficient and complex queries.\n\nCreate data\n// Create unique activity nodes (TestActivityModel1)\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-06\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-13\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-20\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-27\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-09-03\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-07-30\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-08-13\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-08-27\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"TigerDb\", date: date(\"2024-08-06\"), startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"TigerDb\", date: date(\"2024-08-13\"), startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") });\n\n// Create unique student nodes (TestStudentModel1)\nCREATE (:TestStudentModel1 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel1 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\n// Create ATTENDS relationships (one student attends all TigerDb and Neo4j)\nMATCH (s:TestStudentModel1 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel1) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a);\n\nMATCH (s:TestStudentModel1 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel1) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 1\n\n\nTo identify the clashes, this query can be run:\n\nMATCH (s:TestStudentModel1)-[:ATTENDS]-&gt;(a1:TestActivityModel1)\nWITH s, a1\nMATCH (s)-[:ATTENDS]-&gt;(a2:TestActivityModel1) \nWHERE a1 &lt;&gt; a2 \n  AND a1.date = a2.date \n  AND (a1.startTime &lt; a2.endTime AND a1.endTime &gt; a2.startTime)  //  overlap condition\n  AND NOT (a1.startTime = a2.endTime OR a1.endTime = a2.startTime) // xxclude \"touching\" cases\n  AND a1.actName &lt; a2.actName  // ensures only one direction of the pair is returned\nRETURN s.stuFirstName_anon AS Student, \n       a1.date AS ClashDate, \n       a1.actName AS Activity1, \n       a1.startTime + \"-\" + a1.endTime AS Timeslot1, \n       a2.actName AS Activity2, \n       a2.startTime + \"-\" + a2.endTime AS Timeslot2\nORDER BY Student, ClashDate;\nWhich correctly identifies Bob‚Äôs clash:\n\n\n\nModel 1 results\n\n\nOne way of measuring and comparing query performance is to look at the PROFILE and dbhits. In this instance there are 278 database accesses.\n\n\n\nModel 1 profile",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-2---date-and-time-nodes",
    "href": "appendix-cypher5a.html#model-2---date-and-time-nodes",
    "title": "Student Clashes - Deeper Dive",
    "section": "Model 2 - Date and Time Nodes",
    "text": "Model 2 - Date and Time Nodes\nModel 2 uses a single node for each activity but has date and time nodes. This model is more complex in that there are more node labels, but can be more efficient for certain queries.\n\nCreate data\n// Create unique time nodes\nCREATE (:TestStartTimeNode { time: localtime(\"09:00:00\") })\nCREATE (:TestStartTimeNode { time: localtime(\"10:00:00\") })\nCREATE (:TestStartTimeNode { time: localtime(\"11:00:00\") })\nCREATE (:TestEndTimeNode { time: localtime(\"11:00:00\") })\nCREATE (:TestEndTimeNode { time: localtime(\"12:00:00\") })\n\n// Create unique date nodes\nCREATE (:TestDateNode { date: date(\"2024-07-30\") })\nCREATE (:TestDateNode { date: date(\"2024-08-06\") })\nCREATE (:TestDateNode { date: date(\"2024-08-13\") })\nCREATE (:TestDateNode { date: date(\"2024-08-20\") })\nCREATE (:TestDateNode { date: date(\"2024-08-27\") })\nCREATE (:TestDateNode { date: date(\"2024-09-03\") })\n\n// Create activity nodes\nCREATE (:TestActivityModel2 { actName: \"ITGD\" })\nCREATE (:TestActivityModel2 { actName: \"Neo4j\" })\nCREATE (:TestActivityModel2 { actName: \"TigerDb\" });\n\n// Connect ITGD to dates and times (using MERGE)\nMATCH (a:TestActivityModel2 { actName: \"ITGD\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"09:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"11:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Connect Neo4j to dates and times (adjust dates/times and use MERGE)\nMATCH (a:TestActivityModel2 { actName: \"Neo4j\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"10:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"11:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Connect TigerDb to dates and times (adjust dates/times and use MERGE)\nMATCH (a:TestActivityModel2 { actName: \"TigerDb\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"11:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"12:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Create Students and ATTENDS relationships (same as Model 1)\nCREATE (:TestStudentModel2 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel2 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel2 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel2) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a);\n\nMATCH (s:TestStudentModel2 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel2) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 2\n\n\nTo identify student clashes, this query can be run.\nMATCH (s:TestStudentModel2)-[:ATTENDS]-&gt;(a1:TestActivityModel2)-[:SCHEDULED_ON]-&gt;(d:TestDateNode)\nWITH s, a1, d\nMATCH (s)-[:ATTENDS]-&gt;(a2:TestActivityModel2)-[:SCHEDULED_ON]-&gt;(d) // Same date\nMATCH (a1)-[:STARTS_AT]-&gt;(st1:TestStartTimeNode)-[:ENDS_AT]-&gt;(et1:TestEndTimeNode) \nMATCH (a2)-[:STARTS_AT]-&gt;(st2:TestStartTimeNode)-[:ENDS_AT]-&gt;(et2:TestEndTimeNode) \nWHERE a1 &lt;&gt; a2 \n  AND (st1.time &lt; et2.time AND et1.time &gt; st2.time)  //  overlap condition\n  AND NOT (st1.time = et2.time OR et1.time = st2.time) // xxclude \"touching\" cases\n  AND a1.actName &lt; a2.actName  // ensures only one direction of the pair is returned\nRETURN  s.stuFirstName_anon AS Student, \n        d.date AS ClashDate, \n        a1.actName AS Activity1, \n        st1.time + \"-\" + et1.time AS Timeslot1,\n        a2.actName AS Activity2,\n        st2.time + \"-\" + et2.time AS Timeslot2\nORDER BY Student, ClashDate;\nThe results are the same as Model 1, but the profile is different, with fewer database accesses (143):\n\n\n\nModel 2 profile",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-3---date-nodes",
    "href": "appendix-cypher5a.html#model-3---date-nodes",
    "title": "Student Clashes - Deeper Dive",
    "section": "Model 3 - Date Nodes",
    "text": "Model 3 - Date Nodes\nModel 3 uses a single node for each activity as well as date nodes - start and end times are properties of the activity.\n// Create unique date nodes\nCREATE (:TestDateNodeModel3 { date: date(\"2024-07-30\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-06\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-13\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-20\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-27\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-09-03\") })\n\n// Create activity nodes with start/end times as properties\nCREATE (:TestActivityModel3 { actName: \"ITGD\", startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel3 { actName: \"Neo4j\", startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel3 { actName: \"TigerDb\", startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") });\n\n// Connect Activities to Dates (using MERGE)\nMATCH (a:TestActivityModel3 { actName: \"ITGD\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\nMATCH (a:TestActivityModel3 { actName: \"Neo4j\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\nMATCH (a:TestActivityModel3 { actName: \"TigerDb\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\n// Create Students and ATTENDS relationships\nCREATE (:TestStudentModel3 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel3 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel3 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel3) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a) ;\n\nMATCH (s:TestStudentModel3 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel3) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 3\n\n\nThe results are the same as Model 1 and Model 2, but the profile is different again with even fewer database accesses (58):\n\n\n\nModel 3 profile\n\n\n\nModel 4 -\nModel 4 uses a single node for each activity and date - start and end times are now properties of the relationship between the activity and the date.\n// Create unique date nodes\nMERGE (:TestDateNodeModel4 { date: date(\"2024-07-30\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-06\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-13\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-20\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-27\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-09-03\") })\n\n// Create activity nodes \nMERGE (:TestActivityModel4 { actName: \"ITGD\" })\nMERGE (:TestActivityModel4 { actName: \"Neo4j\" })\nMERGE (:TestActivityModel4 { actName: \"TigerDb\" });\n\n// Connect ITGD to Dates with START and END relationships\nMATCH (a:TestActivityModel4 { actName: \"ITGD\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:STARTS { time: localtime(\"09:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"11:00:00\") }]-&gt;(d);\n\n// Connect Neo4j to Dates (adjust dates and times)\nMATCH (a:TestActivityModel4 { actName: \"Neo4j\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:STARTS { time: localtime(\"10:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"11:00:00\") }]-&gt;(d);\n\n// Connect TigerDb to Dates (adjust dates and times)\nMATCH (a:TestActivityModel4 { actName: \"TigerDb\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:STARTS { time: localtime(\"11:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"12:00:00\") }]-&gt;(d);\n\n// Create Students and ATTENDS relationships\nMERGE (:TestStudentModel4 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nMERGE (:TestStudentModel4 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel4 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel4) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\nMATCH (s:TestStudentModel4 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel4) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 4\n\n\nThe results are again the same as Model 1, Model 2, and Model 3, but the profile is different with the most database accesses (293):\n\n\n\nModel 4 profile",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#conclusion",
    "href": "appendix-cypher5a.html#conclusion",
    "title": "Student Clashes - Deeper Dive",
    "section": "Conclusion",
    "text": "Conclusion\nEach model has its own strengths and weaknesses. The choice of model will depend on the specific requirements. The more complex models can be more efficient for certain queries, but can also be more difficult to understand and maintain. The simpler models are easier to understand and maintain, but can be less efficient for certain queries.\nOf the four tested, Model 3 was the most efficient in terms of database hits on the very small test dataset used. However, this may not be the case with larger datasets. It is important to profile the queries and the data to determine the best model for the specific requirements.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#delete-data",
    "href": "appendix-cypher5a.html#delete-data",
    "title": "Student Clashes - Deeper Dive",
    "section": "Delete Data",
    "text": "Delete Data\nThe cypher below deletes all test data.\n\nModel 1\n// Delete all TestActivityModel1 nodes\nMATCH (a:TestActivityModel1)\nDETACH DELETE a; \n\n// Delete all TestStudentModel1 nodes\nMATCH (s:TestStudentModel1)\nDETACH DELETE s;\n\n\nModel 2\n// Delete test data for Model 2\nMATCH (n) \nWHERE n:TestStudentModel2 OR n:TestActivityModel2 OR n:TestDateNode OR n:TestStartTimeNode OR n:TestEndTimeNode\nDETACH DELETE n\n\n\nModel 3\n// Delete test data for Model 3\nMATCH (n) \nWHERE n:TestStudentModel3 OR n:TestActivityModel3 OR n:TestDateNodeModel3 \nDETACH DELETE n\n\n\nModel 4\nMATCH (n) \nWHERE n:TestStudentModel4 OR n:TestActivityModel4 OR n:TestDateNodeModel4\nDETACH DELETE n",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "01-02-background.html#the-hypothesisenter-the-graph",
    "href": "01-02-background.html#the-hypothesisenter-the-graph",
    "title": "Background and Motivation",
    "section": "The hypothesis‚Ä¶Enter the Graph",
    "text": "The hypothesis‚Ä¶Enter the Graph\nThis is where I believe graph data structures could offer unique potential.\nTimetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Graph databases excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable.\nWhile algorithms generate optimised solutions, there remains a gap in post-generation analysis ‚Äì e.g.¬†the ability to delve into a timetable‚Äôs nuanced impacts on student and staff experience. Despite the acknowledged importance and impact of factors like room allocation and teaching period distribution, traditional optimisation-focused approaches lack the tools to explore these relationships in depth (Ceschia, Di Gaspero and Schaerf, 2023; Lindahl, 2017; Rudov√°, M√ºller and Murray, 2011), particularly in a real-world scenario.\nThis potential for deeper analysis motivates the exploration of graph data structures for enhancing timetable understanding and, ultimately, improving timetable quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  }
]