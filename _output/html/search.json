[
  {
    "objectID": "appendix-cypher6.html",
    "href": "appendix-cypher6.html",
    "title": "T: Soft Constraints",
    "section": "",
    "text": "Soft constraints in a timetabling context are (strong) preferences. They should be generally met and only violated when absolutely necessary, although there is an argument for a sliding scale of soft constraint adherence.\nFor example, a member of staff may be unavailable on Fridays, generally, but at a push can be available. Other examples might include ensuring that students have an opportunity to eat lunch by ensuring at least 30 minutes free time between 12:00-14:00 or minimising travel between activities.\nThis page contains cypher queries that can be used to identify where a timetabling soft constraint has been violated.\nExample soft constraints include:\n\nMinimal Idle Time (aka no large gaps): Minimise gaps in staff and student schedules (within reason).\nSpread Activities (aka maximum consecutive hours): Avoid clumping all activities for a student or staff member on one day.\nPreferred Times: Consider staff and student preferences for morning, afternoon, or evening classes\nTravel Time: Minimise the time students need to travel between consecutive classes (especially on large campuses), e.g. between building blocks or by lat/long\nLunch Breaks: Ensure students have sufficient time for lunch breaks.\n\n\nMinimal idle time\nIdentifying time gaps between scheduled activities is very complex and requires several steps, clauses and comprehensions within a single query:\n\ngrouping and sorting - activities are grouped by student and date, and sorted within groups to establish the sequence\ngap calculation - time difference between the end of one activity and the start of the next is calculated for consecutive pairs of activities within a day\nfiltering and aggregation - gaps are filtered based on threshold (e.g. 6 hours) and then the maximum gap for each day is identified\ndata restructuring - output is restructured.\n\n\nCypher logic for identifying gaps\nThe below is the logic for identifying gaps between activities, using an example student:\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWHERE s.stuID_anon = \"stu-10085720\"\nAND a.actStartDate = date(\"2022-10-03\")\nWITH s, a\nORDER BY a.actStartTime\n\n// Collecting the start and end times of the activities\nWITH s, collect({start: a.actStartTime, end: a.actEndTime}) AS times\n\n// Calculating the gaps in minutes between consecutive activities\nWITH s, times, \n     [i IN range(0, size(times)-2) | \n      duration.between(times[i].end, times[i+1].start).minutes / 60.0] AS gaps\n\n// Finding the maximum gap\nRETURN s.stuID_anon AS student, times, gaps, reduce(maxGap = 0.0, gap IN gaps | CASE WHEN gap &gt; maxGap THEN gap ELSE maxGap END) AS maxGap\n\n\n\nExample student with gaps between activities\n\n\n\n\nPython code on graph\nThe below code cell returns the first 5 rows where a 6 hour maximum gap has been violated.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\n// students with gaps between activities\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a\nORDER BY s.stuFirstName_anon, a.actStartDate, a.actStartTime\n// Group activities by student and date\nWITH s, a.actStartDate AS date, collect({start: a.actStartTime, end: a.actEndTime, activity: a}) AS times\n// calculating the gaps in hours between consecutive activities\nWITH s, date, times, \n     [i IN range(0, size(times)-2) | \n      {gap: duration.between(times[i].end, times[i+1].start).minutes / 60.0, \n       firstActivity: times[i].activity, \n       secondActivity: times[i+1].activity}] AS gaps\n// filtering gaps based on a threshold of 6 hours\nWITH s, date, gaps\nWHERE any(gapRecord IN gaps WHERE gapRecord.gap &gt; 6.0)\n// Finding the maximum gap that exceeds the threshold\nWITH s, date, reduce(maxGap = {gap: 0.0, firstActivity: null, secondActivity: null}, gapRecord IN gaps | \n    CASE WHEN gapRecord.gap &gt; maxGap.gap THEN gapRecord ELSE maxGap END) AS maxGapRecord\n// group by student to remove duplications\nWITH s.stuID_anon AS student, \n     collect({date: date, \n              activity1: maxGapRecord.firstActivity.actName,\n              activity1_time: maxGapRecord.firstActivity.actStartTime + \"-\" + maxGapRecord.firstActivity.actEndTime,\n              activity2: maxGapRecord.secondActivity.actName,\n              activity2_time: maxGapRecord.secondActivity.actStartTime + \"-\" + maxGapRecord.secondActivity.actEndTime,\n              maxGapInHours: maxGapRecord.gap}) AS gapRecords\n// Unwind the collected records\nUNWIND gapRecords AS record\n// Returning the result\nRETURN student, \n       record.date AS date,\n       record.activity1 AS activity1, \n       record.activity1_time AS activity1_time,\n       record.activity2 AS activity2,\n       record.activity2_time AS activity2_time,\n       record.maxGapInHours AS maxGapInHours\nORDER BY student, date\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"activity1\", \"activity1_time\", \n                                   \"activity2\", \"activity2_time\", \"maxGapInHours\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000002353E5C4990&gt;\nRunning query...\n\n        student        date                 activity1     activity1_time  \\\n0  stu-10270089  2023-01-30  UFCFGS-15-1 L_oc/01 &lt;29&gt;  09:00:00-10:00:00   \n1  stu-10270089  2023-02-06  UFCFGS-15-1 L_oc/01 &lt;30&gt;  09:00:00-10:00:00   \n2  stu-10270089  2023-02-13  UFCFGS-15-1 L_oc/01 &lt;31&gt;  09:00:00-10:00:00   \n3  stu-10270089  2023-02-20  UFCFGS-15-1 L_oc/01 &lt;32&gt;  09:00:00-10:00:00   \n4  stu-10270089  2023-02-27  UFCFGS-15-1 L_oc/01 &lt;33&gt;  09:00:00-10:00:00   \n\n                   activity2     activity2_time  maxGapInHours  \n0  UFCFES-30-1 L2_oc/01 &lt;29&gt;  17:30:00-19:00:00            7.5  \n1  UFCFES-30-1 L2_oc/01 &lt;30&gt;  17:30:00-19:00:00            7.5  \n2  UFCFES-30-1 L2_oc/01 &lt;31&gt;  17:30:00-19:00:00            7.5  \n3  UFCFES-30-1 L2_oc/01 &lt;32&gt;  17:30:00-19:00:00            7.5  \n4  UFCFES-30-1 L2_oc/01 &lt;33&gt;  17:30:00-19:00:00            7.5  \n\n\n\n\nPython to return total hours and block hours\nAlternatively, we can amend the query to return, for each date and student combination, their total scheduled hours, maximum consecutive block hours and the number of activities within the continuous block.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\n// calculates - total hours, max block hours, max block activities per day \n// to be used for max block hours and max block activities per day\n// logic - example\n\n// matches specified student and all attended activities\nMATCH (s:student {stuID_anon:\"stu-10085720\"})-[:ATTENDS]-&gt;(a:activity)\n\n// sorts activities by start date and then by start time within each date\nWITH s, a ORDER BY a.actStartDate, a.actStartTime\n\n// calculates total hours spent on activities for each date\nWITH s, a.actStartDate AS date, \n     SUM(a.actDurationInMinutes) / 60.0 AS totalHours,\n     // groups activities into blocks based on time overlaps\n     REDUCE(\n        blockInfo = [],\n        activity IN COLLECT(a)\n        | CASE\n            WHEN blockInfo = [] THEN [[activity]]\n            ELSE CASE\n                   WHEN head(last(blockInfo)).actEndTime &gt;= activity.actStartTime\n                     THEN blockInfo[..-1] + [last(blockInfo) + activity]\n                   ELSE blockInfo + [[activity]]\n                 END\n          END\n     ) AS blocks\n\n// unwinds the list of blocks, processing each block individually\nUNWIND blocks AS block\n\n// calculates the total duration in hours for each block\nWITH s, date, totalHours, blocks,\n     REDUCE(blockHours = 0.0, activity IN block | blockHours + activity.actDurationInMinutes) / 60.0 AS blockHours,\n     SIZE(block) AS blockActivities\n\n// returns aggregated results\nRETURN s.stuFullName_anon AS student, date, totalHours, \n       MAX(blockHours) AS blockHours,\n       MAX(blockActivities) AS blockActivities\nORDER BY date;\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"totalHours\", \"blockHours\", \"blockActivities\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000023541E17A50&gt;\nRunning query...\n\n       student        date  totalHours  blockHours  blockActivities\n0  Aaron Evans  2022-09-22         1.0         1.0                1\n1  Aaron Evans  2022-09-23         2.0         2.0                1\n2  Aaron Evans  2022-09-27         2.0         2.0                2\n3  Aaron Evans  2022-09-30         4.0         2.0                2\n4  Aaron Evans  2022-10-03         4.5         2.5                2\n\n\n\n\nExample Use case - identifying students with 5+ hours in a single block\nThis query returns the first five rows where a student has more than 5 consecutive scheduled hours on a date.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)&lt;-[:TEACHES]-(:staff) // Filter for teaching activities\nWITH s, a ORDER BY a.actStartDate, a.actStartTime\nWITH s, a.actStartDate AS date, \n     SUM(a.actDurationInMinutes) / 60.0 AS totalHours,\n     REDUCE(\n         blockInfo = [],\n         activity IN COLLECT(a)\n         | CASE\n             WHEN blockInfo = [] THEN [[activity]]\n             ELSE CASE\n                     WHEN head(last(blockInfo)).actEndTime &gt;= activity.actStartTime\n                         THEN blockInfo[..-1] + [last(blockInfo) + activity]\n                     ELSE blockInfo + [[activity]]\n                 END\n         END\n     ) AS blocks\nUNWIND blocks AS block\nWITH s, date, totalHours, blocks,\n     REDUCE(blockHours = 0.0, activity IN block | blockHours + activity.actDurationInMinutes) / 60.0 AS blockHours,\n     SIZE(block) AS blockActivities\nWHERE blockHours &gt; 5 // Filter for blocks with more than 5 hours\nRETURN s.stuFullName_anon AS student, date, totalHours, \n       blockHours,\n       blockActivities\nORDER BY date;\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"totalHours\", \"blockHours\", \"blockActivities\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000023541E20C50&gt;\nRunning query...\n\n             student        date  totalHours  blockHours  blockActivities\n0        Jacob Jones  2022-07-19         5.5         5.5                2\n1      Rachael Moore  2022-07-19         5.5         5.5                2\n2        Kayla Sharp  2022-07-19         5.5         5.5                2\n3         David Rose  2022-07-19         5.5         5.5                2\n4  Francisco Holland  2022-07-19         5.5         5.5                2\n\n\n\n\nTotal hours per day\nIn contrast, calculating simple total hours per day is achieved by:\nMATCH (s:student )-[:ATTENDS]-&gt;(a:activity)\nWITH s, a.actStartDate AS Date, SUM(a.actDurationInMinutes) / 60.0 AS totalHours\nRETURN s.stuFullName_anon AS Student, Date, totalHours\nORDER BY Date;\n\n\n\nTotal hours per day\n\n\n\n\nLongest consecutive block of activities per day\nWe can use the earlier cypher logic to identify the longest consecutive block of activities for a student, or student on a day, etc.\n\nMATCH (s:student {stuFullName_anon: \"Susan Lopez\"})-[:ATTENDS]-&gt;(a:activity {actStartDate: date(\"2022-09-27\")})\nWITH s, a \nORDER BY a.actStartTime\nWITH s, COLLECT(a) AS activities\nWITH s, activities,\n     REDUCE(\n       state = {currentBlock: {duration: 0, start: null, end: null}, longestBlock: {duration: 0, start: null, end: null}},\n       activity IN activities |\n         CASE\n           WHEN state.currentBlock.end IS NULL OR \n                activity.actStartTime &gt; state.currentBlock.end\n           THEN {\n             currentBlock: {\n               duration: activity.actDurationInMinutes,\n               start: activity.actStartTime,\n               end: activity.actEndTime\n             },\n             longestBlock: \n               CASE\n                 WHEN activity.actDurationInMinutes &gt; state.longestBlock.duration\n                 THEN {\n                   duration: activity.actDurationInMinutes,\n                   start: activity.actStartTime,\n                   end: activity.actEndTime\n                 }\n                 ELSE state.longestBlock\n               END\n           }\n           ELSE {\n             currentBlock: {\n               duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                         (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n               start: state.currentBlock.start,\n               end: activity.actEndTime\n             },\n             longestBlock: \n               CASE\n                 WHEN ((activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                       (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute)) &gt; state.longestBlock.duration\n                 THEN {\n                   duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                             (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n                   start: state.currentBlock.start,\n                   end: activity.actEndTime\n                 }\n                 ELSE state.longestBlock\n               END\n           }\n         END\n     ) AS finalState\nRETURN\n  s.stuFullName_anon AS stuName,\n  activities[0].actStartDate AS date,\n  finalState.longestBlock.duration AS longestConsecutiveBlockDuration,\n  finalState.longestBlock.start AS blockStartTime,\n  finalState.longestBlock.end AS blockEndTime\n\n\nExample - longest consecutive block for ‘Susan Lopez’ on 2022-09-27\nThe below finds the longest consecutive block in a day for a student:\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query (modified RETURN clause)\nquery = \"\"\"\nMATCH (s:student {stuFullName_anon: \"Susan Lopez\"})-[:ATTENDS]-&gt;(a:activity{actStartDate: date(\"2022-09-27\")})\nWITH s, a \nORDER BY a.actStartTime\nWITH s, COLLECT(a) AS activities\nWITH s, activities,\n   REDUCE(\n     state = {currentBlock: {duration: 0, start: null, end: null}, longestBlock: {duration: 0, start: null, end: null}},\n     activity IN activities |\n       CASE\n         WHEN state.currentBlock.end IS NULL OR \n              activity.actStartTime &gt; state.currentBlock.end\n         THEN {\n           currentBlock: {\n             duration: activity.actDurationInMinutes,\n             start: activity.actStartTime,\n             end: activity.actEndTime\n           },\n           longestBlock: \n             CASE\n               WHEN activity.actDurationInMinutes &gt; state.longestBlock.duration\n               THEN {\n                 duration: activity.actDurationInMinutes,\n                 start: activity.actStartTime,\n                 end: activity.actEndTime\n               }\n               ELSE state.longestBlock\n             END\n         }\n         ELSE {\n           currentBlock: {\n             duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                       (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n             start: state.currentBlock.start,\n             end: activity.actEndTime\n           },\n           longestBlock: \n             CASE\n               WHEN ((activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                     (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute)) &gt; state.longestBlock.duration\n               THEN {\n                 duration: (activity.actEndTime.hour * 60 + activity.actEndTime.minute) - \n                           (state.currentBlock.start.hour * 60 + state.currentBlock.start.minute),\n                 start: state.currentBlock.start,\n                 end: activity.actEndTime\n               }\n               ELSE state.longestBlock\n             END\n         }\n       END\n   ) AS finalState\nRETURN\ns.stuFullName_anon AS student,\nactivities[0].actStartDate AS date,\nfinalState.longestBlock.duration AS longestConsecutiveBlockDuration,\nfinalState.longestBlock.start AS blockStartTime,\nfinalState.longestBlock.end AS blockEndTime\n\"\"\"\n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n#  list to hold records\nrecords = []\nfor record in result:\n  records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"student\", \"date\", \"longestConsecutiveBlockDuration\", \"blockStartTime\", \"blockEndTime\"])\n\n# print \nprint(df.head(5))\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000023541DFA410&gt;\nRunning query...\n\n       student        date  longestConsecutiveBlockDuration  \\\n0  Susan Lopez  2022-09-27                              300   \n\n       blockStartTime        blockEndTime  \n0  13:00:00.000000000  18:00:00.000000000  \n\n\n\n\n\nMax hours in a day\nThis query calculates the total scheduled hours for each student on a day and returns the results ordered by date. This example filters for students who have more than 7 hours of activities in a day.\n// sum of activity durations\n// does not account for simultaneous activities (clashes) - so could be inflated, e.g. 12.5 hour students\n\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a.actStartDate AS Date, SUM(a.actDurationInMinutes) / 60.0 AS totalHours\nWHERE totalHours &gt; 7 // Set  maximum here\nRETURN s.stuFullName_anon AS Student, Date, totalHours\nORDER BY Date;\n\n\n\nMax hours in a day\n\n\n\n\nTravel time between activities\nThis query calculates the travel time between consecutive activities for a student on a specific date. It uses the lat/long coordinates of the buildings to calculate the distance and time taken to travel between them and a default walking speed of 1.4 m/s to calculate time.\nThis is a simple example and does not account for factors like traffic, walking speed, or other modes of transport.\n// Calculate travel time between consecutive activities for a student on a specific date\nMATCH (s:student {stuFullName_anon: \"David Johnson\"})-[:ATTENDS]-&gt;(a1:activity)-[:OCCUPIES]-&gt;(r1:room),\n      (s)-[:ATTENDS]-&gt;(a2:activity)-[:OCCUPIES]-&gt;(r2:room)\nWHERE a1.actEndTime = a2.actStartTime AND a1.actStartDate = a2.actStartDate AND a1 &lt;&gt; a2 AND\n      a1.actStartDate IN [date(\"2023-01-11\"), date(\"2022-09-27\"), date(\"2023-03-14\")] \nRETURN DISTINCT \n    s.stuFullName_anon, \n    a1.actName AS act1, a1.actStartDate AS date, a1.actStartTime+\"-\"+a1.actEndTime AS act1Times, a2.actStartTime+\"-\"+a2.actEndTime AS act2Times, a2.actName AS act2,\n    point.distance(r1.location, r2.location) AS distance,\n    round(point.distance(r1.location, r2.location) / 1.4) AS walkingTimeSeconds // Calculate walking time in seconds\n\n\n\nTravel time between activities\n\n\n\n\nLunch breaks\nIt might be expected that a student (or staff) has a lunch break. The cypher below calculates free and booked tie within a window, in this case 12:00 and 14:00. It can be used to find students who do not have a lunch break or count the number of days that a student does not have a lunch break.\n\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)\nWITH s, a\nORDER BY a.actStartDate, a.actStartTime\nWITH s, COLLECT(a) AS activities\nUNWIND activities AS activity\nWITH s.stuFullName_anon AS Student, activity.actStartDate AS Date, time(activity.actStartTime) AS StartTime, time(activity.actEndTime) AS EndTime, duration.between(time('12:00'), time('14:00')).minutes AS BreakWindow_12_14\nWITH Student, Date, BreakWindow_12_14, COLLECT([StartTime, EndTime]) AS Activities\nUNWIND Activities AS activity\nWITH Student, Date, BreakWindow_12_14, activity[0] AS StartTime, activity[1] AS EndTime\nWITH Student, Date, BreakWindow_12_14,\n     CASE\n       WHEN StartTime &gt;= time('14:00') OR EndTime &lt;= time('12:00') THEN 0\n       WHEN StartTime &lt; time('12:00') AND EndTime &gt; time('14:00') THEN BreakWindow_12_14\n       WHEN StartTime &gt;= time('12:00') AND StartTime &lt; time('14:00') THEN duration.between(StartTime, time('14:00')).minutes\n       WHEN EndTime &gt; time('12:00') AND EndTime &lt;= time('14:00') THEN duration.between(time('12:00'), EndTime).minutes\n     END AS BookedDurationMinutes\nRETURN Student, Date, BreakWindow_12_14, BreakWindow_12_14 - SUM(BookedDurationMinutes) AS FreeTimeMinutes, SUM(BookedDurationMinutes) AS BookedTimeMinutes\nORDER BY Date\n\n\n\nLunch breaks\n\n\nInterestingly, Michael Johnson has a negative lunch break! A quick look showed that there are actually two Michael Johnsons attending this class and they both have 30 minutes free time in the 2-hour lunch break window.\nBecause the query was written using student name, it is incorrectly aggregating the two students into one person as follows:\n\\[\n2 \\text{ hour lunch window} - (1.5 \\text{ hours/class} \\times 2 \\text{ students}) = -1 \\text{ hour}\n\\]\nTo remedy this, the query can be updated to use student ID or a different unique identifier. I would also like to update the anonymisation function in the ETL so that it does not duplicate names in the output.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Soft Constraints"
    ]
  },
  {
    "objectID": "appendix-cypher5.html",
    "href": "appendix-cypher5.html",
    "title": "R: Hard Constraints",
    "section": "",
    "text": "“Hard constraints” in a timetabling context are generally rules or conditions which cannot be violated. Violation would indicate non-viable timetable, e.g. a lecturer being scheduled to teach in two places simultaneously. In reality, hard constraints appear in timetables and are accepted with real-world workarounds.\nThis page contains cypher queries that can be used to identify where a timetabling hard constraint has been violated.\nExample hard constraints include:\n\nAll Activities Scheduled: Every lecture, tutorial, lab, etc., must have a designated time and place.\nNo Room Conflicts (aka room clash): Two activities cannot be scheduled in the same room at the same time.\nRoom Capacity Sufficient: The room assigned to an activity must accommodate the expected number of students\nPerson clashes: People, that is staff and students, cannot be allocated to two or more activities occurring at the same time.\n\nNo Staff Conflicts (aka staff clash)\nNo Student Conflicts (aka student clash)\n\nStaff Availability Respected: Activities cannot be scheduled during a staff member’s unavailable times (e.g., research days, meetings, unavailability pattern).\nCurriculum Requirements Met: Required courses must be offered at times when students can take them\n\n\nUnscheduled activities\nUnscheduled activities can be identified as follows. This query can be tweaked to also search for matches where the property equals ’’ - that is, a blank.\nMATCH (a:activity)\nWHERE a.actStartDate IS NULL \nOR a.actStartTime IS NULL \nOR a.actEndTime IS NULL\nRETURN a\n\n\nRoom clashes\nRoom or location clashes are where two or more activities are scheduled at the same datetime in the same space and this is not deliberate. These can be identified with the starter query below. The image clearly shows pairs of activities sharing one location. In reality, I suspect that these are deliberate clashes.\nMATCH (a1:activity)-[r1:OCCUPIES]-&gt;(r:room)&lt;-[r2:OCCUPIES]-(a2:activity)\nWHERE a1.actStartDate = a2.actStartDate AND a1 &lt;&gt; a2\n    AND (\n        (a1.actStartTime &lt;= a2.actStartTime AND a1.actEndTime &gt; a2.actStartTime)\n        OR \n        (a2.actStartTime &lt;= a1.actStartTime AND a2.actEndTime &gt; a1.actStartTime)\n    )\nRETURN a1, a2, r, r1, r2\n\n\n\nRoom Clashes\n\n\nThe same results have been returned as a table.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nMATCH (a1:activity)-[r1:OCCUPIES]-&gt;(r:room)&lt;-[r2:OCCUPIES]-(a2:activity)\nWHERE r.roomName IN [\"4Q50/51 FR\", \"4Q69 FR\", \"3E Maths Open Zone A\", \"3E12 FR\"] \n  AND a1.actStartDate = a2.actStartDate \n  AND a1 &lt;&gt; a2\n  AND (\n        (a1.actStartTime &lt;= a2.actStartTime AND a1.actEndTime &gt; a2.actStartTime)\n        OR \n        (a2.actStartTime &lt;= a1.actStartTime AND a2.actEndTime &gt; a1.actStartTime)\n      )\nRETURN a1.actName AS activity1, a2.actName AS activity2, \n       r.roomName AS room, a1.actStartDate AS date,\n       a1.actStartTime AS activity1_start, a1.actEndTime AS activity1_end,\n       a2.actStartTime AS activity2_start, a2.actEndTime AS activity2_end\n\"\"\" \n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"activity1\", \"activity2\", \"room\", \"date\", \n                                   \"activity1_start\", \"activity1_end\", \n                                   \"activity2_start\", \"activity2_end\"])\n\n# print\nprint(df)\n\n# close session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000001F76AA17DD0&gt;\nRunning query...\n\n                          activity1                         activity2  \\\n0          UFCF8P-15-M Sep W2_oc/01  UFCF8P-15-M Jan Cont W2_oc jt/01   \n1  UFCF8P-15-M Jan Cont W2_oc jt/01          UFCF8P-15-M Sep W2_oc/01   \n2          UFCF8P-15-M Sep W1_oc/01  UFCF8P-15-M Jan Cont W1_oc jt/01   \n3  UFCF8P-15-M Jan Cont W1_oc jt/01          UFCF8P-15-M Sep W1_oc/01   \n4                     Maths_E_oc/01                    Maths_E5_oc/01   \n5                    Maths_E5_oc/01                     Maths_E_oc/01   \n6                     Maths_E_oc/01              UFMFVV-30-3 DI_oc/01   \n7              UFMFVV-30-3 DI_oc/01                     Maths_E_oc/01   \n\n                   room        date     activity1_start       activity1_end  \\\n0            4Q50/51 FR  2022-12-13  16:00:00.000000000  17:30:00.000000000   \n1            4Q50/51 FR  2022-12-13  16:00:00.000000000  17:30:00.000000000   \n2               4Q69 FR  2022-12-13  14:00:00.000000000  15:30:00.000000000   \n3               4Q69 FR  2022-12-13  14:00:00.000000000  15:30:00.000000000   \n4  3E Maths Open Zone A  2022-11-09  09:00:00.000000000  17:00:00.000000000   \n5  3E Maths Open Zone A  2022-11-09  16:00:00.000000000  17:00:00.000000000   \n6               3E12 FR  2022-11-08  09:00:00.000000000  17:00:00.000000000   \n7               3E12 FR  2022-11-08  15:00:00.000000000  16:00:00.000000000   \n\n      activity2_start       activity2_end  \n0  16:00:00.000000000  17:30:00.000000000  \n1  16:00:00.000000000  17:30:00.000000000  \n2  14:00:00.000000000  15:30:00.000000000  \n3  14:00:00.000000000  15:30:00.000000000  \n4  16:00:00.000000000  17:00:00.000000000  \n5  09:00:00.000000000  17:00:00.000000000  \n6  15:00:00.000000000  16:00:00.000000000  \n7  09:00:00.000000000  17:00:00.000000000  \n\n\n\n\nRoom capacity exceeded\nThis query identifies activities where the number of students exceeds the room capacity. It includes an optional WHERE clause if looking at a specific date range.\nMATCH (r:room)&lt;-[r1:OCCUPIES]-(a:activity)&lt;-[:ATTENDS]-(s:student)\n//WHERE a.Date &gt;= date(\"2022-01-01\") AND a.Date &lt;= date(\"2022-06-30\") \nWITH r, a, count(s) as numStudents\nWHERE numStudents &gt; r.roomCapacity\nRETURN r, a.actStartDate, a.actName AS Activity, r.roomCapacity, numStudents - r.roomCapacity AS extraNeeded\nORDER BY extraNeeded DESC\nThese are the results in a dataframe:\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nMATCH (r:room)&lt;-[r1:OCCUPIES]-(a:activity)&lt;-[:ATTENDS]-(s:student)\n//WHERE a.Date &gt;= date(\"2022-01-01\") AND a.Date &lt;= date(\"2022-06-30\") \nWITH r, a, count(s) as numStudents\nWHERE numStudents &gt; r.roomCapacity\nRETURN DISTINCT r.roomName, r.roomType,  a.actName AS Activity, \n       r.roomCapacity, numStudents - r.roomCapacity AS extraNeeded\nORDER BY extraNeeded DESC\n\"\"\" \n\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df\ndf = pd.DataFrame(records, columns=[\"roomName\", \"roomType\", \"Activity\", \n                                   \"roomCapacity\", \"extraNeeded\"])\n\n# print\nprint(\"Printing first 5 records...\\n\")\nprint(df.head())\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x000001F76C5B6750&gt;\nRunning query...\n\nPrinting first 5 records...\n\n               roomName  roomType       Activity  roomCapacity  extraNeeded\n0              3E006 FR  FAC STUD  Maths_E_oc/01             4           88\n1              3E007 FR  FAC STUD  Maths_E_oc/01             4           88\n2  3E Maths Open Zone B  FAC STUD  Maths_E_oc/01            12           80\n3               3E28 FR  TEACHING  Maths_E_oc/01            24           68\n4               3E12 FR    PC LAB  Maths_E_oc/01            30           62\n\n\n\n\nStudent clashes\nThis query identifies students who are scheduled to attend two or more activities at the same time. Identifying clashes is a complex undertaking and it is one where the graph structure in terms of nodes, properties and relationships could potentially make a significant different to performance.\nThe reason for the complexity is that you need to look for overlapping times between two activities for each date, for each student. The query to achieve this and the ensuing calculations will vary significantly depending on the structure and syntax.\nBecause of this, I explored the student clash scenario in more detail here: Student Clashes\nMATCH (s:student)-[:ATTENDS]-&gt;(a1:activity)\nWITH s, a1\nMATCH (s)-[:ATTENDS]-&gt;(a2:activity) \nWHERE a1 &lt;&gt; a2 \n  AND a1.actStartDate = a2.actStartDate \n  AND (a1.actStartTime &lt; a2.actEndTime AND a1.actEndTime &gt; a2.actStartTime)  \n  AND NOT (a1.actStartTime = a2.actEndTime OR a1.actEndTime = a2.actStartTime) \n  AND a1.actName &lt; a2.actName  // Ensure only one direction of the pair is returned\nRETURN s.stuFirstName_anon AS Student, \n       a1.actStartDate AS ClashDate, \n       a1.actName AS Activity1, \n       a1.actStartTime + \"-\" + a1.actEndTime AS Timeslot1, \n       a2.actName AS Activity2, \n       a2.actStartTime + \"-\" + a2.actEndTime AS Timeslot2\nORDER BY Student, ClashDate;\n\n\n\nStudent Clashes",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Hard (timetabling) Constraints"
    ]
  },
  {
    "objectID": "02-01-comparison.html",
    "href": "02-01-comparison.html",
    "title": "Graph vs Relational Data Models",
    "section": "",
    "text": "As outlined in the project aims, my hypothesis is that graph-based approaches have the potential to offer new insights and efficiencies in timetable analysis. This section will briefly explore the theoretical underpinnings of graph data structures and their application to the domain of university timetabling.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#relational-models",
    "href": "02-01-comparison.html#relational-models",
    "title": "Graph vs Relational Data Models",
    "section": "Relational Models",
    "text": "Relational Models\n\nTables, Joins and the Limits of Interconnectedness\nRelational databases, using SQL1 as their query language, have long been the go-to for managing data, including timetabling information. They structure data into tables, where rows represent instances of entities (e.g. individual rooms, staff, or students) and columns represent entity attributes (name, capacity, email, etc.).\nRelationships between these entity tables are established through foreign keys, forming links between tables. This often involves intermediary “relationship” tables to handle the many-to-many nature of timetabling data (e.g., a student attends many activities, and an activity has many students) (Khan et al., 2023;Sokolova, Gómez and Borisoglebskaya, 2020).\nWhile robust and well-understood, relational databases start to show their limitations when dealing with the highly interconnected nature of timetables:\n\nJoin Complexity: Even seemingly simple queries, like “find students attending a specific lecturer’s class in a particular building,” require joining multiple tables. As queries become more nuanced, the number of joins increases, often impacting performance, especially with large datasets.\nRigidity: Relational databases rely on a predefined schema, making them less adaptable to evolving needs. Adding new entities or relationships is not possible without disrupting existing queries and applications.\n\n\n\n\nExample Simple Entity Relationship Diagram",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#graph-models",
    "href": "02-01-comparison.html#graph-models",
    "title": "Graph vs Relational Data Models",
    "section": "Graph Models",
    "text": "Graph Models\n\nEmbracing interconnectedness\nIn contrast to the rigid table structure of relational databases, graph databases offer a more intuitive and flexible approach for representing interconnected data like timetables. They utilise:\n\nNodes: Represent entities. These are often the “nouns” like activity, room, staff, student.\nEdges: Represent relationships between nodes. These are often the “verbs” like TAUGHT_BY, ENROLLED_IN, SCHEDULED_AT, OWNED_BY.\n\n\n\n\nSimple Graph Data Model\n\n\nThis node-and-edge structure inherently reflects how timetabling elements connect. Instead of relying on cumbersome joins, relationships are directly encoded in the data model itself. This results in several advantages:\n\nNatural Representation: Graph databases visually and conceptually mirror the relationships inherent in timetables, making them easier to understand and query.\nRelationship-Centric Queries: Graph databases are optimised for traversing and analysing relationships. Queries that would require multiple joins in a relational database often become significantly simpler and faster in a graph database.\nFlexibility: The schema-less or schema-optional nature of most graph databases allows for greater flexibility in data modeling. New entities or relationships can be added effortlessly without impacting existing structures or queries (Nan and Bai, 2019; Webber, Eifrem and Robinson, 2013).",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#comparing-queries",
    "href": "02-01-comparison.html#comparing-queries",
    "title": "Graph vs Relational Data Models",
    "section": "Comparing queries",
    "text": "Comparing queries\nExample Insight: Find all students attending a specific lecturer’s class in a particular building\nRepresentative queries have been written in SQL and Cypher to find this insight. The SQL query is much longer and requires six joins, each coming at a computational cost.\n\nSQL Query\nSELECT DISTINCT ss.[FirstName], ss.[LastName], ss.[Email]\nFROM [RDB_MAIN2223].[rdowner].[V_STUDENTSET] ss\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STUDENTSET] acts ON ss.[Id] = acts.[StudentSetId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY] a ON acts.[ActivityId] = a.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_LOCATION] al ON a.[Id] = al.[ActivityId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_LOCATION] l ON al.[LocationId] = l.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_BUILDING] b ON l.[BuildingId] = b.[Id] \nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STAFF] ast ON a.[Id] = ast.[ActivityId]\nWHERE ast.[StaffId] = 'StaffID'  \n  AND b.[Name] = 'BuildingName'; \n\n\nCypher Query\nIn contrast, the Cypher query pattern is much simpler - written in one line (MATCH pattern). The query is more intuitive and easier to understand, especially for those unfamiliar with the database schema.\n\nMATCH (s:Student)-[:ATTENDS]-&gt;(a:Activity)&lt;-[:TEACHES_ON]-(st:Staff), \n      (a:Activity)-[:TAKES_PLACE_IN]-&gt;(r:Room)\nWHERE st.last_name = \"LecturerLastName\" AND r.building = \"BuildingName\"\nRETURN s.first_name, s.last_name, s.email",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#key-differences-and-implications",
    "href": "02-01-comparison.html#key-differences-and-implications",
    "title": "Graph vs Relational Data Models",
    "section": "Key Differences and Implications",
    "text": "Key Differences and Implications\n\n\n\n\n\n\n\n\n\nFeature\nRelational Model\nGraph Model\n\n\n\n\nData Structure\nTables with rows and columns\nNodes and edges\n\n\nSchema\nRigid, predefined\nFlexible, schema-less or schema-optional\n\n\nRelationship Handling\nForeign keys, joins\nDirect connections (edges)\n\n\nQuery Performance\nCan be slow for relationship-heavy queries\nOptimised for traversing relationships, potentially faster\n\n\nData Modeling\nLess intuitive for interconnected data\nNaturally represents complex relationships\n\n\nAdaptability\nLess adaptable to schema changes\nMore flexible, accommodates evolving data needs\n\n\n\n\nThese advantages position graph databases as a powerful tool for uncovering insights hidden within complex, interconnected datasets like university timetables.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#footnotes",
    "href": "02-01-comparison.html#footnotes",
    "title": "Graph vs Relational Data Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStructured Query Language (Wikipedia contributors, 2024)↩︎",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "appendix-cypher.html",
    "href": "appendix-cypher.html",
    "title": "M: Cypher Queries",
    "section": "",
    "text": "These pages collate Cypher1 queries used in the development of this project. The queries are grouped by the type of operation they perform, such as creating nodes, relationships, or querying the graph. The queries are presented in a format that can be copied and pasted directly into a Neo4j browser or other Cypher-compatible interface.\nThey represent a starting point for further exploration and development of the graph database. The queries are not exhaustive, and there are many more possibilities for querying and analysing the data.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#constraints",
    "href": "appendix-cypher.html#constraints",
    "title": "M: Cypher Queries",
    "section": "Constraints",
    "text": "Constraints\nConstraints in Neo4j are used to enforce rules on the graph data, such as ensuring that certain properties are unique or that nodes have specific properties. Constraints can be used to maintain data integrity and prevent duplicate or inconsistent data. For example, we would want to enforce uniqueness constraints on most nodes and relationships, so that we do not duplicate students or allocations, etc.\n\n\n\nConstraints",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#indexes",
    "href": "appendix-cypher.html#indexes",
    "title": "M: Cypher Queries",
    "section": "Indexes",
    "text": "Indexes\nIndexes in Neo4j are used to speed up queries by allowing the database to quickly locate nodes or relationships based on a property value. They will depend heavily on graph structure and specific use-cases. For example, if performing regular searches on (activity{actStartTime}) it is probably beneficial to create an index on this property.\nSearch performance indexes include RANGE, FULLTEXT, and POINT. Range indexes are the default and support most queries. Text indexes, like FULLTEXT, are used for string based searches and optimised for queries containing operators like CONTAINS or STARTS WITH. Point indexes are used for spatial queries and are optimised for latitude and longitude properties.\nIndexes in Graph\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\nSHOW INDEXES;\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\nfor record in result:\n    print(record)\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000023D862F1890&gt;\nRunning query...\n\n&lt;Record id=7 name='activity_clash_index' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartDate', 'actStartTime', 'actEndTime'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=19 name='graphid_demoRoom_uniq' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['graphid'] indexProvider='range-1.0' owningConstraint='graphid_demoRoom_uniq' lastRead=neo4j.time.DateTime(2024, 8, 20, 18, 16, 44, 231000000, tzinfo=&lt;UTC&gt;) readCount=3&gt;\n&lt;Record id=4 name='index_12b79beb' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actEndTime'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=6 name='index_207d313' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['id'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=8 name='index_241bd22f' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomName'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 21, 14, 8, 33, 622000000, tzinfo=&lt;UTC&gt;) readCount=160&gt;\n&lt;Record id=9 name='index_2d375d70' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomLatitude'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 645000000, tzinfo=&lt;UTC&gt;) readCount=107&gt;\n&lt;Record id=0 name='index_343aff4e' state='ONLINE' populationPercent=100.0 type='LOOKUP' entityType='NODE' labelsOrTypes=None properties=None indexProvider='token-lookup-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 21, 14, 8, 44, 304000000, tzinfo=&lt;UTC&gt;) readCount=117816&gt;\n&lt;Record id=10 name='index_43c5c824' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['room'] properties=['roomLongitude'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 646000000, tzinfo=&lt;UTC&gt;) readCount=106&gt;\n&lt;Record id=2 name='index_6acb9a84' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartDate'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 21, 14, 8, 45, 504000000, tzinfo=&lt;UTC&gt;) readCount=117&gt;\n&lt;Record id=5 name='index_7ade165f' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actModSplusID'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 18, 40, 6, 638000000, tzinfo=&lt;UTC&gt;) readCount=148&gt;\n&lt;Record id=1 name='index_f7700477' state='ONLINE' populationPercent=100.0 type='LOOKUP' entityType='RELATIONSHIP' labelsOrTypes=None properties=None indexProvider='token-lookup-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 15, 13, 53, 33, 557000000, tzinfo=&lt;UTC&gt;) readCount=134&gt;\n&lt;Record id=3 name='index_f86013e' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['activity'] properties=['actStartTime'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 21, 9, 8, 41, 307000000, tzinfo=&lt;UTC&gt;) readCount=18&gt;\n&lt;Record id=18 name='lat_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['lat'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=17 name='lon_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['lon'] indexProvider='range-1.0' owningConstraint=None lastRead=None readCount=0&gt;\n&lt;Record id=16 name='rm_cat_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['rm_cat'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 20, 18, 16, 44, 250000000, tzinfo=&lt;UTC&gt;) readCount=3&gt;\n&lt;Record id=15 name='rm_type_demoRoom' state='ONLINE' populationPercent=100.0 type='RANGE' entityType='NODE' labelsOrTypes=['demoRoom'] properties=['rm_type'] indexProvider='range-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 20, 18, 16, 44, 258000000, tzinfo=&lt;UTC&gt;) readCount=3&gt;\n&lt;Record id=11 name='room_fulltext_index' state='ONLINE' populationPercent=100.0 type='FULLTEXT' entityType='NODE' labelsOrTypes=['room'] properties=['roomName', 'roomDescription', 'roomType', 'roomCapacity'] indexProvider='fulltext-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 20, 18, 16, 43, 786000000, tzinfo=&lt;UTC&gt;) readCount=99&gt;\n&lt;Record id=12 name='room_geo_index' state='ONLINE' populationPercent=100.0 type='FULLTEXT' entityType='NODE' labelsOrTypes=['room'] properties=['roomLatitude', 'roomLongitude'] indexProvider='fulltext-1.0' owningConstraint=None lastRead=neo4j.time.DateTime(2024, 8, 20, 18, 16, 43, 803000000, tzinfo=&lt;UTC&gt;) readCount=12&gt;\n\n\n\nCreating Indexes\nThis is the general syntax for creating an INDEX and some examples.\n// General syntax\nCREATE INDEX FOR (n:NodeLabel) ON (n.propertyName)\n\n// Examples\nCREATE INDEX FOR (a:activity) ON (a.actStartDate);\nCREATE INDEX FOR (a:activity) ON (a.actStartTime);\nCREATE INDEX FOR (a:activity) ON (a.actEndTime);\nCREATE INDEX FOR (a:activity) ON (a.actModSplusID);\nCREATE INDEX FOR (a:activity) ON (a.id);\n// composite index for clashes\nCREATE INDEX activity_clash_index FOR (a:activity) ON (a.actStartDate, a.actStartTime, a.actEndTime);\n\nCREATE INDEX FOR (r:room) ON (r.roomName);\nCREATE INDEX FOR (r:room) ON (r.roomLatitude);\nCREATE INDEX FOR (r:room) ON (r.roomLongitude);\n\nCREATE SPATIAL INDEX room_location_index FOR (r:demoRoom) ON (r.location)\nCREATE SPATIAL INDEX demoroom_location_index FOR (r:room) ON (r.location)",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "appendix-cypher.html#footnotes",
    "href": "appendix-cypher.html#footnotes",
    "title": "M: Cypher Queries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Cypher is a declarative graph query language that allows for expressive and efficient data querying in a property graph” (Wikipedia contributors, 2024)↩︎",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Cypher Queries"
    ]
  },
  {
    "objectID": "04-03-timetable-cypher.html",
    "href": "04-03-timetable-cypher.html",
    "title": "Implementing TQI",
    "section": "",
    "text": "Prototype queries have been identified to identify constraint violations. Several of these queries are quite complex but their final form will dependent on the use-case as well as the graph data model.\nAs a simple example, the following query identifies students with back-to-back activities in different buildings, highlighting a potential travel time issue:\n// Identify students with back-to-back activities in different buildings\nMATCH (s:Student)-[:ATTENDS]-&gt;(a1:Activity)-[:NEXT]-&gt;(a2:Activity)\nWHERE a1.endTime = a2.startTime AND a1.building &lt;&gt; a2.building\nRETURN s.name, a1.name, a2.name, a1.building, a2.building\nOr we could craft a query to calculate the travel time between back-to-back activities:\n// Calculate travel time between consecutive activities for a student on a specific date\nMATCH (s:student {stuFullName_anon: \"David Johnson\"})-[:ATTENDS]-&gt;(a1:activity)-[:OCCUPIES]-&gt;(r1:room),\n      (s)-[:ATTENDS]-&gt;(a2:activity)-[:OCCUPIES]-&gt;(r2:room)\nWHERE a1.actEndTime = a2.actStartTime AND a1.actStartDate = a2.actStartDate AND a1 &lt;&gt; a2 AND\n      a1.actStartDate IN [date(\"2023-01-11\"), date(\"2022-09-27\"), date(\"2023-03-14\")] \nRETURN DISTINCT \n    s.stuFullName_anon, \n    a1.actName AS act1, a1.actStartDate AS date, a1.actStartTime+\"-\"+a1.actEndTime AS act1Times, a2.actStartTime+\"-\"+a2.actEndTime AS act2Times, a2.actName AS act2,\n    point.distance(r1.location, r2.location) AS distance,\n    round(point.distance(r1.location, r2.location) / 1.4) AS walkingTimeSeconds // Calculate walking time in seconds\n\n\n\nTravel time between activities\n\n\nSee Cypher Queries - Hard Constraints and Cypher Queries - Soft Constraints for more examples.\n\nPenalty and Reward System\nOne way of implementing this is to store the quality score as a property on the relevant node (student, programme, room, etc.). Starting with a baseline score, the quality score is dynamically updated by subtracting penalties and adding rewards based on the specific metrics calculated. The weighting of these penalties and rewards can be adjusted to reflect institutional priorities.\nUsing the back-to-back activities example above, we can imagine using either distance or walkingTimeSeconds and a sliding scale to calculate a penalty. For example, if the walking time is greater than 5 minutes, a penalty of -3 points could be applied with the penalty increasing as the walking time increases.\nFurther examples:\nNo lunch break: -5 points\nBack-to-back activities 5+ minutes apart: -3 points per instance\nActivity clash: -10 points\nRoom at full capacity: -2 points\nHigh room utilisation rate: +2 points",
    "crumbs": [
      "Home",
      "Timetable Metrics",
      "Implementing Metrics"
    ]
  },
  {
    "objectID": "03-06a-load.html",
    "href": "03-06a-load.html",
    "title": "Google Load",
    "section": "",
    "text": "The free cloud instance of Neo4j (Aura) requires that csv files are stored in public cloud storage like Google Drive or Dropbox.\nTherefore, my project requires an intermediary step.\n\n\n\n\n\n\n\ngoogle_drive_storage\n\n\n\nsource_files\n\nLocal Files\n(./{hostkeys}/process)\n\n\n\ngdrive_api\n\n\n\nGoogle Drive API\n\n\n\nsource_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nget_folder\n\nGet Folder Details\n\n\n\ngdrive_api-&gt;get_folder\n\n\n\n\n\ncreate_folder\n\nCreate Folder if Needed\n\n\n\nget_folder-&gt;create_folder\n\n\n\n\n\nupload_nodes\n\nUpload Files\nMatching Node Pattern\nto './{hostkeys}/node'\n\n\n\ncreate_folder-&gt;upload_nodes\n\n\n\n\n\nupload_relationships\n\nUpload Files\nMatching Relationship Pattern\nto './{hostkeys}/relationships'\n\n\n\ncreate_folder-&gt;upload_relationships\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nupload_nodes-&gt;process_done\n\n\n\n\n\nupload_relationships-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nFile storage and directories are controlled via Config. I created a publicly shared folder in Google drive which contains all project csvs:\n- root: Google Drive folder\n  - hostkeys (automatically created, unless override) \n    - nodes\n    - relationships\n\n\n\nScreenshot of Google Drive folder and files for MSc Data Science (INB112): note that files were created by Google API",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Google Drive Load"
    ]
  },
  {
    "objectID": "03-01-overview.html",
    "href": "03-01-overview.html",
    "title": "Data Engineering Overview",
    "section": "",
    "text": "A main objective of my project is the development of a data pipeline which efficiently and securely transfers selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j).\nThis section provides an overview of the pipeline architecture, fundamental design principles, implementation approach and key learning takeaways.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "ETL Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#high-level-architecture",
    "href": "03-01-overview.html#high-level-architecture",
    "title": "Data Engineering Overview",
    "section": "High-level Architecture",
    "text": "High-level Architecture\nThe data pipeline consists of these core stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: CSV files are processed, cleaned, transformed, merged, and anonymised using Python.\nIntermediate Storage: Processed CSVs are uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. 𝗘𝗫𝗧𝗥𝗔𝗖𝗧: Filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. 𝗧𝗥𝗔𝗡𝗦𝗙𝗢𝗥𝗠: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. 𝐔𝐏𝐋𝐎𝐀𝐃\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. (Optional) Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. 𝗟𝗢𝗔𝗗: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "ETL Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#design-principles",
    "href": "03-01-overview.html#design-principles",
    "title": "Data Engineering Overview",
    "section": "Design Principles",
    "text": "Design Principles\nSeveral “best practices” in data handling, processing, and database management were incorporated in developing this ETL. The data pipeline is built on several core design principles:\n\n\n\nDesign Principles\n\n\nI started with a strong sense of what I wanted to build - a modular, scalable, secure and configurable design - however, what exactly this meant was only discovered during the development process.\nGiven project constraints - deadline, word-limits, resources, data, technology - it is fair to say that compromises were made. That said, it was important that the final artefact is one that can be developed further for specific business use-cases.\n\nSecurity and Data Protection\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\nModularity, Scalability and Automation\n\n\nDistinct, interoperable modules (extract, process, upload, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\nError Handling and Logging\n\n\nRobust error handling\nComprehensive logging for troubleshooting and auditing\n\n\n\nUser configurable\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "ETL Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#implementation-approach",
    "href": "03-01-overview.html#implementation-approach",
    "title": "Data Engineering Overview",
    "section": "Implementation Approach",
    "text": "Implementation Approach\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nCrucial aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. See Technology Stack for more details.\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that components are behaving as expected.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "ETL Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#upcoming-sections",
    "href": "03-01-overview.html#upcoming-sections",
    "title": "Data Engineering Overview",
    "section": "Upcoming Sections",
    "text": "Upcoming Sections\nThe following sections will delve into specific implementation details of each stage, demonstrating how these principles are put into practice, before reflecting on lessons learned and potential future enhancements.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "ETL Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Graph Data Models for Timetabling Insights",
    "section": "",
    "text": "Supervisor: Xiaodong Li\nProgramme: MSc Data Science\nAbstract:\nTimetables are central to the daily experience of university students and staff. Timetables also influence resource utilisation and play a key role in institutional efficiency. In short, timetables are critical to the individual experience and the overall success of an institution. But timetables are also contentious - there is no ‘perfect’ timetable, only a series of compromises that balance competing priorities.\nThis project explores the use of graph data models to provide deeper insights into timetables. The aim is to investigate the viability of graph-based approaches for enhanced timetabling analytics and reporting. The objectives include designing a graph data model, developing a configurable ETL pipeline, and discussing how graph-based analysis could contribute to quantitaviely measuring timetable quality by introducing the concept of a Timetable Quality Index.\nThe project is a proof-of-concept, and the results are intended to inform future research and development in the area of timetabling analytics. The project is implemented primarily in Python, using the Neo4j cloud instance graph database, and the results and documentation are presented in a Quarto website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "appendix-tech-stack.html",
    "href": "appendix-tech-stack.html",
    "title": "B: Technology Stack",
    "section": "",
    "text": "This project used a variety of tools, applications, programming languages, and so on. Below is a high-level record of the ‘tech stack’ - the what and why:\n\nProgramming\n\nPython - Main programming language.\nSQL - SELECT queries to extract source data from relational database.\nCypher - Querying language for Neo4j Graph Databases.\nBatch - Windows command language to handle yaml files and multi-format rendering.\nVSCode - Main IDE (Integrated Development Environment).\n\n\n\nDocumentation\n\nQuarto - Open-source technical publishing system.\nJupyter - Open-source application for interactive notebooks.\nZotero - Open-source reference management system.\n\n\n\nVisualisation\n\nGraphviz - Open-source graph visualisation application.\nMermaid - Open-source JavaScript diagramming tool.\nArrows - Neo4j Labs diagramming tool.\n\n\n\nVersioning\n\nGithub - Web-based platform for version control and collaboration using Git.\n\n\n\nPython Libraries\nSeveral Python libraries were explored in the development of this prototype. The below libraries are the ones used in the current implementation.\n\nDirectory/File Handling\n\nos - Interacting with the operating system for tasks like creating, deleting, and navigating directories and files.\nglob - Finding files and directories based on pattern matching.\nio - Working with input/output streams for reading and writing data.\n\n\n\nData Handling\n\npandas - Handling tabular data for analysis and manipulation.\njson - Encoding and decoding JSON (JavaScript Object Notation) data.\n\n\n\nTyping and Logging\n\ntyping - Adding type hints to code for better code readability, maintainability, and static type checking.\nlogging - Configuring and managing logging for application.\ntime - Working with time-related functions, potentially used for logging timestamps.\n\n\n\nDatabase Connectivity\n\nkeyring - Securely storing and retrieving passwords and other sensitive information.\npyodbc - Connecting to and interacting with SQL databases using the Open Database Connectivity (ODBC) standard.\nneo4j - Interacting with Neo4j graph databases.\n\n\n\nGoogle API Integration\n\ngoogleapiclient - Interacting with various Google APIs.\ngoogle.oauth2 - Handling OAuth 2.0 authentication for accessing Google services securely.\n\n\n\nAnonymisation\n\nrandom - Generating random numbers and making random choices.\nhashlib - Implementing various secure hash and message digest algorithms.\nFaker - Generating fake data for testing and development purposes.",
    "crumbs": [
      "Home",
      "Appendices",
      "Technology Stack"
    ]
  },
  {
    "objectID": "appendix-supervision2.html",
    "href": "appendix-supervision2.html",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "",
    "text": "Significant progress in the backend and more proof-of-concept in front end.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#summary",
    "href": "appendix-supervision2.html#summary",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "",
    "text": "Significant progress in the backend and more proof-of-concept in front end.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#accomplishments",
    "href": "appendix-supervision2.html#accomplishments",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nProject Management\n\npost supervisor meeting notes\n\nData Collection:\n\nAnonymisation function for staff/student personal data developed and tested.\nMSc Data Science cohort isolated into separate csv files\nData extraction pipeline designed, developed, tested. This allows for extracting data filtered by programme on demand.\n\nmodularised, scalable, configurable, efficient pipeline\n\nTransformation pipeline (preprocessing, anonymising)\n\nMore than half way completed - same principles\nneed to add staff, student, activity nodes and test outcomes\nneed to add relationships and test.\n\nLoading to Neo4j pipeline developed - suitable for version 1\n\nAnalysis / Wrangling:\n\nMore advanced cypher queries - constraint violation queries developed using cypher for version 1. some are very complex. more testing needed.\nlist of insights which can be derived\n\nModel Development:\n\nComparing different representations of time\n\nResults:\nPipeline development\nCypher queries for version 1\nconversation with business owners to validate work",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#next-steps",
    "href": "appendix-supervision2.html#next-steps",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goal: What is goal of next fortnight?\n\nwrite up and benchmark v1 notes\nfinish developing and testing pipeline:\n\nextract\ntransform\nload\n\ndocument pipeline - written and visual (mermaid?)\ndevelop cypher queries for version 2\ndesign (theory) timetable quality index\nconsider scaling dataset\nstart writing project notes",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#issuesblockers",
    "href": "appendix-supervision2.html#issuesblockers",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nDigital certificates on machines preventing load\nNeo4j Aura (free) limitations - loading issues\n\nMethodological: Concerns about approach or analysis?\n\nSpending a lot of time getting ETL ‘right’\nNot sure about balance of project and what I will deliver at the end\n\nData-Related: Issues with data quality, access, or quantity?\n\nWorking on pipelines which will mean I can scale accordingly (within constraints of free instance)",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision2.html#post-meeting-notes",
    "href": "appendix-supervision2.html#post-meeting-notes",
    "title": "Y: Fortnightly Update - 2024-06-24",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\nMy superviser and I spoke about where I am at in the project at the moment and next steps. We discussed what I am attempting to do and why, including graph data structures, proof-of-concept, etc. and that it is becoming a data engineering project. I stated that my timeline lookes to complete a robust data ETL pipeline by the middle of July, with a view to shifting towards more work within Neo4j from the end of July to the middle of August.\nWe agreed to meet in two weeks.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 2"
    ]
  },
  {
    "objectID": "appendix-supervision.html",
    "href": "appendix-supervision.html",
    "title": "W: Supervision",
    "section": "",
    "text": "Project supervision was undertaken by Dr. Xiaodong Li who was very supportive and made himself available to me in-person and by email, etc.\nIn general, we met for 30-60 minutes every fortnight where I presented progress, issues/blockers, planned next steps, etc. and we discussed the project in general. Dr. Li was very engaged and helpful and I am indebted to his supervision. Following each meeting, I wrote up notes.\nI have shared a few examples in the following pages. Please note that links within these pages will not necessarily work as they point to working files and repositories.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "W: Supervision"
    ]
  },
  {
    "objectID": "appendix-cypher7.html",
    "href": "appendix-cypher7.html",
    "title": "U: Rooms and Spaces",
    "section": "",
    "text": "The timetabling database contains some information about rooms and spaces on campus, but the master data systems contains much more. I have added a few rooms and properties from the master source, as an experiemnt.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Rooms and Spaces"
    ]
  },
  {
    "objectID": "appendix-cypher7.html#room-geo-location",
    "href": "appendix-cypher7.html#room-geo-location",
    "title": "U: Rooms and Spaces",
    "section": "Room Geo-location",
    "text": "Room Geo-location\n\nImport locations from file\n\n\n\nimported locations from file\n\n\n\n\nload cypher\nUNWIND $nodeRecords AS nodeRecord\nWITH *\nWHERE NOT nodeRecord.`graphid` IN $idsToSkip AND NOT nodeRecord.`graphid` IS NULL\nMERGE (n: `demoRoom` { `graphid`: nodeRecord.`graphid` })\nSET n.`#rm.bl_id` = nodeRecord.`#rm.bl_id`\nSET n.`fl_id` = toInteger(trim(nodeRecord.`fl_id`))\nSET n.`rm_id` = nodeRecord.`rm_id`\nSET n.`rm_type` = nodeRecord.`rm_type`\nSET n.`dp_id` = nodeRecord.`dp_id`\nSET n.`bu_id` = nodeRecord.`bu_id`\nSET n.`rm_std` = nodeRecord.`rm_std`\nSET n.`rm_use` = nodeRecord.`rm_use`\nSET n.`site_id` = nodeRecord.`site_id`\nSET n.`dv_id` = nodeRecord.`dv_id`\nSET n.`asb_risk` = nodeRecord.`asb_risk`\nSET n.`rm_cat` = nodeRecord.`rm_cat`\nSET n.`lon` = toFloat(trim(nodeRecord.`lon`))\nSET n.`lat` = toFloat(trim(nodeRecord.`lat`))\nSET n.`roomHostKey` = nodeRecord.`roomHostKey`;\n\n\nThoughts\nSome rooms have longitude and latitude, which have been used to calculate distance. The screenshot below shows locations on Frenchay campus - you can clearly see rooms aligned into the shape our the buildings. \n\n\n\nClose of up B Block rooms on Frenchay Campus\n\n\nAn alternative view includes locations in City Campus.\n\n\n\nFrenchay and City Campus rooms\n\n\nThe above images show the potential of using coordinates although there is a lot more to consider including actual locations, accuracy, missing data, and coordinate transformations.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Rooms and Spaces"
    ]
  },
  {
    "objectID": "appendix-cypher7.html#room-use",
    "href": "appendix-cypher7.html#room-use",
    "title": "U: Rooms and Spaces",
    "section": "Room use",
    "text": "Room use\nBeing able to measure room usage - utilisation, frequency, occupancy - is a key metric for the university.\n// room occupancy\nMATCH (room:room )&lt;-[:OCCUPIES]-(activity:activity)\nWITH room, activity, \n     activity.actDurationInMinutes / 60.0 AS occupancyHours\nRETURN room.roomName AS room, \n       SUM(occupancyHours) AS totalOccupancyHours\n\n\n\nRoom occupancy\n\n\n// frequency\nMATCH (room:room )&lt;-[:OCCUPIES]-(activity:activity)\nRETURN room.roomName AS room, \n       COUNT(activity) AS totalActivities\n\n\n\nRoom frequency\n\n\n// simple utilisation\nMATCH (room:room {roomName: \"2Q12 FR\"})&lt;-[:OCCUPIES]-(activity:activity)\nWITH room, activity, \n     CASE \n         WHEN activity.actStartTime.hour = activity.actEndTime.hour \n         THEN 1 \n         ELSE activity.actEndTime.hour - activity.actStartTime.hour + 1 \n     END AS occupiedHours\nUNWIND range(activity.actStartTime.hour, activity.actEndTime.hour) AS hour\nRETURN room.roomName AS room, \n       hour AS hourBlockStart, \n       hour + 1 AS hourBlockEnd,\n       SUM(CASE WHEN hour IN range(activity.actStartTime.hour, activity.actEndTime.hour) THEN 1 ELSE 0 END) AS utilizationCount,\n       occupiedHours AS totalOccupiedHours\nORDER BY hourBlockStart\n\n\n\nRoom utilisation\n\n\nThe above queries need to be developed further as they are very simplistic. For example, the utilisation query does not take into account the day of the week, nor does it consider the number of people in the room. The occupancy query does not consider the capacity of the room.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Rooms and Spaces"
    ]
  },
  {
    "objectID": "appendix-cypher3.html",
    "href": "appendix-cypher3.html",
    "title": "P: General Queries",
    "section": "",
    "text": "This page contains a selection general queries which can be used to explore the graph database. The queries are designed to provide insights into the data and relationships between nodes.\n\nList all nodes\nThe following query lists all nodes in the graph.\n\n\n\n\n\n\nCaution\n\n\n\nConsider size of graph and limits in settings before running this query.\n\n\nMATCH (n)\nRETURN n\n\n\n\nAll Nodes\n\n\n\n\ndatatype of property\nProperties in a graph have datatypes which will enable different operations and there for insights. The query below returns the datatype of a node property.\n/* return datatype of actStartTime on activity node */\n\nMATCH (a:activity)\nRETURN DISTINCT apoc.meta.cypher.type(a.actStartTime) as actStartTimeType\n\n\n\nDatatype of Property\n\n\n\n\nunique properties\nA node or relationship can potentially have many properties. The query below lists the properties of a node - in this case, activity.\n// List unique properties for a Node\n\nMATCH (a:activity)\nUNWIND keys(a) AS propertyKey\nRETURN COLLECT(DISTINCT propertyKey) AS propertyKeys\n//RETURN DISTINCT propertyKey as propertyKeys\n\n\n\nUnique Property Values\n\n\n\n\nnode labels without relationships\nGraph databases are all about the relationships between nodes. It can be useful identifying nodes without relationships as they could indicate a problem with the data, data loading mechanism or be the outliers you want to identify.\nFor example, in a timetabling scenario, we would expect all nodes to be related to another node. However, we can see that several node labels are orphans. In the proof-of-concept, these results are expected or deliberate, due to the source data.\n// Find node labels without relationships and their count\n\nMATCH (n)\nWHERE NOT EXISTS(()-[]-(n)) AND NOT EXISTS((n)-[]-())\nRETURN DISTINCT labels(n) AS nodeLabels, count(n) AS nodeCount\n\n\n\nNode labels without Relationships\n\n\n\n\nnodes without relationships - aka orphans\nInstead of returning a count of nodes without relationships per node label we can return the nodes as a graph or a table:\n// Find nodes without relationships\n\nMATCH (n)\nWHERE NOT EXISTS(()-[]-(n)) AND NOT EXISTS((n)-[]-())\nRETURN n\n\n\n\nNodes Without Relationships\n\n\n\n\nstudents without activities\nIn the timetabling context, we would expect students to be allocated to activities. It turns out that we have 219 students without activities. A bit more investigation indidates that they are all from a particular programme of study run.\n// Students without Activities\nMATCH (s:student)\nWHERE NOT (s)-[:ATTENDS]-&gt;()\nRETURN s\n\n\n\nStudents Without Activities\n\n\n\n\nactivityType without activity\nActivities can have a activity type - the graph model could have activity type as a property or as a relationship. The query below finds activity typewithout activity. The decision may be to delete these orphaned nodes as they may cause problems with some calculations. They would be created if they become required in the future.\n// Activities without Rooms\n\nMATCH (at:activityType)\nWHERE NOT (at)&lt;-[:HAS_TYPE]-()\nRETURN at;\n\n\n\nActivityType Without Room\n\n\n\n\nactivities without rooms\nThe graph has over 1500 activity instances without rooms. Most of these will be deliberate - online, virtual sessions - but we may want to query the graph to identify those where a room is expected.\n// Activities without Rooms\n\nMATCH (a:activity)\nWHERE NOT (a)-[]-&gt;(:room)\nRETURN a;\n\n\n\nActivities Without Rooms",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "General Queries"
    ]
  },
  {
    "objectID": "appendix-cypher1.html",
    "href": "appendix-cypher1.html",
    "title": "N: Creating Nodes and Relationships",
    "section": "",
    "text": "Creating nodes and relationships is the first step in building a graph database. Nodes represent entities in the graph, such as students, rooms, or activities. Relationships connect nodes and represent the connections between them.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#creating-nodes",
    "href": "appendix-cypher1.html#creating-nodes",
    "title": "N: Creating Nodes and Relationships",
    "section": "Creating Nodes",
    "text": "Creating Nodes\nNodes are created using the CREATE clause in Cypher. The general syntax for creating a node is:\nCREATE (n:NodeLabel {propertyName: propertyValue, ...})\nWhere:\n\nn is the node variable\nNodeLabel is the label assigned to the node\npropertyName is the property name\npropertyValue is the value assigned to the property\n... represents additional properties\n\n\nExample: Creating a Student Node\nCREATE (s:Student {studentID: '123456', studentName: 'Alice'...})",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#creating-relationships",
    "href": "appendix-cypher1.html#creating-relationships",
    "title": "N: Creating Nodes and Relationships",
    "section": "Creating Relationships",
    "text": "Creating Relationships\nRelationships are created using the CREATE clause in Cypher. The general syntax for creating a relationship is:\nMATCH (n1:NodeLabel1), (n2:NodeLabel2)\nWHERE n1.propertyName = propertyValue1 AND n2.propertyName = propertyValue2\nCREATE (n1)-[r:RELATIONSHIP_TYPE]-&gt;(n2)\nWhere:\n\nn1 and n2 are the node variables\nNodeLabel1 and NodeLabel2 are the labels assigned to the nodes\npropertyName is the property name\npropertyValue is the value assigned to the property\nr is the relationship variable\nRELATIONSHIP_TYPE is the type of relationship\n-&gt; represents the direction of the relationship\nMATCH is used to find the nodes to connect, optional\nWHERE is used to filter the nodes, optional\nCREATE is used to create the relationship\n... represents additional properties\n\n\nExample: Creating a Relationship Between a Student and an Activity\nMATCH (s:Student {studentID: '123456'}), (a:Activity {activityID: '789'})\nCREATE (s)-[r:ATTENDS]-&gt;(a)",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher1.html#relationships-created-after-etl",
    "href": "appendix-cypher1.html#relationships-created-after-etl",
    "title": "N: Creating Nodes and Relationships",
    "section": "Relationships created after ETL",
    "text": "Relationships created after ETL\nThe following relationships were applied to my proof-of-concept graph after loading data using the ETL, using node properties. Delete code also supplied below.\n\n\n\n\n\n\nCaution\n\n\n\nUse code with caution, especially DELETE code. Run pattern matching and investigate results before committing to delete.\n\n\n\n(student)-[REGISTERED_ON]-&gt;(programme)\n\n// Create REGISTERED_ON relationship between student and programme nodes\n\n// Match student and programme nodes based on matching properties\nMATCH (s:student), (p:programme)\nWHERE s.stuProgSplusID = p.posSplusID\n\n// Create REGISTERED_ON relationship\nMERGE (s)-[:REGISTERED_ON]-&gt;(p)\n// Delete REGISTERED_ON relationships\nMATCH (:student)-[r:REGISTERED_ON]-&gt;(:programme)\nDELETE r\n\n\n(student)-[ENROLLED_ON]-&gt;(module)\n// Create ENROLLED_ON relationship between students and modules \n\n// Match student, activity, and module nodes based on ATTEND and BELONGS_TO relationships\nMATCH (s:student)-[:ATTENDS]-&gt;(a:activity)-[:BELONGS_TO]-&gt;(m:module)\n\n// Create ENROLLED_ON relationship\nMERGE (s)-[:ENROLLED_ON]-&gt;(m)\n// Delete ENROLLED_ON relationships\nMATCH (:student)-[r:ENROLLED_ON]-&gt;(:module)\nDELETE r\n\n\n(activity)-[HAS_TYPE]-&gt;(activity_type)\n// Create HAS_TYPE relationship between activity and activityType nodes\n\n// Match activity and activityType nodes based on matching properties\nMATCH (a:activity), (at:activityType)\nWHERE a.actTypeName = at.actTypeDescription\n\n// Create HAS_TYPE relationship\nMERGE (a)-[:HAS_TYPE]-&gt;(at)\n// Find activities without an activityType\n\nMATCH (a:activity)\nWHERE NOT EXISTS ((a)-[:HAS_TYPE]-&gt;(:activityType))\nRETURN a\n// Delete HAS_TYPE relationships between activity and activityType nodes\n\nMATCH (a:activity)-[r:HAS_TYPE]-&gt;(at:activityType)\nDELETE r\n\n\n(module)-[HAS_OWNING_DEPT]-&gt;(department)\n// Create HAS_OWNING_DEPT relationship between module and department nodes\n\n// Match module and department nodes based on matching properties\nMATCH (m:module), (d:department)\nWHERE m.modDepartment = d.deptName\n\n// Create HAS_OWNING_DEPT relationship\nMERGE (m)-[:HAS_OWNING_DEPT]-&gt;(d)\n// Delete HAS_OWNING_DEPT relationships between module and department nodes\n\nMATCH (m:module)-[r:HAS_OWNING_DEPT]-&gt;(d:department)\nDELETE r\n\n\n(programme)-[HAS_OWNING_DEPT]-&gt;(department)\n// Create HAS_OWNING_DEPT relationship between programme and department nodes\n\n// Match programme and department nodes based on matching properties\nMATCH (p:programme), (d:department)\nWHERE p.posDepartment = d.deptName\n\n// Create HAS_OWNING_DEPT relationship\nMERGE (p)-[:HAS_OWNING_DEPT]-&gt;(d)\n// Delete HAS_OWNING_DEPT relationships between programme and department nodes\n\nMATCH (p:programme)-[r:HAS_OWNING_DEPT]-&gt;(d:department)\nDELETE r",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Creating Nodes and Relationships"
    ]
  },
  {
    "objectID": "appendix-code6-load.html",
    "href": "appendix-code6-load.html",
    "title": "L: Neo4j Load",
    "section": "",
    "text": "Graph Timetable - Quarto - Load\n\nload_schema\nload_utils\nload_nodes\nload_relationships\nconvert_properties\nset_display_properties\nload_main",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Neo4j Load"
    ]
  },
  {
    "objectID": "appendix-code4-gdrive.html",
    "href": "appendix-code4-gdrive.html",
    "title": "J: Google Drive Load",
    "section": "",
    "text": "Graph Timetable - Quarto - Upload to Google Drive\n\ngdrive_upload\ngoogle_drive_utils",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Google Drive Load"
    ]
  },
  {
    "objectID": "appendix-code2-sql.html",
    "href": "appendix-code2-sql.html",
    "title": "H: Extract-SQL",
    "section": "",
    "text": "Graph Timetable - Quarto - SQL Queries\n\ncreate_temp_tables.sql\nnode-activity-by-pos-temp.sql\nnode-activityType-by-pos-temp.sql\nnode-dept-all.sql\nnode-module-by-pos-temp.sql\nnode-pos-by-pos-temp.sql\nnode-room-by-pos-temp.sql\nnode-staff-by-pos-temp.sql\nnode-student-by-pos-temp.sql\nrel-activity-module-by-pos-temp.sql\nrel-activity-room-by-pos-temp.sql\nrel-module-programme-by-pos-temp.sql\nrel-staff-activity-by-pos-temp.sql\nrel-student-activity-by-pos-temp.sql",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Extract-SQL"
    ]
  },
  {
    "objectID": "appendix-code.html",
    "href": "appendix-code.html",
    "title": "F: ETL Code Gists",
    "section": "",
    "text": "Complete code for the Data Pipeline can be found in these Github gists or on the following pages.\nGitHub Gists:\nGraph Timetable - Quarto - Config and Misc\nGraph Timetable - Quarto - SQL Queries\nGraph Timetable - Quarto - Extract\nGraph Timetable - Quarto - Transform\nGraph Timetable - Quarto - Upload to Google Drive\nGraph Timetable - Quarto - Load\nGraph Timetable - Quarto - Config and Misc\nGraph Timetable - Quarto - SQL Queries\nGraph Timetable - Quarto - Extract\nGraph Timetable - Quarto - Transform\nGraph Timetable - Quarto - Upload to Google Drive\nGraph Timetable - Quarto - Load",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "ETL Code"
    ]
  },
  {
    "objectID": "appendix-blue-sky.html",
    "href": "appendix-blue-sky.html",
    "title": "Blue Sky Opportunities",
    "section": "",
    "text": "Here I imagine how the timetabling data could be combined with other data sources to unlock insights. This does not mean that any of this should be done. It also does not mean that it can’t be done very effectively using non-graph solutions. It is very much blue-skies thinking in that it is not constrainted by the realities of data, systems, technology, or policy.\nIf any of these ideas are taken forward, the usual data governance, ethics and privacy considerations need to be addressed, in addition to the technical and practical challenges like data quality, integration, and scalability."
  },
  {
    "objectID": "appendix-blue-sky.html#popular-module-combinations-and-student-choice",
    "href": "appendix-blue-sky.html#popular-module-combinations-and-student-choice",
    "title": "Blue Sky Opportunities",
    "section": "Popular Module Combinations and Student Choice:",
    "text": "Popular Module Combinations and Student Choice:\n\nData: Analyse student enrolment data within programmes and identify popular module combinations. Combine with student feedback and performance (outcomes) and engagement data.\n\nInsights: Understand student preferences and identify frequently chosen module combinations. This can inform program design, curriculum development, and elective module offerings.\nAction: Adapt programme structures to align with student choices, and offer targeted module recommendations based on popular combinations and individual student interests."
  },
  {
    "objectID": "appendix-blue-sky.html#modules-and-library-resources",
    "href": "appendix-blue-sky.html#modules-and-library-resources",
    "title": "Blue Sky Opportunities",
    "section": "Modules and Library Resources:",
    "text": "Modules and Library Resources:\n\nData: Integrate module data with library resources such as reading lists, e-resources, and physical collections.\nInsights: Identify the most frequently accessed resources for each module and assess their impact on student learning outcomes. Make recommendations for additional resources or support services.\nAction: Enhance module delivery by aligning resources with learning objectives, providing targeted support, and improving access to relevant materials."
  },
  {
    "objectID": "appendix-blue-sky.html#student-travel-and-engagement",
    "href": "appendix-blue-sky.html#student-travel-and-engagement",
    "title": "Blue Sky Opportunities",
    "section": "Student Travel and Engagement:",
    "text": "Student Travel and Engagement:\n\nData: Include student term-time addresses and combine with attendance and engagement data. Combine with public transport data, e.g. bus and train schedules. Insights: Understand the impact of travel distances on student engagement and academic performance. Identify potential transport barriers for specific student groups or areas. This is particularly relevant to Bristol where there is a large student population and accommodation challenges. Action: Optimise timetable scheduling to minimise travel distances, particularly for students with long commutes. Explore targeted transportation solutions or support services."
  },
  {
    "objectID": "appendix-blue-sky.html#equitable-access-and-outcomes-analysis",
    "href": "appendix-blue-sky.html#equitable-access-and-outcomes-analysis",
    "title": "Blue Sky Opportunities",
    "section": "Equitable Access and Outcomes Analysis:",
    "text": "Equitable Access and Outcomes Analysis:\n\nData: Combine timetable data with various pre and post student datasets, like SES, POLAR, Free Lunch data, and graduate outcomes.\nInsights: Identify if timetable structures disproportionately impact specific student groups. Does this impact their academic performance or future prospects?\nAction: Use insights to proactively address disparities in access and outcomes by adjusting timetables, providing targeted support, or informing policy changes."
  },
  {
    "objectID": "appendix-blue-sky.html#self-serve-timetable-changes-and-student-behavior",
    "href": "appendix-blue-sky.html#self-serve-timetable-changes-and-student-behavior",
    "title": "Blue Sky Opportunities",
    "section": "Self-Serve Timetable Changes and Student Behavior:",
    "text": "Self-Serve Timetable Changes and Student Behavior:\n\nData: Analyse student self-serve timetable changes1 and compare original and modified schedules.\nInsights: Gain insights into student preferences and identify common reasons for timetable modifications. This can inform timetable design and optimise scheduling processes - this could be particularly useful to inform the Timetable Quality Index.\nAction: Address common issues identified through timetable adjustments, such as frequent clashes or gaps in scheduling. Optimize the self-serve process based on student behavior."
  },
  {
    "objectID": "appendix-blue-sky.html#facilities-optimisation-and-space-utilisation",
    "href": "appendix-blue-sky.html#facilities-optimisation-and-space-utilisation",
    "title": "Blue Sky Opportunities",
    "section": "Facilities Optimisation and Space Utilisation:",
    "text": "Facilities Optimisation and Space Utilisation:\n\nData: Integrate estates/facilities datasets like master location data, wifi hits, etc. with timetable and occupancy data.\nInsights: Identify underutilised spaces and peak usage times. Optimise space allocation and improve campus resource management. Understand flow of students and staff around campus.\nAction: Adjust timetables to balance space utilisation, explore flexible learning environments, and inform future campus development plans."
  },
  {
    "objectID": "appendix-blue-sky.html#student-clustering-and-community-building",
    "href": "appendix-blue-sky.html#student-clustering-and-community-building",
    "title": "Blue Sky Opportunities",
    "section": "Student Clustering and Community Building:",
    "text": "Student Clustering and Community Building:\n\nData: Incorporate student demographic (with appropriate data privacy safeguards)\nInsights: Identify clusters of students with shared characteristics or interests. This can inform targeted support services, social events, and community-building initiatives.\nAction: Foster a sense of belonging by connecting students with similar backgrounds or interests. Tailor support services to meet the needs of specific student groups."
  },
  {
    "objectID": "appendix-blue-sky.html#module-and-timeslot-recommendations",
    "href": "appendix-blue-sky.html#module-and-timeslot-recommendations",
    "title": "Blue Sky Opportunities",
    "section": "Module and Timeslot Recommendations:",
    "text": "Module and Timeslot Recommendations:\n\nData: Combine timetable, enrolment, student performance, and feedback data.\nInsights: Develop a recommender system to suggest module combinations and timeslots based on student interests, past performance, and peer recommendations.\nAction: Personalise the student experience and improve module selection.\n\n\nUnpopular Activity Analysis:\n\nData: Analyse attendance data and identify unpopular activities, correlating them with time, location, staff, and other properties.\nInsights: Understand the factors contributing to low attendance and identify potential areas for improvement.\nAction: Adjust timetable scheduling, explore alternative teaching approaches, or provide additional support to address identified issues.\n\n\n\nIdentifying Anomalies and Opportunities:\n\nData: Apply graph algorithms and machine learning techniques to identify patterns, anomalies, and trends in combined datasets.\nInsights: Discover hidden opportunities for improvement, detect potential issues proactively, and gain deeper insights into complex relationships between various factors.\nAction: Address identified issues, optimise processes, and leverage opportunities for innovation and continuous improvement."
  },
  {
    "objectID": "appendix-blue-sky.html#footnotes",
    "href": "appendix-blue-sky.html#footnotes",
    "title": "Blue Sky Opportunities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStudents are able to make certain changes to their timetables, at activity and module level. These changes are recorded and can be analysed.↩︎"
  },
  {
    "objectID": "05-final-thoughts.html",
    "href": "05-final-thoughts.html",
    "title": "Final Thoughts",
    "section": "",
    "text": "As I reflect on this project, I am struck by my progress in realising the original objectives I set out to achieve and what it took to get here. Through a combination of perseverance, problem-solving, and a relentless drive to deliver a tangible solution, I am proud to say that I have successfully:\nBut I could not have done it without others. I am grateful for the support, guidance and encouragement I have received from many people along the way.",
    "crumbs": [
      "Home",
      "Final Thoughts"
    ]
  },
  {
    "objectID": "05-final-thoughts.html#reflections-on-journey",
    "href": "05-final-thoughts.html#reflections-on-journey",
    "title": "Final Thoughts",
    "section": "Reflections on Journey",
    "text": "Reflections on Journey\nThis journey has had a little bit of everything: challenges, setbacks, breakthroughs, and moments of clarity. It has tested my limits, pushed me to grow, and allowed me to create something that I believe can make a meaningful impact. I have learned a lot about myself, my capabilities, and the power of perseverance; I have gained new skills and insights; I have developed a deeper understanding of where I want to go from here.\nAdmittedly, the project was ambitious, and I found myself struggling with to navigate the ever-expanding scope, not knowing when to ‘stop.’ But, I am proud of what I have achieved and learned, as well as where I am right now - when I can comfortably draw a line under this project as a proof-of-concept, knowing that it provides a foundation for future work and exploration.\nIn short, it has been fulfilling and I did what I wanted to do - something exploratory, practical, new, challenging, and impactful.\nBut it is especially rewarding to receive feedback from subject matter experts, such as the timetabling data manager at UWE, who said:\n\n\nOpens new reporting and analytics opportunities\nVery useful graphical representation of the relations within the database\nHuge time saving comparing to current SQL methods\nEasily adjustable and scalable\nDate and time represented in much better way\nMakes reporting of timetable clashes such an easy task",
    "crumbs": [
      "Home",
      "Final Thoughts"
    ]
  },
  {
    "objectID": "05-final-thoughts.html#looking-ahead-the-future-of-graph-at-universities",
    "href": "05-final-thoughts.html#looking-ahead-the-future-of-graph-at-universities",
    "title": "Final Thoughts",
    "section": "Looking Ahead: The Future of Graph at Universities",
    "text": "Looking Ahead: The Future of Graph at Universities\nLooking ahead, I am confident that the potential of graph databases in the realm of timetabling analysis has only begun to be explored. My project has only scratched the surface but there is a vast reserve of untapped opportunity. By continuing to explore and refine the concepts introduced in this project, Higher Edducation Institutions can unlock new levels of insight to improve efficiency, agility, and student satisfaction.\nAnd it does not only apply to timetabling datasets.\nUniversities hold a significant amount of interconnected data that can be leveraged to improve the student experience, make more informed decisions, and drive positive change. The possibilities are endless[^1] - to illustrate, I have included some blue-skies thinking in the Appendix.",
    "crumbs": [
      "Home",
      "Final Thoughts"
    ]
  },
  {
    "objectID": "04-02-timetable-agg.html",
    "href": "04-02-timetable-agg.html",
    "title": "Metric Aggregations",
    "section": "",
    "text": "With the individual metrics calculated, the next step is to aggregate these into meaningful scores at different levels. This could be at the student level, programme level, department level, or even at the room or building object.\nThe metrics used and their weightings will depend on the use-case and the priorities of the institution. For example, a student-level score could be used to identify students with particularly poor timetables, while a programme-level score could be used to compare the quality of timetables across different programmes, and a room-level score could be used to identify rooms that are underutilised or overbooked or are otherwise unsuitable.\nThis allows for a more nuanced understanding of timetable quality and can help identify areas for improvement.\n\nStudent-level\nEach student node can have a quality score reflecting their individual timetable experience based on assigned activities and associated penalties.\n\n\nProgramme-level\nBy aggregating student scores within a programme, we gain insights into the overall quality experienced by students in that programme.\n\n\nOther groupings\nScores can be aggregated at various levels, such as by department, room type, or time slot, to identify potential areas for improvement.",
    "crumbs": [
      "Home",
      "Timetable Metrics",
      "Metric Aggregations"
    ]
  },
  {
    "objectID": "03-07-reflections.html",
    "href": "03-07-reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "I was always wary that the data engineering portion of my project might be too ambitious in both scale and scope. However, the reality of its magnitude became increasingly apparent.\nYet, despite my initial awareness, I found myself continually expanding the project’s boundaries, often pushing for a “gold-plated” solution rather than acknowledging when certain aspects were “good enough.” 1 This tendency towards scope creep, while driven by a desire for excellence, has significantly increased the project’s complexity and time requirements.\nThe learning curve has been exceptionally steep. I’ve had to rapidly acquire proficiency in a diverse range of technologies and tools: Python, Neo4j, Google APIs, Quarto, and GraphViz. This intensive learning process, while challenging, has also been incredibly rewarding. My technical toolkit has expanded far beyond my initial expectations - but this also contributed to the continuously expanding scope, as each new skill opened possibilities for further enhancement and the necessity for on-the-fly troubleshooting.\nUnexpected challenges have been a constant companion. From deleted servers and databases to access issues to discrepancies between development environments (such as missing certificates), I’ve encountered a wide array of unforeseen obstacles. These issues have necessitated the development of strong troubleshooting skills and a flexible approach to problem-solving.\nWhile often frustrating, these challenges have also provided valuable learning opportunities, pushing me to deepen my understanding of the systems and technologies I’m working with.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-07-reflections.html#lessons-learned",
    "href": "03-07-reflections.html#lessons-learned",
    "title": "Reflections",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nScope management is crucial: Work on recognising when a solution is “good enough” and resist the urge to continually expand scope. Set clear boundaries at the start and be prepared to reassess and adjust plans when necessary.\nEmbrace modularisation from the beginning: Avoid the temptation to create oversized code blocks. Maintain a list of “future enhancements” to prevent immediate implementation of every idea.\nBalance documentation with development: Document sufficiently during the development process, but save comprehensive documentation for appropriate milestones. This approach maintains progress while ensuring proper record-keeping.\nView obstacles as learning opportunities: Embrace continuous learning and see challenges as chances to grow. Invest time in understanding the right technologies and approaches, particularly focusing on modularisation.\nCelebrate incremental progress: Recognise and appreciate small achievements throughout the development process. This helps maintain motivation and provides a clearer sense of overall progress.\n\nThe next section will start looking at the newly transferred data in the graph database.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-07-reflections.html#footnotes",
    "href": "03-07-reflections.html#footnotes",
    "title": "Reflections",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs my manager often needs to remind me…↩︎",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-02-approach.html",
    "href": "03-02-approach.html",
    "title": "Data Engineering Approach",
    "section": "",
    "text": "I followed an interative, agile-inspired approach despite being a team of one. This method allowed for flexibility, continuous improvement and the opportunity to adapt to new insights during the process (Beck, K., et al. 2001).\nThe bulk of my effort was spent prototyping, testing and reviewing with each iteration resulting in a new challenge, issue, or opportunity.\n\n\n\nIterative Development Approach\n\n\n\nInitial Planning and Requirements Gathering\nThe development cycle began with initial high-level planning and requirements gathering, where I imagined how each stage should work, trying to bear in mind future-proofing and repeatability principles.\nI defined core functionality for each module (extraction, transformation, loading) and outlined initial technical requirements and constraints. The planning documentation was maintained in Quarto and markdown files in a centralised repository for project information.\n\n\nPrototyping\nFollowing initial planning, rapid prototyping was undertaken for each module:\n\nSQL prototyping for data extraction queries\nPython prototyping for data transformation and processing logic\nNeo4j prototyping for graph database schema and loading procedures\n\nThis stage allowed for quick exploration of different approaches and early identification of potential challenges as well as giving me the confidence to continue with my exploration.\n\n\nComponent-Based Development and Testing\n\nEach module (extraction, transformation, loading) was developed separately with a view to distinct “handovers”\nAn iterative, component-based testing approach was employed\nWhile formal unit tests were not always created, each component was thoroughly tested for functionality\n\nThis approach allowed for continuous progress while maintaining a focus on component-level quality. It was during this phase that I started expanding configuration, logging and error-handling options - and I am glad I did!\n\n\nIntegration -&gt; Review -&gt; Demo -&gt; Feedback -&gt; Repeat\nAs components reached a (more) stable state, they were integrated and reviewed:\n\nComponents were combined to form larger functional units\nIntegrated functionality was occasionally demonstrated to subject matter experts (e.g. data manager)\nFeedback was gathered on functionality, usability, and alignment with requirements\n\nInsights gained from reviews, demonstrations and ongoing development were continuously fed back into the process. New requirements or modifications were documented, for example updates to SQL SELECT statements and data model interpretations.\nEach change required decisions - but I did not always make the right ones!\n\n\nVersion Validation and Documentation\nAt pivotal junctures, e.g., when a stable version was achieved:\n\nEnd-to-end validation of the entire pipeline was performed.\nResults were documented in notebooks, including opportunities for improvement.\nBugs and opportunities were logged for future iterations.\n\n\n\nContinuous Learning and Adaptation\nLearning and adaptation became central to the project. Each iteration brought new insights, often through trial and error and certainly through unintended consequences or unforeseen complications. Early challenges included the need to modularise components before they became unmanageable and resisting the temptation to make overly ambitious changes. With practice, I became better at recognising when refactoring was necessary.\nDeveloping the ETL was not a linear journey. There were many moments of frustration, periods of seemingly endless, painstaking troubleshooting, and a constant battle against the urge to over-deliver. Yet, with each stumble, the process itself became more refined, transforming into a powerful tool for identifying and resolving issues.\nWhile core MVP (minimum viable product) requirements remained relatively stable (I set them after all!), iterating allowed me to seize opportunities for enhancement. Each chance to modularise, parameterise, or fine-tune sparked an almost compulsive drive for improvement, pushing the pipeline beyond its initial scope.\nUltimately it all resulted in a robust, flexible solution that can adapt (relatively) gracefully to unforeseen challenges and serve as the starting point for future opportunities.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Approach"
    ]
  },
  {
    "objectID": "02-02c-expand.html",
    "href": "02-02c-expand.html",
    "title": "Model Expansion",
    "section": "",
    "text": "The true power of the graph model lies in its extensibility. Introducing additional nodes and properties allows for a more comprehensive representation and enables more sophisticated analysis. The resulting graph model will depend on desired use cases and performance requirements but the following are some potential expansions to the basic model:\n\nPotential Expansions:\n\nOrganisational Units: Include departments, colleges, or schools to analyse timetabling within organisational structures.\nCurriculum Data: Incorporate modules and programmes to understand the interconnectedness of courses and student enrolment patterns.\nActivity Types: Differentiate between lectures, seminars, labs, etc., for a more granular analysis of teaching and learning activities.\nActivity Delivery: Understand teaching delivery (virtual, in-person, hybrid, drop-in).\nStudent Attributes: Add properties like “international student”, “reasonable adjustment flag”, “first-year student”.\n\nThe below image (click to enlarge) shows a graph model augmented with additional data contained within the timetable database. It is much richer and therefore more complex, but this allows for richer analysis.\n\n\n\nExample of Expanded Timetable Graph Model",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Model Expansion"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html",
    "href": "02-02-graph-timetable.html",
    "title": "Graph Data Model for Timetabling",
    "section": "",
    "text": "Having discussed advantages of graph databases for representing interconnected data, this section delves into the specifics of a proposed graph data model tailored for university timetabling.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#an-iterative-approach",
    "href": "02-02-graph-timetable.html#an-iterative-approach",
    "title": "Graph Data Model for Timetabling",
    "section": "An Iterative Approach",
    "text": "An Iterative Approach\nDue to flexibility, creating graph data models is an iterative process: design -&gt; build -&gt; test -&gt; review -&gt; revise -&gt; …and repeat.\nMy first model was small in scope, incorporating minimal nodes and properties in an MVP1 approach. Eventually, my expanded model was created in a cloud-instance of Neo4j Aura.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "href": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "title": "Graph Data Model for Timetabling",
    "section": "Core Nodes - Building Blocks",
    "text": "Core Nodes - Building Blocks\nAt its core, the timetable model revolves around four key entities represented as nodes:\n\n\n\n\nNode\nProperty\nDescription\nData Type\n\n\n\n\nStudent\nfirstName\nLegal first name\nstring\n\n\n\nlastName\nLegal last name\nstring\n\n\n\nstudentID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nLecturer\nfirstName\nFirst name\nstring\n\n\n\nlastName\nLast name\nstring\n\n\n\nstaffID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nRoom\nname\nRoom name\nstring\n\n\n\nsplusID\nTimetable URN\ninteger\n\n\nActivity\nname\nActivity name\nstring\n\n\n\ndescription\nActivity description\nstring\n\n\n\nstartTime\nScheduled start time\ndatetime\n\n\n\nendTime\nScheduled end time\ndatetime\n\n\n\ndate\nDate of activity\ndate",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "href": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "title": "Graph Data Model for Timetabling",
    "section": "Relationships - Connecting the Dots",
    "text": "Relationships - Connecting the Dots\nThe core nodes are interconnected through relationships that reflect the dynamics of a timetable:\n\n(Student)-[IS_ALLOCATED_TO]-&gt;(Activity)\n(Staff)-[TEACHES_ON]-&gt;(Activity)\n(Activity)-[TAKES_PLACE_IN]-&gt;(Room)",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#mvp-model",
    "href": "02-02-graph-timetable.html#mvp-model",
    "title": "Graph Data Model for Timetabling",
    "section": "MVP model",
    "text": "MVP model\n\n\n\nCore Nodes and Properties\n\n\n\n\n\nNeo4j Interface showing basic nodes and properties",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#footnotes",
    "href": "02-02-graph-timetable.html#footnotes",
    "title": "Graph Data Model for Timetabling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMinimum Viable Product: “First, a definition: the minimum viable product is that version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort or in other words building the most minimum version of their product that will still allow them to learn.” (Ries, 2024)↩︎",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "01-02-background.html",
    "href": "01-02-background.html",
    "title": "Background and Motivation",
    "section": "",
    "text": "Many years ago I grappled with the complexities of timetable generation and optimisation, and battled with trying to balance competing, but conflicting demands like maximising room utilisation and adhering to staff working patterns and producing a ‘decent’ timetable for the students. It is an unwinnable battle.\nThese experiences and challenges left an indelible mark - highlighting the need for robust tools and metrics to understand and assess timetable quality - a factor which is often overshadowed by the pursuit of mere feasibility.\nThis project is the result of a deliberate clash of my professional experiences and data science learning where I aim to deliver a practical solution to a real-world problem.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#personal",
    "href": "01-02-background.html#personal",
    "title": "Background and Motivation",
    "section": "",
    "text": "Many years ago I grappled with the complexities of timetable generation and optimisation, and battled with trying to balance competing, but conflicting demands like maximising room utilisation and adhering to staff working patterns and producing a ‘decent’ timetable for the students. It is an unwinnable battle.\nThese experiences and challenges left an indelible mark - highlighting the need for robust tools and metrics to understand and assess timetable quality - a factor which is often overshadowed by the pursuit of mere feasibility.\nThis project is the result of a deliberate clash of my professional experiences and data science learning where I aim to deliver a practical solution to a real-world problem.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "href": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "title": "Background and Motivation",
    "section": "Research Gap: Bridging Theory and Practice",
    "text": "Research Gap: Bridging Theory and Practice\nMuch current research into university timetabling centres on combinatorial optimisation (Chen et al., 2021), that is using various sophisticated techniques designed to efficiently generate feasible solutions given a set of constraints. This computationally-driven optimisation research is often referred to as the university course timetabling problem (UCTTP) and is categorised as NP-hard1, meaning finding the absolute “best” timetable is exceptionally challenging (Babaei, Karimpour and Hadidi, 2015; Herres and Schmitz, 2021; Wikipedia contributors, 2024).\nConsequently, significant effort has been dedicated to developing algorithms like constraint programming (Holm et al., 2022) and local search techniques such as Tabu Search and simulated annealing (Oude Vrielink et al., 2019), aiming to create workable timetables within reasonable timeframes. While crucial for advancing algorithmic development, these idealised scenarios2 do not fully capture the dynamic complexity of real-world university timetabling.\nUniversities grapple with constantly shifting demands: fluctuating student populations, evolving institutional preferences, resource limitations, and the ever-present need to balance diverse stakeholder needs. These complexities extend beyond simply finding a feasible solution – they necessitate tools to understand the trade-offs inherent in any timetable, enabling informed decisions about which “good” outcomes to prioritise (Lindahl, 2017).",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#the-hypothesisenter-the-graph",
    "href": "01-02-background.html#the-hypothesisenter-the-graph",
    "title": "Background and Motivation",
    "section": "The hypothesis…Enter the Graph",
    "text": "The hypothesis…Enter the Graph\nThis is where I believe graph data structures could offer unique potential.\nTimetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Graph databases excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable.\nWhile algorithms generate optimised solutions, there remains a gap in post-generation analysis – e.g. the ability to delve into a timetable’s nuanced impacts on student and staff experience. Despite the acknowledged importance and impact of factors like room allocation and teaching period distribution, traditional optimisation-focused approaches lack the tools to explore these relationships in depth (Ceschia, Di Gaspero and Schaerf, 2023; Lindahl, 2017; Rudová, Müller and Murray, 2011), particularly in a real-world scenario.\nThis potential for deeper analysis motivates the exploration of graph data structures for enhancing timetable understanding and, ultimately, improving timetable quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#footnotes",
    "href": "01-02-background.html#footnotes",
    "title": "Background and Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“In computational complexity theory, a computational problem H is called NP-hard if, for every problem L which can be solved in non-deterministic polynomial-time, there is a polynomial-time reduction from L to H.” (Wikipedia contributors, 2024)↩︎\nResearch on computational optimisation often makes use of standardised datasets and predefined constraints in order to facilitate comparison, repeatability and evaluation.↩︎",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": " ",
    "section": "",
    "text": "Abdipoor, S., Yaakob, R., Goh, S.L. and Abdullah, S. (2023) Meta-heuristic approaches for the University Course Timetabling Problem. Intelligent Systems with Applications [online]. 19, p. 200253. Available from: https://www.sciencedirect.com/science/article/pii/S2667305323000789 [Accessed 25 July 2024].\nAnon. (no date) CPB Projects [online]. Available from: https://www.cpbprojects.co.uk/solutions/timetabling-and-teaching-space [Accessed 25 July 2024a].\nBabaei, H., Karimpour, J. and Hadidi, A. (2015)‘A survey of approaches for university course timetabling problem’ Computers & Industrial EngineeringApplications of Computational Intelligence and Fuzzy Logic to Manufacturing and Service Systems [online]. 86, pp. 43–59. Available from: https://www.sciencedirect.com/science/article/pii/S0360835214003714 [Accessed 28 July 2024].\nBeck, K., et al. (2001) The Agile Manifesto. Agile Alliance. http://agilemanifesto.org/\nBellio, R., Ceschia, S., Di Gaspero, L., Schaerf, A. and Urli, T. (2016) Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem. Computers & Operations Research [online]. 65, pp. 83–92. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0305054815001690 [Accessed 26 January 2024].\nBonutti, A., De Cesco, F., Di Gaspero, L. and Schaerf, A. (2012) Benchmarking curriculum-based course timetabling: formulations, data formats, instances, validation, visualization, and results. Annals of Operations Research [online]. 194 (1), pp. 59–70. Available from: https://doi.org/10.1007/s10479-010-0707-0 [Accessed 3 February 2024].\nBruggen, R. van (2014)’Learning Neo4j: run blazingly fast queries on complex graph datasets with the power of the Neo4j graph database’Community Experience Distilled. 1st edition. Birmingham, England: Packt Publishing.\nBurke, E., Mccollum, B., Meisels, A., Petrovic, S. and Qu, R. (2007) A graph-based hyper-heuristic for educational timetabling problems. European Journal of Operational Research [online]. 176, pp. 177–192.\nCeschia, S., Di Gaspero, L. and Schaerf, A. (2023) Educational timetabling: Problems, benchmarks, and state-of-the-art results. European Journal of Operational Research [online]. 308 (1), pp. 1–18. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0377221722005641 [Accessed 25 January 2024].\nChen, M., Sze, S., Goh, S.L., Sabar, N. and Kendall, G. (2021) A Survey of University Course Timetabling Problem: Perspectives, Trends and Opportunities. IEEE Access [online]. PP, pp. 1–1.\nChicken, S., Fogg Rogers, L., Hobbs, L., Hunt-Fraisse, T. and Lewis, D. (2023) Amplifying the voices of neurodivergent students in relation to higher education assessment at UWE Bristol. [online]. Available from: https://uwe-repository.worktribe.com/output/10879555 [Accessed 25 July 2024].\nDammak, A., Elloumi, A. and Kamoun, H. (2007) An enterprise system component based on graph colouring for exam timetabling: A case study in a Tunisian university. Transforming Government: People, Process and Policy [online]. 1 (3), pp. 255–270. Available from: https://www.emerald.com/insight/content/doi/10.1108/17506160710778095/full/html [Accessed 19 February 2024].\nde Werra, D. (1997) The combinatorics of timetabling. European Journal of Operational Research [online]. 96 (3), pp. 504–513.\nDon State Technical University, Rostov-on-Don, Russian Federation and Al-Gabri, W.M. (2017) Literature review for the topic of automation of scheduling classes and exams in higher education institutions. Vestnik of Don State Technical University [online]. 17 (1), pp. 132–143. Available from: https://vestnik.donstu.ru/jour/article/view/255 [Accessed 25 January 2024].\nDowland, D. (2018) Rubik’s cube or Battenburg? The university timetable Wonkhe. 11 January 2018 [online]. Available from: https://wonkhe.com/blogs/rubiks-cube-or-battenburg-the-university-timetable/ [Accessed 25 July 2024].\nFoung, D. and Chen, J. (2019) Discovering disciplinary differences: blending data sources to explore the student online behaviors in a University English course. Information Discovery and Delivery [online]. 47 (2), pp. 106–114. Available from: https://www.emerald.com/insight/content/doi/10.1108/IDD-10-2018-0053/full/html [Accessed 19 February 2024].\nhelenclu (2024) Database normalization description - Microsoft 365 Apps. 6 June 2024 [online]. Available from: https://learn.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description [Accessed 11 August 2024].\nHerres, B. and Schmitz, H. (2021) Decomposition of university course timetabling: A systematic study of subproblems and their complexities. Annals of Operations Research [online]. 302 (2), pp. 405–423. Available from: https://ezproxy.uwe.ac.uk/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&AN=150973572&site=ehost-live [Accessed 28 July 2024].\nHolm, D.S., Mikkelsen, R.Ø., Sørensen, M. and Stidsen, T.J.R. (2022) A graph-based MIP formulation of the International Timetabling Competition 2019. Journal of Scheduling [online]. 25 (4), pp. 405–428. Available from: https://doi.org/10.1007/s10951-022-00724-y [Accessed 2 November 2023].\nJohnson, D. (1993) A Database Approach to Course Timetabling. The Journal of the Operational Research Society [online]. 44 (5), pp. 425–433. Available from: https://www.jstor.org.ezproxy.uwe.ac.uk/stable/2583909 [Accessed 28 July 2024].\nKhan, W., Kumar, T., Zhang, C., Raj, K., Roy, A.M. and Luo, B. (2023) SQL and NoSQL Database Software Architecture Performance Analysis and Assessments—A Systematic Literature Review. Big Data and Cognitive Computing [online]. 7 (2), p. 97. Available from: https://www.mdpi.com/2504-2289/7/2/97 [Accessed 28 July 2024].\nLemos, A., Melo, F.S., Monteiro, P.T. and Lynce, I. (2019) Room usage optimization in timetabling: A case study at Universidade de Lisboa. Operations Research Perspectives [online]. 6, p. 100092. Available from: https://linkinghub.elsevier.com/retrieve/pii/S2214716018301696 [Accessed 26 January 2024].\nLindahl, M., Mason, A.J., Stidsen, T. and Sørensen, M. (2018) A strategic view of University timetabling. European Journal of Operational Research [online]. 266 (1), pp. 35–45. Available from: https://www.sciencedirect.com/science/article/pii/S0377221717308433 [Accessed 28 July 2024].\nMirHassani, S.A. and Habibi, F. (2013) Solution approaches to the course timetabling problem. Artificial Intelligence Review [online]. 39 (2), pp. 133–149. Available from: http://link.springer.com/10.1007/s10462-011-9262-6 [Accessed 26 January 2024].\nMühlenthaler, M. and Wanka, R. (2016) Fairness in academic course timetabling. Annals of Operations Research [online]. 239 (1), pp. 171–188. Available from: https://doi.org/10.1007/s10479-014-1553-2 [Accessed 3 February 2024].\nMüller, T. and Murray, K. (2010) Comprehensive approach to student sectioning. Annals of Operations Research [online]. 181 (1), pp. 249–269. Available from: http://link.springer.com/10.1007/s10479-010-0735-9 [Accessed 26 January 2024].\nNan 1, Z., Bai, X. 1 1 C. of I. and Economics, T.Y. (2019) The study on data migration from relational database to graph database. [online]. Available from: https://www.proquest.com/docview/2568058349?pq-origsite=primo [Accessed 4 November 2023].\nNegro, A. (2021) Graph-Powered Machine Learning [online]. O’Reilly Media, Inc. [Accessed 4 November 2023].\nNeo4j (2023) The Neo4j Cypher Manual v5.\nNguyen, V.D. and Nguyen, T. (2021) An SHO-based approach to timetable scheduling: a case study. Journal of Information and Telecommunication [online]. 5 (4), pp. 421–439. Available from: https://doi.org/10.1080/24751839.2021.1935644 [Accessed 2 November 2023].\nNorman, R. and Williams, E. (no date) PSP Board Pack 220804 v1.3.pptx [online]. Available from: https://uweacuk-my.sharepoint.com/:p:/g/personal/richard2_norman_uwe_ac_uk/EWKNTqInQuRDoSaPmHCOp20B72Dp_2vYpJ__GbCzaY5tiA?email=Petter.Lovehagen%40uwe.ac.uk&e=4%3AwEUjpu&fromShare=true&at=31&CID=256ab09c-758c-97f4-07dd-ff61808257cf [Accessed 19 February 2024].\nOude Vrielink, R.A., Jansen, E.A., Hans, E.W. and Van Hillegersberg, J. (2019) Practices in timetabling in higher education institutions: a systematic review. Annals of Operations Research [online]. 275 (1), pp. 145–160. Available from: http://link.springer.com/10.1007/s10479-017-2688-8 [Accessed 2 November 2023].\nRies, E. (2024) What Is an MVP? Eric Ries Explains Lean Startup Co. 28 February 2024 [online]. Available from: https://leanstartup.co/resources/articles/what-is-an-mvp/ [Accessed 11 August 2024].\nRudová, H., Müller, T. and Murray, K. (2011) Complex university course timetabling. Journal of Scheduling [online]. 14 (2), pp. 187–207. Available from: http://link.springer.com/10.1007/s10951-010-0171-3 [Accessed 26 January 2024].\nSanchez, C.A. (2015) An analytics based architecture and methodology for collaborative timetabling in higher education - ProQuest [online]. Available from: https://www.proquest.com/docview/1779550151?pq-origsite=primo&parentSessionId=GLad2hOIbVrF%2F0eiOKHGi%2BO%2BFOyV9GXuQTQCxSfgWNw%3D&sourcetype=Dissertations%20&%20Theses [Accessed 25 January 2024].\nScifo, E. (2023) Graph Data Science with Neo4j [online]. [Accessed 4 November 2023].\nSokolova, Marina V., Francisco J. Gómez, and Larisa N. Borisoglebskaya. ‘Migration from an SQL to a Hybrid SQL/NoSQL Data Model’. Journal of Management Analytics 7, no. 1 (March 2020): 1–11. https://doi.org/10.1080/23270012.2019.1700401.\nWebber, J., Eifrem, E. and Robinson, I. (2013) Graph Databases [online]. [Accessed 4 November 2023].\nWikipedia contributors (2024) Cypher (query language) — Wikipedia, the free encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=Cypher_(query_language)&oldid=1219736900.\nWikipedia contributors (2024) NP-hardness — Wikipedia, the free encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=NP-hardness&oldid=1236371945.\nWikipedia contributors (2024) SQL Wikipedia, The Free Encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=SQL&oldid=1238737606 [Accessed 11 August 2024].",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "01-01a-introduction.html",
    "href": "01-01a-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "University timetabling is the process of scheduling resources within the constraints of an academic institution and calendar. At its core, it involves collecting and combining time slots, rooms, students, and other resources while satisfying a multitude of constraints and preferences to achieve a viable outcome.\nHowever, the reality of timetabling is far more complicated than this simple definition suggests.\nTimetablers must juggle numerous hard constraints (e.g., room capacities, pre-assigned times) and soft constraints (e.g., staff preferences, student travel times) to reach a workable solution. The scale of this task, combined with interdependencies between scheduling decisions, makes university timetabling one of the most challenging administrative tasks in higher education (de Werra, 1997).\nTimetables can make or break a university - they shape the daily experiences of students and staff, influence resource utilisation, and play a significant role in institutional efficiency. The complexity of timetabling stems from various factors:\n\nScale: Tens of thousands of students and activities, and limited resources create a logistical nightmare.\nConstraints: Juggling hard limits (room capacities) and soft preferences (College desires) is a constant balancing act.\nInterdependencies: Changes in one part of the schedule can have cascading effects throughout the entire timetable.\nDiversity of Needs: Different organisational units (colleges, faculties, schools, departments) have varying requirements and preferences.\nOptimisation Goals: Timetablers must balance efficiency, fairness, and quality of education.\n\nWhile traditional studies on “timetabling” focus heavily on generating or optimising feasible timetables (Bonutti et al., 2012; Ceschia, Di Gaspero and Schaerf, 2023; Rudová, Müller and Murray, 2011) – ensuring no clashes or rule violations – this project explores a different facet: how analysing scheduled timetables can lead to deeper insights and ultimately, improved quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html",
    "href": "01-03-good-timetable.html",
    "title": "What is a “good” timetable?",
    "section": "",
    "text": "One of the most challenging aspects of university timetabling is defining what constitutes a “good” timetable. Despite best efforts, it is virtually impossible to deliver universal satisfaction from a university timetable. The quality of a timetable is inherently subjective and varies among stakeholders depending on their preferences and the demands on their time.\nBased on surveys across various institutions, students typically prioritise (Dowland, 2018; Norman, 2022):\nThe above are easy-to-measure deliverables but they do not address what a ‘good’ timetable should look like; individual stakeholders often have conflicting priorities:\nThis divergence in preferences and the complex interplay of constraints make it challenging to define and achieve a universally “good” timetable (Lindahl et al., 2018). It is this complexity that sets the stage for this project.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html#consider-these-timetables",
    "href": "01-03-good-timetable.html#consider-these-timetables",
    "title": "What is a “good” timetable?",
    "section": "Consider these timetables:",
    "text": "Consider these timetables:\n\nThe first timetable is evenly spread over five days.\nThe second timetable has two days free of activities.\nThe third timetable has activities on five days, with gaps.\n\nWhich timetable is better? Is any of them ‘good’? The answer is it depends! or none of them!\n(Click to enlarge)\n\n\n\n\n\n\n\nEvenly spread over 5 days.  Tuesday afternoons are heavily scheduled; activities take place over lunch\n\n\nTwo days free of activity (Wednesday and Thursday).  Monday has a single activity at 18:00-19:00\n\n\nActivities on five days.  There are large gaps between activities on Tuesday and Wednesday.",
    "crumbs": [
      "Home",
      "Project Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "02-02b-early.html",
    "href": "02-02b-early.html",
    "title": "Early Insights",
    "section": "",
    "text": "Even with this basic model, we can easily extract valuable insights, for example:\n\nActivity Load: Identify staff with the highest number of teaching activities or total teaching hours.\nStudent Timetable Profiles: Calculate average hours per student or per programme to understand workload distribution.\nResource utilisation: Determine the busiest teaching locations or times on campus.\nAnomaly detection: Identify students who have unexpected profiles or unusual combinations.\n\n\n\nBusiest locations overall\n{cypher}{.scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WITH r, sum(a.actDuration)/60 AS totalDurationInHours RETURN r.roomName AS Room, r.roomCapacity AS Capacity, r.roomType AS Type, totalDurationInHours ORDER BY totalDurationInHours DESC LIMIT 3\n\n\n\nRoom\nCapacity\nType\ntotalDurationInHours\n\n\n\n\n2Q12 FR\n25\nPC LAB\n21\n\n\n4Q69 FR\n36\nPC LAB\n19\n\n\n3E11 FR\n48\nTEACHING\n18\n\n\n\nBusiest location for a specific time\n{cypher}{.scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WHERE a.actStartTime = localtime({hour:9, minute:0, second:0})  WITH r, count(a) AS Count, a.actStartTime AS StartTime RETURN r.roomName AS Room, Count, StartTime ORDER BY Count DESC LIMIT 3\n\n\n\nRoom\nCount\nStartTime\n\n\n\n\n2Q12 FR\n86\n09:00:00\n\n\n3E28 FR\n50\n09:00:00\n\n\n3E11 FR\n49\n09:00:00\n\n\n\nStudents with below/above average hours\nThis query returns students and whether they have more or less scheduled time on their timetable compared to the programme cohort average. 1\n{cypher}{.scroll-cypher} MATCH (s:student)-[:ATTENDS]-&gt;(a:activity) WITH s.stuProgName AS progName, s.stuID_anon AS studentID, SUM(a.actDurationInMinutes) AS studentTotalDuration WITH progName,       AVG(studentTotalDuration) / 60 AS progAverageHoursPerStudent,       collect({studentID: studentID, studentTotalHours: studentTotalDuration / 60}) AS studentsData UNWIND studentsData AS studentData RETURN progName,         progAverageHoursPerStudent,        studentData.studentID AS studentID,         studentData.studentTotalHours AS studentTotalHours,         CASE             WHEN studentData.studentTotalHours &lt; (progAverageHoursPerStudent * 0.9) THEN 'Below Average'            WHEN studentData.studentTotalHours &gt; (progAverageHoursPerStudent * 1.1) THEN 'Above Average'            ELSE 'Average'        END AS compare\n\nUsing Percentage\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n“Maths NS”\n274.7692307692308\n“stu-23442558”\n361\n“Above Average”\n\n\n“Maths NS”\n274.7692307692308\n“stu-91911371”\n126\n“Below Average”\n\n\n“Maths NS”\n274.7692307692308\n“stu-75224499”\n251\n“Average”\n\n\n\nWhen using standard deviation (1SD) the same three students have a different outcome. This is primarily due to the exceptionally large standard deviation. This is because students on this version of the programme could be trailing modules and either attending significantly more or less activity, making the range very large.\n\nUsing Standard Deviation\n\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nprogStDevHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-23442558”\n361\n“Average”\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-91911371”\n126\n“Below Average”\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-42997469”\n251\n“Below Average”",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights"
    ]
  },
  {
    "objectID": "02-02b-early.html#unveiling-basic-patterns",
    "href": "02-02b-early.html#unveiling-basic-patterns",
    "title": "Early Insights",
    "section": "",
    "text": "Even with this basic model, we can easily extract valuable insights, for example:\n\nActivity Load: Identify staff with the highest number of teaching activities or total teaching hours.\nStudent Timetable Profiles: Calculate average hours per student or per programme to understand workload distribution.\nResource utilisation: Determine the busiest teaching locations or times on campus.\nAnomaly detection: Identify students who have unexpected profiles or unusual combinations.\n\n\n\nBusiest locations overall\n{cypher}{.scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WITH r, sum(a.actDuration)/60 AS totalDurationInHours RETURN r.roomName AS Room, r.roomCapacity AS Capacity, r.roomType AS Type, totalDurationInHours ORDER BY totalDurationInHours DESC LIMIT 3\n\n\n\nRoom\nCapacity\nType\ntotalDurationInHours\n\n\n\n\n2Q12 FR\n25\nPC LAB\n21\n\n\n4Q69 FR\n36\nPC LAB\n19\n\n\n3E11 FR\n48\nTEACHING\n18\n\n\n\nBusiest location for a specific time\n{cypher}{.scroll-cypher} MATCH (r:room)&lt;-[:OCCUPIES]-(a:activity) WHERE a.actStartTime = localtime({hour:9, minute:0, second:0})  WITH r, count(a) AS Count, a.actStartTime AS StartTime RETURN r.roomName AS Room, Count, StartTime ORDER BY Count DESC LIMIT 3\n\n\n\nRoom\nCount\nStartTime\n\n\n\n\n2Q12 FR\n86\n09:00:00\n\n\n3E28 FR\n50\n09:00:00\n\n\n3E11 FR\n49\n09:00:00\n\n\n\nStudents with below/above average hours\nThis query returns students and whether they have more or less scheduled time on their timetable compared to the programme cohort average. 1\n{cypher}{.scroll-cypher} MATCH (s:student)-[:ATTENDS]-&gt;(a:activity) WITH s.stuProgName AS progName, s.stuID_anon AS studentID, SUM(a.actDurationInMinutes) AS studentTotalDuration WITH progName,       AVG(studentTotalDuration) / 60 AS progAverageHoursPerStudent,       collect({studentID: studentID, studentTotalHours: studentTotalDuration / 60}) AS studentsData UNWIND studentsData AS studentData RETURN progName,         progAverageHoursPerStudent,        studentData.studentID AS studentID,         studentData.studentTotalHours AS studentTotalHours,         CASE             WHEN studentData.studentTotalHours &lt; (progAverageHoursPerStudent * 0.9) THEN 'Below Average'            WHEN studentData.studentTotalHours &gt; (progAverageHoursPerStudent * 1.1) THEN 'Above Average'            ELSE 'Average'        END AS compare\n\nUsing Percentage\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n“Maths NS”\n274.7692307692308\n“stu-23442558”\n361\n“Above Average”\n\n\n“Maths NS”\n274.7692307692308\n“stu-91911371”\n126\n“Below Average”\n\n\n“Maths NS”\n274.7692307692308\n“stu-75224499”\n251\n“Average”\n\n\n\nWhen using standard deviation (1SD) the same three students have a different outcome. This is primarily due to the exceptionally large standard deviation. This is because students on this version of the programme could be trailing modules and either attending significantly more or less activity, making the range very large.\n\nUsing Standard Deviation\n\n\n\n\n\n\n\n\n\n\nprogName\nprogAverageHoursPerStudent\nprogStDevHoursPerStudent\nstudentID\nstudentTotalHours\ncompare\n\n\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-23442558”\n361\n“Average”\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-91911371”\n126\n“Below Average”\n\n\n“Maths NS”\n274.7692307692308\n118.81231781219205\n“stu-42997469”\n251\n“Below Average”",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights"
    ]
  },
  {
    "objectID": "02-02b-early.html#footnotes",
    "href": "02-02b-early.html#footnotes",
    "title": "Early Insights",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe average is calculated based on the total duration of activities attended by each student. The below/above average classification is based on a 10% deviation from the average. Alternative approaches have been used to define the average and the deviation threshold including median values and standard deviations.↩︎",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights"
    ]
  },
  {
    "objectID": "02-03-graph-time.html",
    "href": "02-03-graph-time.html",
    "title": "Graphing Time",
    "section": "",
    "text": "The biggest challenge I encountered when translating timetables into graph data involved temporal elements - that is, start and end times, dates, weeks, recurrences, durations, etc. While the basic model successfully captured the core entities and relationships, it lacked the necessary detail to perform meaningful time-based analysis.\nThe flexibility of graph databases is appealing but finding the optimal balance between efficient representation, query performance, and data redundancy requires careful consideration. This section details some challenges encountered and the approach taken for the proof-of-concept.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#the-problem-of-normalised-time",
    "href": "02-03-graph-time.html#the-problem-of-normalised-time",
    "title": "Graphing Time",
    "section": "The Problem of Normalised Time",
    "text": "The Problem of Normalised Time\nTraditional relational databases often store timetable information in a highly normalised1 format, condensing recurring events into single rows with date ranges, week patterns, or lists of occurrences. While efficient for storage and basic display, this approach severely hinders analysis, especially when aiming to:\n\nIdentify Time-Based Patterns: Determining if students lack lunch breaks or experience excessive gaps between classes becomes difficult when time is fragmented across multiple fields.\nPerform Aggregations: Calculating total teaching hours for a lecturer across specific weeks or days requires complex queries and data transformations.\nModel Temporal Relationships: Representing relationships between activities based on their temporal proximity, such as students attending consecutive classes, becomes convoluted.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#exploring-potential-solutions",
    "href": "02-03-graph-time.html#exploring-potential-solutions",
    "title": "Graphing Time",
    "section": "Exploring Potential Solutions",
    "text": "Exploring Potential Solutions\nSeveral time modelling approaches were considered, each with its own trade-offs.\nTo illustrate this, let’s explore using a fictional example - Introduction to Graph Databases - focusing on the lecture:\n\nExample Source Data (Relational)\n\n\n\n\n\n\n\n\n\n\nName\nActivityType\nDay\nStartTime\nEndTime\nWeeks\n\n\n\n\nITGD\nLecture\nWednesday\n09:00\n11:00\n1-3, 5-7, 9-13\n\n\nITGD\nSeminar\nWednesday\n10:00\n13:00\n4, 7-8, 15\n\n\nITGD\nSeminar\nMonday\n13:00\n16:00\n4, 7-8, 15\n\n\n\n\nOption 0: Proof-of-concept activity\nThe basic model creates nodes for each activity exactly as they exist in the relational database. This simple approach is perfectly acceptable but makes any time based calculations difficult because each activity node can represent a different number of occurrences due to the week ranges.\nThis in turn means you cannot simply COUNT each activity “equally” - for example, the lecture has 11 instances, each of two hours. The seminars have four instances, each of three hours. Calculating aggregations, finding clashes and similar is very challenging.\n\n\n\nBasic example of Graphing Normalised Activities\n\n\nIf we assume that each student attends the lecture and one of the seminars, some students have a clash in week 7 (Wednesday 10:00-11:00) - this is very difficult to identify and isolate in a highly normalised dataset.\n\n\nOption 1: Unique Activity Nodes\nOption 1 addresses this by creating nodes for each unique combination of name, startTime, endTime and date - this means de-normalising the relational data and deliberately introducing duplication.\n\n\n\nUnique Activity Nodes Graph\n\n\nGraph Structure:\n\n11 separate Activity nodes one for each occurrence (date)\nEach node has date, startTime, endTime properties\nOnly date is different between each node.\n\n// cypher structure     \n(Activity {Name: \"ITGD\", Date: \"2024-01-03\", StartTime: \"09:00\", EndTime: \"11:00\"}) \n(Activity {Name: \"ITGD\", Date: \"2024-01-10\", StartTime: \"09:00\", EndTime: \"11:00\"}) \n... \n(Activity {Name: \"ITGD\", Date: \"2024-03-20\", StartTime: \"09:00\", EndTime: \"11:00\"})\nPros:\n\nConceptual Simplicity: Easy to understand and implement.\nDirect Time Representation: Time is directly associated with each activity instance.\n\nCons:\n\nNode Proliferation: Leads to a high volume of nodes, potentially impacting performance with large datasets.\n\nUse Case dependent\n\nTime-Based Queries: Answering questions about time patterns or conflicts requires traversing numerous nodes and relationships. Some queries will benefit - e.g. identifying clashes which may only occur in a specific week, others will become more complex as de-normalised data needs to be re-aggregated.\n\n\n\nOption 2: Date and Time Nodes\nOption 2 creates a single activity node but also additional date and time nodes, as required, thus not proliferating activities.\n\n\n\nTime and Date Nodes\n\n\nGraph Structure:\n\n1 Activity node\n11 Date nodes - shared by ALL activities on those dates.\n2 Time nodes (09:00 and 11:00) - shared by ALL activities on those times!\nAdditional Relationships\n\nActivity -[:SCHEDULED_ON]-&gt; Date (11 relationships)\nActivity -[:STARTS_AT]-&gt; Time (11 relationships to 09:00)\nActivity -[:ENDS_AT]-&gt; Time (11 relationships to 11:00)\n\n\n\n// cypher structure         \n(Activity {Name: \"ITGD\"})\n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"})\n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-10\"})\n    ...\n    -[:STARTS_AT]-&gt; (Time {time: \"09:00\"}) \n    -[:ENDS_AT]-&gt; (Time {time: \"11:00\"})\nKey point: Relationships encode which activity happens when.\nPros:\n\nIncreased Flexibility: Facilitates queries across time ranges and aggregations across time slots.\nReduced Redundancy: Avoids replicating time information for activities occurring on the same date and time.\nLower Node Count: Potentially fewer nodes overall compared to Option 1 as date and time nodes are shared with all activities in the database.\n\nCons:\n\nIncreased Model Complexity: Requires managing relationships between Activity, Date, and Time nodes.\nPotential Performance Overhead: Querying might involve traversing multiple relationships, impacting efficiency.\n\n\n\nOption 3: Date and Time Block Nodes\nOption 3 creates a single activity but instead of individual start and end time nodes, we use predetermined timeBlocks encompassing both. For example, if using 30-minute blocks, we would have a node for “09:00-09:30” and another for “09:30-10:00”, etc.\n\n\n\nTimeBlock and Date Nodes\n\n\nGraph Structure:\n\n1 Activity node\n11 Date nodes\n4 Timeblock nodes (09:00-09:30, etc.) - shared by ALL activities on those times!\nAdditional Relationships\n\nActivity -[:SCHEDULED_ON]-&gt; Date (11 relationships)\nActivity -[:TAKES_PLACE_DURING]-&gt; timeBlock 09:00-09:30 (11 relationships)\nActivity -[:TAKES_PLACE_DURING]-&gt; timeBlock 09:30-10:00 (11 relationships)\n…\n\n\n// cypher structure\n(Activity {Name: \"ITGD\"}) \n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"}) \n    -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-10\"})\n    ...\n    -[:TAKES_PLACE_DURING]-&gt; (TimeBlock {timeBlock: \"09:00-09:30\"}) \n    -[:TAKES_PLACE_DURING]-&gt; (TimeBlock {timeBlock: \"09:30-10:00\"})\n    -[:TAKES_PLACE_DURING]-&gt; (Timeblock {timeBlock: \"10:00-10:30\"}) \n    -[:TAKES_PLACE_DURING]-&gt; (Timeblock {timeBlock: \"10:30-11:00\"})\nPros:\n\nGranular Time Representation: Enables analysis at specific time intervals\nEasier Time Calculations: Duration is encoded and allows for easy calculations.\n\nCons:\n\nPotential for Data Sparsity: Some time blocks might be sparsely populated, leading to storage inefficiencies.\nPotential for High Node Codes: Lots of TimeBlocks if using small intervals\nLess flexible: Timeblocks are not dynamic.\n\n\n\nVariations\nStartTime and Duration: This option simplifies the model by representing time using only StartTime and DurationInMinutes properties on the Activity node, omitting explicit EndTime nodes. This approach is suitable for duration based queries but it is limiting in that it is more difficult to query events occurring at specific times, overlapping time ranges or on end-times.\n// cypher structure\n(Timeblock {name: \"09:00-11:00\", start: 09:00, end: 11:00, duration:120}) (Timeblock {name: \"10:30-11:30\", start: 10:30, end: 11:30, duration:60}) (Timeblock {name: \"11:00-12:00\", start: 11:00, end: 12:00, duration:60})\nTime Chains: This option retains date and time nodes, but instead of having relationships from activity, the nodes are chained: activity -&gt; startTime -&gt; endTime.\nTime as Relationship Property: This option stores time information as properties on the relationship between Activity and Date nodes. This approach is more compact but can be less intuitive and may limit the ability to query based on time.\nDynamic TimeBlocks: This variation does not pre-create timeblocks based on a set interval (e.g. 30 minutes). They are created dynamically as required by the data and what already exists. For example, activities at 09:00-11:00, 10:30-11:30 and 11:00-12:00 would require these TimeBlocks:\n\n\nSummary\n\nOption summary\n\n\n\n\n\n\n\n\n#\nOption\nPros\nCons\n\n\n\n\n0\nDirect transfer (Normalised)\nSimple\nMinimal benefits (for time calculations)\n\n\n1\nUnique Occurrences\nSimple, direct\nHigh node count, complex time pattern queries\n\n\n2\nDate & Time Nodes\nLower activity node count, good for time-based queries\nMore complex relationships\n\n\n3\nDate & TimeBlock Nodes\nGranular, easier duration calculations\nPotentially high node count, sparsity if blocks are fine-grained\n\n\n\n\nGiven the proof-of-concept scope of this project, I chose to proceed with Option 1. While this approach can lead to node proliferation, it offers the most straightforward implementation for exploring fundamental time-based queries and insights. It also acts as an easy jumping off point for exploring any of the other options.2",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#footnotes",
    "href": "02-03-graph-time.html#footnotes",
    "title": "Graphing Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Normalization is the process of organizing data in a database. It includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating redundancy and inconsistent dependency.” (helenclu, 2024)↩︎\nSome of the above variations are explored in Appendix-Student Clashes↩︎",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "03-03-config.html",
    "href": "03-03-config.html",
    "title": "Configuration and Logging",
    "section": "",
    "text": "Configuration and logging are essential components of the ETL pipeline. Config allows the user to manage different aspects of the ETL pipeline, while logging provides a record of the pipeline’s execution. They emerged from initial design and from discovering during development.\n\nMain Configuration options\n\nConfiguration parameters are centralised in Python scripts.\nThe design primarily aims for automatic and dynamic operation with well-structured data, but includes override options.\nA YAML file (Appendix-config) holds configuration options, including general settings for filtering data extraction, dynamic folder/filepath creation, and secure credential storage.\nConfig also controls options for validation, data augmentation and graph structures (nodes, relationships) to be created.\n\n\n\nLogging\n\nEach module has its own log file with customisable log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\nTiming function which tracks and stores various execution and elapsed times with a view to optimising performance or identifying bottlenecks.\n\n\nExample Extract Log\nSetting up logger: extract\nLogger extract setup completed with 1 handlers.\nSetting up logger: process\nLogger process setup completed with 1 handlers.\nSetting up logger: load\nLogger load setup completed with 1 handlers.\n2024-07-04 11:26:01,124 - INFO - extract_main - Starting data extraction process\n2024-07-04 11:26:01,124 - INFO - extract_main - Output Directory: C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract\n2024-07-04 11:26:01,124 - INFO - extract_main - Hostkeys: ['INB112']\n2024-07-04 11:26:01,124 - INFO - extract_main - Chunksize: 20000\n2024-07-04 11:26:01,124 - INFO - extract_data - Starting data extraction process...\n2024-07-04 11:26:01,284 - INFO - Connected to SQL Server database.\n2024-07-04 11:26:01,284 - INFO - temp_table_loader - Creating global temporary tables...\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Processing SQL file: node-activity-by-pos-temp.sql\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Extracting data for node-activity-by-pos-temp with hostkey: INB112\nC:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\extract_sql_file.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  for chunk in pd.read_sql_query(query, conn, chunksize=CHUNK_SIZE):\n2024-07-04 11:26:07,960 - INFO - extract_sql_file - Saved 631 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activity-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Query for node-activity-by-pos-temp, INB112 took 0.09 seconds\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Processing SQL file: node-activityType-by-pos-temp.sql\n2024-07-04 11:26:07,968 - INFO - extract_sql_file - Extracting data for node-activityType-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Saved 16 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activityType-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Query for node-activityType-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Processing SQL file: node-dept-all.sql\n2024-07-04 11:26:07,987 - INFO - extract_sql_file - Extracting data for node-dept-all with hostkey: INB112\n2024-07-04 11:26:07,990 - INFO - extract_sql_file - Saved 24 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-dept-all_INB112_1.csv\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Query for node-dept-all, INB112 took 0.01 seconds\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Processing SQL file: node-module-by-pos-temp.sql\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Extracting data for node-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Saved 42 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Query for node-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Processing SQL file: node-pos-by-pos-temp.sql\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Extracting data for node-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Saved 8 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Query for node-pos-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Processing SQL file: node-room-by-pos-temp.sql\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Extracting data for node-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Saved 44 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Query for node-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Processing SQL file: node-staff-by-pos-temp.sql\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Extracting data for node-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Saved 33 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Query for node-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Processing SQL file: node-student-by-pos-temp.sql\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Extracting data for node-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Saved 206 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Query for node-student-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Processing SQL file: rel-activity-module-by-pos-temp.sql\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Extracting data for rel-activity-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Saved 168 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Query for rel-activity-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Processing SQL file: rel-activity-room-by-pos-temp.sql\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Extracting data for rel-activity-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Saved 611 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Query for rel-activity-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Processing SQL file: rel-activity-staff-by-pos-temp.sql\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Extracting data for rel-activity-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Saved 868 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Query for rel-activity-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Processing SQL file: rel-activity-student-by-pos-temp.sql\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Extracting data for rel-activity-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Saved 13423 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Query for rel-activity-student-by-pos-temp, INB112 took 0.12 seconds\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Processing SQL file: rel-mod-pos-by-pos-temp.sql\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Extracting data for rel-mod-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Saved 82 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-mod-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Query for rel-mod-pos-by-pos-temp, INB112 took 0.00 seconds\n2024-07-04 11:26:08,265 - INFO - extract_data - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Extraction Time Summary:\n2024-07-04 11:26:08,285 - INFO - extract_main - Function load_temp_tables took 6.59 seconds\n2024-07-04 11:26:08,285 - INFO - extract_main - Function main took 7.17 seconds\n\n\n\nExample Google Drive Log\n2024-07-11 13:28:22,339 - INFO - gdrive_upload - Starting Google Drive upload process.\n2024-07-11 13:28:22,789 - INFO - gdrive_upload - Found existing folder: INB112\n2024-07-11 13:28:23,115 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:23,366 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:23,576 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:24,810 - INFO - gdrive_upload - File node-activity-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:25,046 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:26,170 - INFO - gdrive_upload - File node-activityType-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:26,394 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:27,495 - INFO - gdrive_upload - File node-department-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:27,715 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:28,970 - INFO - gdrive_upload - File node-module-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:29,206 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:30,294 - INFO - gdrive_upload - File node-programme-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:30,515 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:31,595 - INFO - gdrive_upload - File node-room-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:31,846 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:32,895 - INFO - gdrive_upload - File node-staff-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:33,128 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:34,220 - INFO - gdrive_upload - File node-student-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:34,220 - WARNING - gdrive_upload - Skipping file with unrecognized prefix: process.log\n2024-07-11 13:28:34,416 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:35,565 - INFO - gdrive_upload - File rel-activity_activityType-processed.csv uploaded to Google Drive folder ID: 1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\n\n\n\nExample Process Log\n2024-07-11 13:31:24,284 - INFO - process_utils - Processing relationship data\n2024-07-11 13:31:24,284 - INFO - process_utils - Config: {'filename_pattern': 'rel-mod-pos-by-pos-temp_INB112*.csv', 'node1_col': 'modSplusID', 'node2_col': 'posSplusID', 'relationship': 'BELONGS_TO', 'properties': ['modType']}\n2024-07-11 13:31:24,285 - INFO - process_utils - Saving processed relationship file for: module_programme\n2024-07-11 13:31:24,289 - INFO - process_main - Function process_department took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_module took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_room took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_programme took 0.01 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activityType took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_staff took 1.82 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_student took 8.76 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activity took 0.10 seconds\n\n\n\nExample Load Log\n2024-07-11 16:28:45,220 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:28:45,220 - INFO - load_main - Total execution time: 0.51 seconds\n2024-07-11 16:33:31,166 - INFO - connect_to_neo4j_db - Connected to Neo4j database successfully! Driver: \n2024-07-11 16:33:31,601 - INFO - load_relationships - Found 1 relationship files in Google Drive.\n2024-07-11 16:33:31,604 - INFO - load_relationships - Processing file: rel-activity_room-processed.csv (ID: 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v)\n2024-07-11 16:33:31,605 - INFO - google_drive_utils - Downloading CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v from Google Drive.\n2024-07-11 16:33:35,665 - INFO - google_drive_utils - Downloaded CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v.\n2024-07-11 16:33:45,428 - INFO - load_relationships - Finished loading activity_room relationships:\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Total rows processed: 611\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Relationships created: 1\n2024-07-11 16:33:45,431 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:33:45,432 - INFO - load_main - Total execution time: 14.34 seconds",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Configuration and Logging"
    ]
  },
  {
    "objectID": "04-01-timetable-metrics.html",
    "href": "04-01-timetable-metrics.html",
    "title": "Timetable Metrics",
    "section": "",
    "text": "So far, we have explored the complexities and challenges of university timetabling, the pros and cons of graph databases and investigated different graph data models. We have also built and implemented a data engineering solution which extracts, transforms and loads data from SQL RDMS to a graph database.\nIn this section, we delve deeper into the concept of timetable quality metrics and explore how graph databases can help us quantify and measure the quality of timetables.\n\nDefining Timetable Quality\nAs discussed in the introduction, the inherent complexity of timetabling, with its competing objectives and subjective evaluations, makes it difficult to objectively assess the quality of a timetable. There is no universally agreed-upon definition of a “good” timetable, as it is often a balancing act between satisfying hard constraints (e.g., avoiding clashes) and optimising for softer constraints (e.g., minimising travel time).\nHow can we move beyond anecdotal evidence and subjective opinions to a more data-driven understanding of timetable quality?\nI propose the development of a timetable quality index.\n\n\nTowards a Quantifiable Measure\nA Timetable Quality Index (TQI)is a quantifiable and measurable score reflecting the overall “goodness” of a timetable, both at individual and aggregate levels. This score would be based on a flexible and adaptable system of penalties and rewards tied to specific metrics, allowing institutions to tailor the index to their unique needs and priorities.\nUse cases for TQI include:\n\nBenchmarking and comparison\nThe index allows institutions to compare timetable quality across different programmes, departments, or even years, facilitating the identification of best practices and areas for improvement. It would aid in benchmarking timetable models and approaches.\n\n\nResource optimisation\nInsights from the index can help institutions allocate resources, such as lecture rooms and teaching staff, more effectively by identifying underutilised or overbooked facilities.\n\n\nStudent experience enhancement\nBy prioritising metrics related to student well-being, such as travel time and consecutive teaching hours, institutions can enhance the overall student experience and satisfaction.\n\n\nData-driven decision making\nHistorical timetable quality data can inform future planning and course scheduling, allowing institutions to anticipate and address potential issues proactively.\n\n\nStakeholder communication\nThe TQI can serve as a transparent and data-driven tool for communicating the performance and challenges of the timetabling process to various stakeholders, including faculty, students, and administrative staff. It takes the guesswork out of the discussion.\n\n\n\nImplemented Metrics\nThe foundation of this quality index lies in defining and calculating specific metrics that capture various aspects of timetable quality.\n\nConstraint or preference violations\nIndividual timetables have certain measurable qualities - the shape and feel of the timetable. Often, these can be summarised into rules or constraints - either desirable qualities to strive for or undesirable qualities to avoid. They can be ‘hard’ - must not be violated, or ‘soft’ - should not be violated.\nThe presence or absence of these qualities on an individual’s timetable can be measured in the form of a reward or penalty. The flexibility of the graph database allows for experimentation in terms of which metrics are most relevant, how they should be calculated, where they should be stored, and how they should be weighted in the overall quality score.\nExamples include:\n\nmaximum hours per day, e.g., no more than 6 hours of teaching per day\nmaximum consecutive hours, e.g., no more than 3 hours of teaching without a break\nminimum hours per day, e.g., at least 2 hours of teaching per day\nlunch break, e.g., should have a break between 12-2pm\nminimal idle time, e.g., no more than a 4-hour gap between activities on a day\npreferred timeblocks, e.g., bonus points for activities scheduled in the core of the day\n\n\n\n\nDistance-based metrics\nBy incorporating room location data, we can calculate travel distances and times between activities. Long travel times or back-to-back activities in distant locations would incur penalties. See Rooms and Spaces for more details.\n\n\nResource Utilisation\nMetrics related to room utilisation, such as occupancy rates and frequency of use, can provide insights into the efficient allocation of space. Low utilisation rates could be penalised, encouraging more effective use of resources.\n\n\nActivity Characteristics\nFactors like activity clashes, oversubscription rates, and room suitability (size, type) can impact student experience. Penalties could be based on the severity of these characteristics.",
    "crumbs": [
      "Home",
      "Timetable Metrics",
      "Timetable Metrics"
    ]
  },
  {
    "objectID": "04-04-timetable-summary.html",
    "href": "04-04-timetable-summary.html",
    "title": "TQI Summary",
    "section": "",
    "text": "Visualisation of Results\nThe calculated scores and underlying metrics can be effectively visualised using various techniques:\nBloom visualisations in Neo4j: These can provide an intuitive overview of timetable quality across different programmes, time slots, or other groupings. They enable users to explore hierarchical relationships, identify patterns and outliers, and drill down into specific data points. See [Appendix-perspectives.qmd] for initial ideas.\nCharts and dashboards: Bar charts, line graphs, and heatmaps can be used to display and compare scores, identify trends, and track changes over time. Interactive dashboards can be built to provide a various views of timetable quality metrics and enable stakeholders to explore the data, identify trends, and make informed decisions.\n\n\nPotential Challenges\nWhile the concept of a timetable quality index offers many benefits, there are several challenges to acknowledge:\nData quality and availability: As with any analysis, accuracy and completeness of timetable data is crucial to calculate reliable quality scores. Inconsistent or missing data can lead to inaccurate results and skewed conclusions.\nComplexity of metrics: Defining and calculating meaningful metrics that capture the nuances of timetable quality can be challenging and time-consuming.\nMetric Definition and Weighting: Ironically, the very attempt to quantify quality is based on subjective judgements - which metrics to include, how to calculate them and how to weight them.\n\n\nBenefits and Future Development\nA timetable quality index is a potentially a powerful mechanism which can help universities gain a more quantifiable and data-driven understanding of their timetabling function.\nThe flexibility of graph allows for rapid prototyping, experimentation and deployment of new metrics and scoring systems. This can help institutions to identify areas for improvement, allocate resources more effectively, and enhance the overall student experience.\nFuture developments could include:\n\nIdentifying additional metrics that capture the quality of timetables more comprehensively.\nRefining the weighting system for penalties and rewards based on stakeholder feedback and institutional priorities.\nIncorporating additional datasets, such as student preferences or transportation schedules, to enhance the accuracy and granularity of the quality index.\nIncorporating additional constraints and preferences, such as room suitability, staff availability, student preferences and any reasonable adjustments.\nDeveloping interactive dashboards that allow users to explore timetable data, simulate changes, and assess their impact on the quality score.",
    "crumbs": [
      "Home",
      "Timetable Metrics",
      "TQI Summary"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgement",
    "section": "",
    "text": "First, I would like to express my gratitude to my supervisor, Dr. Xiaodong Li, for their guidance, support and feedback throughout this project. They were particular helpful in keeping me focused and staying on topic, allowing me to incrementally achieve my goals.\nI extend my sincere thanks to the faculty and staff of the MSc Data Science programme for providing the environment and resources necessary for my project:\n\nDr Paul Matthews - Course lead for MSc Data Science\nPrakash Chatterjee - Data Management Fundamentals\n\nReintroduced the concept of graph databases and noSQL.\n\nDr David Wyatt - Programming for Data Science\n\nProvided foundations of Python programming, Git version control, markdown.\n\nDr Hisham Ihshaish - Machine Learning and Predictive Analytics\n\nIntroduced me to Machine Learning, marrying programming and statistics.\n\nDr Deirdre Toher - Advanced Statistics\n\nPulled me through challenging statistics, introduced me to Quarto (in an R context) and was generally very supportive over the years. Now working for Central Statistics Office in Ireland.\n\nDr Jason Anquandah - Interdisciplinary Group Project\n\nSupported me through the challenges of a group project.\n\nDr Mahmoud Elbattah - Interdisciplinary Group Project\n\n\nSupported me through the challenges of a group project.\n\n\n\nI am indebted to my colleagues in SDS for their support, camaraderie, intellectual discussions and willingness to share knowledge, expertise and time. I am especially grateful to my mentor, colleague, manager and friend, Esther Williams, who has supported me throughout this journey and others, always magnanimously sharing her wisdom and unwavering encouragement.\nFinally, I would like to thank my family and friends for their unwavering love and support throughout my graduate studies. Their encouragement and understanding have been invaluable in helping me navigate the challenges and celebrate the successes along the way.\nIn the spirit of graphs…\n\n\n\nAcknowledgement graph",
    "crumbs": [
      "Home",
      "Acknowledgements"
    ]
  },
  {
    "objectID": "appendix-code-intro.html",
    "href": "appendix-code-intro.html",
    "title": "E: ETL Summary",
    "section": "",
    "text": "For the proof-of-concept, the following programmes and associated data (students, staff, modules, activities, rooms, etc.) were extracted from the source system and transformed before being loaded into a Neo4j cloud instance.\nThe table below summarises the time taken for each programme.\n\n\n\n\n\n\n\n\n\n\n\n\npos\nlevel\nhostkey\ncount\nextract & process\ngdrive\nneo4j\n\n\n\n\nArtificial Intelligence\nPG\nI400\n16\n26.8s\n26.1s\n2m 50.6s\n\n\nData Science\nPG\nINB112\n206\n57.6s\n26.5s\n1m 35.1s\n\n\nMathematics\nUG\nG90D\n103\n38.8s\n25.5s\n6m 12.9s\n\n\nComputer Science\nUG\nI10J\n431\n1m 9.8s\n24.1s\n11m 16.1s\n\n\nComputer Science\nUG\nG500\n45\n30s\n24.4s\n2m 9.5s\n\n\nCyber Security and Digital Forensics\nUG\nG4H4\n271\n57.4s\n51.9s\n6m 36.3s\n\n\nCyber Security\nPG\nI900\n216\n45.7s\n25.9s\n3m 7.5s\n\n\nInformation Management\nPG\nP110\n42\n17.2s\n25.8s\n1m 35.1s\n\n\nInformation Technology\nPG\nG56A12\n174\n28.3s\n25.8s\n2m 22.3s\n\n\n\nThe largest programme (Computer Science) took just over 1 minute to extract and process and 11 minutes to load into Neo4j. The Google Load consistently took ~25 seconds regardless of file sizes.\n\n\n\nComputer Science (I10J) - Department-Programme-Students\n\n\n\n\n\nComputer Science (I10J) - Department-Programme-Modules\n\n\nHowever, the current graph model creates a significant amount of relationships between nodes:\n\n\n\nnode/relationship\ncount\n\n\n\n\nprogramme (n)\n4\n\n\ndepartment (n)\n1\n\n\nhasOwningDept (r)\n4\n\n\nstudent (n)\n413\n\n\nregisteredOn (r)\n413\n\n\nmodule (n)\n11\n\n\nenrolledOn (r)\n1048\n\n\nactivity (n)\n1847\n\n\nattends (r)\n65493\n\n\nstaff (n)\n23\n\n\nteaches (r)\n538\n\n\nroom (n)\n32\n\n\noccupies (r)\n462\n\n\nactivityType (n)\n15\n\n\nhasType (r)\n1847",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "ETL Summary"
    ]
  },
  {
    "objectID": "appendix-code1-config.html",
    "href": "appendix-code1-config.html",
    "title": "G: Config and Misc",
    "section": "",
    "text": "Graph Timetable - Quarto - Config and Misc\n\nconfig.py\nlogger_config.py\nneo4j_schema.py\nutils.py\nconnect_to_neo4j.py\nconnect_to_rdb.py",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Config and Misc"
    ]
  },
  {
    "objectID": "appendix-code3-extract.html",
    "href": "appendix-code3-extract.html",
    "title": "I: Extract",
    "section": "",
    "text": "Graph Timetable - Quarto - Extract\n\ntemp_table_loader\nextract_sql_file\nextract_data\nextract_main",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Extract"
    ]
  },
  {
    "objectID": "appendix-code5-transform.html",
    "href": "appendix-code5-transform.html",
    "title": "K: Process",
    "section": "",
    "text": "Graph Timetable - Quarto - Process\n\nprocess_utils\nprocess_node\nprocess_main",
    "crumbs": [
      "Home",
      "Appendices",
      "ETL Summary and Code",
      "Transform"
    ]
  },
  {
    "objectID": "appendix-config.html",
    "href": "appendix-config.html",
    "title": "C: Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\"",
    "crumbs": [
      "Home",
      "Appendices",
      "Configuration"
    ]
  },
  {
    "objectID": "appendix-cypher2.html",
    "href": "appendix-cypher2.html",
    "title": "O: Deleting Nodes and Relationships",
    "section": "",
    "text": "Deleting nodes and relationships, using the DELETE clause in Cypher, is an important operation in managing the graph database.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher2.html#deleting-nodes",
    "href": "appendix-cypher2.html#deleting-nodes",
    "title": "O: Deleting Nodes and Relationships",
    "section": "Deleting Nodes",
    "text": "Deleting Nodes\nThe general syntax for deleting a node is:\nMATCH (n:NodeLabel {propertyName: propertyValue})\nDELETE n\nWhere:\n\nn is the node variable\nNodeLabel is the label assigned to the node\npropertyName is the property name\npropertyValue is the value assigned to the property\nDELETE is used to delete the node\nMATCH is used to find the node to delete\n... represents additional properties\n\n\nExample: Deleting a Student Node\nMATCH (s:Student {studentID: '123456'})\nDELETE s",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher2.html#deleting-relationships",
    "href": "appendix-cypher2.html#deleting-relationships",
    "title": "O: Deleting Nodes and Relationships",
    "section": "Deleting Relationships",
    "text": "Deleting Relationships\nThe general syntax for deleting a relationship is:\nMATCH (n1:NodeLabel1)-[r:RELATIONSHIP_TYPE]-&gt;(n2:NodeLabel2)\nDELETE r\nWhere:\n\nn1 and n2 are the node variables\nNodeLabel1 and NodeLabel2 are the labels assigned to the nodes\nr is the relationship variable\nRELATIONSHIP_TYPE is the type of relationship\nDELETE is used to delete the relationship\nMATCH is used to find the relationship to delete\n... represents additional properties\n\n\nExample: Deleting a Relationship Between a Student and an Activity\nMATCH (s:Student {studentID: '123456'})-[r:ATTENDS]-&gt;(a:Activity {activityID: '789'})\nDELETE r",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Deleting Nodes and  Relationships"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html",
    "href": "appendix-cypher5a.html",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "",
    "text": "This page explore different graph data structures and queries for the purposes of identifying student clashes. It illustrates the inherent flexibility of graph databases and that thorough modelling and profiling of the data can lead to more efficient and effective queries.\nUse-case is king when it comes to optimised databases and performant queries.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#scenario",
    "href": "appendix-cypher5a.html#scenario",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Scenario",
    "text": "Scenario\nEach model below will use the same basic scenario:\n\nTwo students - Alice and Bob\nThree activities:\n\nITGD - Introduction to Graph Databases\nNeo4j - Neo4j for Beginners\nTigerDB - TigerGraph for Data Scientists\nEach activity has a start and end time\nEach activity is scheduled for several weeks\n\nThere are deliberate clashes between the activities to illustrate the concept of a student clash",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-1---activity-occurrence",
    "href": "appendix-cypher5a.html#model-1---activity-occurrence",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Model 1 - Activity Occurrence",
    "text": "Model 1 - Activity Occurrence\nEach ‘occurrence’ of an activity is a separate node. This model is simple and easy to understand, but proliferates nodes which lead to inefficient and complex queries.\n\nCreate data\n// Create unique activity nodes (TestActivityModel1)\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-06\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-13\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-20\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-08-27\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"ITGD\", date: date(\"2024-09-03\"), startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-07-30\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-08-13\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"Neo4j\", date: date(\"2024-08-27\"), startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"TigerDb\", date: date(\"2024-08-06\"), startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") })\nCREATE (:TestActivityModel1 { actName: \"TigerDb\", date: date(\"2024-08-13\"), startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") });\n\n// Create unique student nodes (TestStudentModel1)\nCREATE (:TestStudentModel1 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel1 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\n// Create ATTENDS relationships (one student attends all TigerDb and Neo4j)\nMATCH (s:TestStudentModel1 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel1) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a);\n\nMATCH (s:TestStudentModel1 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel1) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 1\n\n\nTo identify the clashes, this query can be run:\n\nMATCH (s:TestStudentModel1)-[:ATTENDS]-&gt;(a1:TestActivityModel1)\nWITH s, a1\nMATCH (s)-[:ATTENDS]-&gt;(a2:TestActivityModel1) \nWHERE a1 &lt;&gt; a2 \n  AND a1.date = a2.date \n  AND (a1.startTime &lt; a2.endTime AND a1.endTime &gt; a2.startTime)  //  overlap condition\n  AND NOT (a1.startTime = a2.endTime OR a1.endTime = a2.startTime) // xxclude \"touching\" cases\n  AND a1.actName &lt; a2.actName  // ensures only one direction of the pair is returned\nRETURN s.stuFirstName_anon AS Student, \n       a1.date AS ClashDate, \n       a1.actName AS Activity1, \n       a1.startTime + \"-\" + a1.endTime AS Timeslot1, \n       a2.actName AS Activity2, \n       a2.startTime + \"-\" + a2.endTime AS Timeslot2\nORDER BY Student, ClashDate;\nWhich correctly identifies Bob’s clash:\n\n\n\nModel 1 results\n\n\nOne way of measuring and comparing query performance is to look at the PROFILE and dbhits. In this instance there are 278 database accesses.\n\n\n\nModel 1 profile",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-2---date-and-time-nodes",
    "href": "appendix-cypher5a.html#model-2---date-and-time-nodes",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Model 2 - Date and Time Nodes",
    "text": "Model 2 - Date and Time Nodes\nModel 2 uses a single node for each activity but has date and time nodes. This model is more complex in that there are more node labels, but can be more efficient for certain queries.\n\nCreate data\n// Create unique time nodes\nCREATE (:TestStartTimeNode { time: localtime(\"09:00:00\") })\nCREATE (:TestStartTimeNode { time: localtime(\"10:00:00\") })\nCREATE (:TestStartTimeNode { time: localtime(\"11:00:00\") })\nCREATE (:TestEndTimeNode { time: localtime(\"11:00:00\") })\nCREATE (:TestEndTimeNode { time: localtime(\"12:00:00\") })\n\n// Create unique date nodes\nCREATE (:TestDateNode { date: date(\"2024-07-30\") })\nCREATE (:TestDateNode { date: date(\"2024-08-06\") })\nCREATE (:TestDateNode { date: date(\"2024-08-13\") })\nCREATE (:TestDateNode { date: date(\"2024-08-20\") })\nCREATE (:TestDateNode { date: date(\"2024-08-27\") })\nCREATE (:TestDateNode { date: date(\"2024-09-03\") })\n\n// Create activity nodes\nCREATE (:TestActivityModel2 { actName: \"ITGD\" })\nCREATE (:TestActivityModel2 { actName: \"Neo4j\" })\nCREATE (:TestActivityModel2 { actName: \"TigerDb\" });\n\n// Connect ITGD to dates and times (using MERGE)\nMATCH (a:TestActivityModel2 { actName: \"ITGD\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"09:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"11:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Connect Neo4j to dates and times (adjust dates/times and use MERGE)\nMATCH (a:TestActivityModel2 { actName: \"Neo4j\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"10:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"11:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Connect TigerDb to dates and times (adjust dates/times and use MERGE)\nMATCH (a:TestActivityModel2 { actName: \"TigerDb\" })\nMATCH (d:TestDateNode) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d)\nWITH a\nMATCH (st:TestStartTimeNode { time: localtime(\"11:00:00\") })\nMATCH (et:TestEndTimeNode { time: localtime(\"12:00:00\") })\nMERGE (a)-[:STARTS_AT]-&gt;(st)\nMERGE (st)-[:ENDS_AT]-&gt;(et);\n\n// Create Students and ATTENDS relationships (same as Model 1)\nCREATE (:TestStudentModel2 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel2 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel2 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel2) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a);\n\nMATCH (s:TestStudentModel2 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel2) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 2\n\n\nTo identify student clashes, this query can be run.\nMATCH (s:TestStudentModel2)-[:ATTENDS]-&gt;(a1:TestActivityModel2)-[:SCHEDULED_ON]-&gt;(d:TestDateNode)\nWITH s, a1, d\nMATCH (s)-[:ATTENDS]-&gt;(a2:TestActivityModel2)-[:SCHEDULED_ON]-&gt;(d) // Same date\nMATCH (a1)-[:STARTS_AT]-&gt;(st1:TestStartTimeNode)-[:ENDS_AT]-&gt;(et1:TestEndTimeNode) \nMATCH (a2)-[:STARTS_AT]-&gt;(st2:TestStartTimeNode)-[:ENDS_AT]-&gt;(et2:TestEndTimeNode) \nWHERE a1 &lt;&gt; a2 \n  AND (st1.time &lt; et2.time AND et1.time &gt; st2.time)  //  overlap condition\n  AND NOT (st1.time = et2.time OR et1.time = st2.time) // xxclude \"touching\" cases\n  AND a1.actName &lt; a2.actName  // ensures only one direction of the pair is returned\nRETURN  s.stuFirstName_anon AS Student, \n        d.date AS ClashDate, \n        a1.actName AS Activity1, \n        st1.time + \"-\" + et1.time AS Timeslot1,\n        a2.actName AS Activity2,\n        st2.time + \"-\" + et2.time AS Timeslot2\nORDER BY Student, ClashDate;\nThe results are the same as Model 1, but the profile is different, with fewer database accesses (143):\n\n\n\nModel 2 profile",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#model-3---date-nodes",
    "href": "appendix-cypher5a.html#model-3---date-nodes",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Model 3 - Date Nodes",
    "text": "Model 3 - Date Nodes\nModel 3 uses a single node for each activity as well as date nodes - start and end times are properties of the activity.\n// Create unique date nodes\nCREATE (:TestDateNodeModel3 { date: date(\"2024-07-30\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-06\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-13\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-20\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-08-27\") })\nCREATE (:TestDateNodeModel3 { date: date(\"2024-09-03\") })\n\n// Create activity nodes with start/end times as properties\nCREATE (:TestActivityModel3 { actName: \"ITGD\", startTime: localtime(\"09:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel3 { actName: \"Neo4j\", startTime: localtime(\"10:00:00\"), endTime: localtime(\"11:00:00\") })\nCREATE (:TestActivityModel3 { actName: \"TigerDb\", startTime: localtime(\"11:00:00\"), endTime: localtime(\"12:00:00\") });\n\n// Connect Activities to Dates (using MERGE)\nMATCH (a:TestActivityModel3 { actName: \"ITGD\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\nMATCH (a:TestActivityModel3 { actName: \"Neo4j\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\nMATCH (a:TestActivityModel3 { actName: \"TigerDb\" })\nMATCH (d:TestDateNodeModel3) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:SCHEDULED_ON]-&gt;(d);\n\n// Create Students and ATTENDS relationships\nCREATE (:TestStudentModel3 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nCREATE (:TestStudentModel3 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel3 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel3) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a) ;\n\nMATCH (s:TestStudentModel3 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel3) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nCREATE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 3\n\n\nThe results are the same as Model 1 and Model 2, but the profile is different again with even fewer database accesses (58):\n\n\n\nModel 3 profile\n\n\n\nModel 4 -\nModel 4 uses a single node for each activity and date - start and end times are now properties of the relationship between the activity and the date.\n// Create unique date nodes\nMERGE (:TestDateNodeModel4 { date: date(\"2024-07-30\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-06\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-13\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-20\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-08-27\") })\nMERGE (:TestDateNodeModel4 { date: date(\"2024-09-03\") })\n\n// Create activity nodes \nMERGE (:TestActivityModel4 { actName: \"ITGD\" })\nMERGE (:TestActivityModel4 { actName: \"Neo4j\" })\nMERGE (:TestActivityModel4 { actName: \"TigerDb\" });\n\n// Connect ITGD to Dates with START and END relationships\nMATCH (a:TestActivityModel4 { actName: \"ITGD\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\"), date(\"2024-08-20\"), date(\"2024-08-27\"), date(\"2024-09-03\")]\nMERGE (a)-[:STARTS { time: localtime(\"09:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"11:00:00\") }]-&gt;(d);\n\n// Connect Neo4j to Dates (adjust dates and times)\nMATCH (a:TestActivityModel4 { actName: \"Neo4j\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-07-30\"), date(\"2024-08-13\"), date(\"2024-08-27\")]\nMERGE (a)-[:STARTS { time: localtime(\"10:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"11:00:00\") }]-&gt;(d);\n\n// Connect TigerDb to Dates (adjust dates and times)\nMATCH (a:TestActivityModel4 { actName: \"TigerDb\" })\nMATCH (d:TestDateNodeModel4) WHERE d.date IN [date(\"2024-08-06\"), date(\"2024-08-13\")]\nMERGE (a)-[:STARTS { time: localtime(\"11:00:00\") }]-&gt;(d)\nMERGE (a)-[:ENDS { time: localtime(\"12:00:00\") }]-&gt;(d);\n\n// Create Students and ATTENDS relationships\nMERGE (:TestStudentModel4 { stuFirstName_anon: \"Alice\", stuID_anon: \"test-student-1\" })\nMERGE (:TestStudentModel4 { stuFirstName_anon: \"Bob\", stuID_anon: \"test-student-2\" });\n\nMATCH (s:TestStudentModel4 { stuID_anon: \"test-student-1\" })\nMATCH (a:TestActivityModel4) WHERE a.actName IN [\"TigerDb\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\nMATCH (s:TestStudentModel4 { stuID_anon: \"test-student-2\" })\nMATCH (a:TestActivityModel4) WHERE a.actName IN [\"ITGD\", \"Neo4j\"]\nMERGE (s)-[:ATTENDS]-&gt;(a) ;\n\n\n\nModel 4\n\n\nThe results are again the same as Model 1, Model 2, and Model 3, but the profile is different with the most database accesses (293):\n\n\n\nModel 4 profile",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#conclusion",
    "href": "appendix-cypher5a.html#conclusion",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Conclusion",
    "text": "Conclusion\nEach model has its own strengths and weaknesses. The choice of model will depend on the specific requirements. The more complex models can be more efficient for certain queries, but can also be more difficult to understand and maintain. The simpler models are easier to understand and maintain, but can be less efficient for certain queries.\nOf the four tested, Model 3 was the most efficient in terms of database hits on the very small test dataset used. However, this may not be the case with larger datasets. It is important to profile the queries and the data to determine the best model for the specific requirements.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-cypher5a.html#delete-data",
    "href": "appendix-cypher5a.html#delete-data",
    "title": "S: Student Clashes - Deeper Dive",
    "section": "Delete Data",
    "text": "Delete Data\nThe cypher below deletes all test data.\n\nModel 1\n// Delete all TestActivityModel1 nodes\nMATCH (a:TestActivityModel1)\nDETACH DELETE a; \n\n// Delete all TestStudentModel1 nodes\nMATCH (s:TestStudentModel1)\nDETACH DELETE s;\n\n\nModel 2\n// Delete test data for Model 2\nMATCH (n) \nWHERE n:TestStudentModel2 OR n:TestActivityModel2 OR n:TestDateNode OR n:TestStartTimeNode OR n:TestEndTimeNode\nDETACH DELETE n\n\n\nModel 3\n// Delete test data for Model 3\nMATCH (n) \nWHERE n:TestStudentModel3 OR n:TestActivityModel3 OR n:TestDateNodeModel3 \nDETACH DELETE n\n\n\nModel 4\nMATCH (n) \nWHERE n:TestStudentModel4 OR n:TestActivityModel4 OR n:TestDateNodeModel4\nDETACH DELETE n",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Student Clashes"
    ]
  },
  {
    "objectID": "appendix-perspectives.html",
    "href": "appendix-perspectives.html",
    "title": "V: Perspectives and Scenes for Neo4j Explore Functionality",
    "section": "",
    "text": "This page contains example scenarios for demonstrating Neo4j “Perspectives” and “Scenes” which are used to explore data in a graph database from the perspective of a particular user. Perspectivies focus the view by selecting specific nodes and relationships, while scenes allow for visualising the data interactively.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Perspectives"
    ]
  },
  {
    "objectID": "appendix-perspectives.html#scenario-1-programme-leader-perspective",
    "href": "appendix-perspectives.html#scenario-1-programme-leader-perspective",
    "title": "V: Perspectives and Scenes for Neo4j Explore Functionality",
    "section": "Scenario 1: Programme Leader Perspective",
    "text": "Scenario 1: Programme Leader Perspective\nGoal: Enable a programme lead to gain insights into their programme’s structure, student engagement, and potential areas for improvement.\n\nPerspective:\nNodes:\n\nProgramme (central node)\nModule\nStudent\nActivity\nRoom\nStaff\n\nRelationships:\n\nProgramme &lt;-[:BELONGS_TO]- Module\nModule &lt;-[:BELONGS_TO]- Activity\nStudent -[:REGISTERED_ON]-&gt; Programme\nStudent -[:ATTENDS]-&gt; Activity\nStudent-[:ENROLLED_ON]-&gt; Module\nActivity -[:OCCUPIES]-&gt; Room\nStaff -[:TEACHES]-&gt; Activity\n\n\n\nScenes:\nProgramme Overview: Visualise the entire programme structure, showcasing modules, their connections, and associated activities. Highlight popular and less popular modules based on student enrolment.\nStudent Engagement: Focus on a specific module and visualise student attendance patterns. Identify students with low attendance and potential at-risk students.\nResource Allocation: Visualise room utilisation across the programme’s activities. Identify potential scheduling conflicts or underutilised spaces.\nStaff Workload: Analyse the distribution of teaching workload among staff members within the programme.\n\n\nExample Cypher Queries:\n//Programme Structure:\n\nMATCH (p:programme {name: 'Computer Science'})&lt;-[:BELONGS_TO]-(m:module)\nRETURN p, m\n\n\n//Student Attendance:\n\nMATCH (m:module {name: 'Data Structures'})-[:BELONGS_TO]-&gt;(p:programme)&lt;-[:REGISTERED_ON]-(s:student)-[:ATTENDS]-&gt;(a:Activity)\nRETURN m, s, a\n\n\n//Room Utilisation:\n\nMATCH (p:programme)&lt;-[:BELONGS_TO]-(m:module)&lt;-[:BELONGS_TO]-(a:activity)-[:OCCUPIES]-&gt;(r:room)\nRETURN p, m, a, r\n\nStaff Workload:\n\nMATCH (p:programme)&lt;-[:BELONGS_TO]-(m:module)&lt;-[:BELONGS_TO]-(a:activity)&lt;-[:TEACHES]-(s:staff)\nRETURN p, m, a, s",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Perspectives"
    ]
  },
  {
    "objectID": "appendix-perspectives.html#scenario-2-module-leader-perspective",
    "href": "appendix-perspectives.html#scenario-2-module-leader-perspective",
    "title": "V: Perspectives and Scenes for Neo4j Explore Functionality",
    "section": "Scenario 2: Module Leader Perspective",
    "text": "Scenario 2: Module Leader Perspective\nGoal: Provide a module leader with insights into their module’s scheduling, student performance, and room allocation.\n\nPerspective:\nNodes:\n\nModule (central node)\nActivity\nStudent\nRoom\nAssessment\n\nRelationships:\n\nmodule &lt;-[:BELONGS_TO]- activity\nstudent -[:ATTENDS]-&gt; activity\nactivity -[:OCCUPIES]-&gt; room\nstudent -[:HAS_RESULT_FOR]-&gt; assessment\nmodule -[:HAS_ASSESSMENT]-&gt; assessment\n\n\n\nScenes:\n\nModule Schedule: Visualise the module’s activities, their timeslots, and associated rooms. Highlight potential scheduling conflicts or overlaps.\nStudent Performance: Focus on a specific assessment and visualise student results. Identify students who may need additional support.\nRoom Suitability: Analyse the rooms used for the module’s activities and their capacities. Identify potential issues with room size or suitability.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Perspectives"
    ]
  },
  {
    "objectID": "appendix-perspectives.html#scenario-3-administrator-perspective-room-usage",
    "href": "appendix-perspectives.html#scenario-3-administrator-perspective-room-usage",
    "title": "V: Perspectives and Scenes for Neo4j Explore Functionality",
    "section": "Scenario 3: Administrator Perspective (Room Usage)",
    "text": "Scenario 3: Administrator Perspective (Room Usage)\nGoal: Ability to analyse room usage, identify underutilised spaces, and optimise room allocation.\n\nPerspective:\nNodes:\n\nRoom (central node)\nActivity\nModule\nProgramme\n\nRelationships:\n\nactivity -[:OCCUPIES]-&gt; room\nmodule &lt;-[:BELONGS_TO]- activity\nprogramme &lt;-[:BELONGS_TO]- module\n\n\n\nScenes:\n\nRoom Utilization Overview: Visualise all rooms and their occupancy levels across different time periods. Highlight rooms with low utilisation or frequent conflicts.\nRoom Activity Breakdown: Focus on a specific room and visualise the activities scheduled there. Identify peak usage times and potential scheduling issues.\nProgramme Room Usage: Analyse room usage by different programmes. Identify programmes with high room demands or potential conflicts.",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Perspectives"
    ]
  },
  {
    "objectID": "appendix-supervision1.html",
    "href": "appendix-supervision1.html",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "",
    "text": "This week has been about getting back on track after an extended period away from the dissertation. It has mainly revolved around refocusing, rescoping, reacquainting and rekindling motivation.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#summary",
    "href": "appendix-supervision1.html#summary",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "",
    "text": "This week has been about getting back on track after an extended period away from the dissertation. It has mainly revolved around refocusing, rescoping, reacquainting and rekindling motivation.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#accomplishments",
    "href": "appendix-supervision1.html#accomplishments",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nResults:\n\nrecreated single student graph timetable - see poc-1-basic\ncreated SQL to csv pipeline\ncreated cypher queries\n\nProject Management\n\nnew github repo (currently private) https://github.com/zoonalink/graph-project\n\ndraft readme\nfolder/file structure\n\nweekly update template\n\nData Collection:\n\nSQL to CSV files for single student\nraw SQL data tables\n\nAnalysis / Wrangling:\n\nTransformation in SQL query\n\nModel Development:\nInvestigation\n\npipelines to automate data flow - prefect\nhow to represent time in graph",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#issuesblockers",
    "href": "appendix-supervision1.html#issuesblockers",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nOriginal servers were deleted and access to new servers was gone\nSome original work no longer works\n\nMethodological:\n\nI need to ensure that scope does not creep.\nI need to stay focused and not start solving problems which are out of scope or are interesting.\n\nData-Related:\n\ndevelop a robust anonymisation process\nrepresenting time - see representing time",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#next-steps",
    "href": "appendix-supervision1.html#next-steps",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goals\n\nBigger cohort of data (MSc DataScience?)\nExplore time in Graph\nData pipeline documentation and steps (anonymisation)\n\n\nProject Management Project tasks - planning, admin.\n\nadd supervisor to github repo\n\nData:\n\nAnonymisation or generation\nMSc Data Science cohort isolated into separate csv files\nsplitting data into single rows\nPipeline steps plotted\n\nAnalysis / Wrangling:\n\nMore advanced cypher queries\n\nModeling:\n\nComparing different representations of time\n\nValidation:\nDeadlines:",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#post-meeting-notes",
    "href": "appendix-supervision1.html#post-meeting-notes",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\nWe met on 13 June 2024 for approximatly 45 minutes. I showed Xiaodong what I have been up to referencing my working files, poc files, and some code. I also showed what I currently have in my free instance of neo4j aura - which was MSc DS students activities and rooms. Staff and Students to be loaded but I want to ensure my anonymisation function is tested first.\nWe discussed what my aims are with this project, clarifying that it is not an building a timetable schedule based on graph structure; it is also not a timetable optimiser. Instead it is a data engineering project that ultimately explores whether representing timetables in graph format can bring opportunities for reporting and insight which is currently difficult to produce using the current system.\nWe agreed to meet in two weeks time. I will schedule a meeting for us.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision1.html#additional-notes",
    "href": "appendix-supervision1.html#additional-notes",
    "title": "X: Fortnightly Update - 10 June 2024",
    "section": "Additional Notes",
    "text": "Additional Notes\nSummary from intial meeting\nI met with Xiaodong Li my supervisor on Wednesday 01 May 2024 where we introduced ourselves and our backgrounds. We discussed my proposal which Xiaodong had read in advance of our meeting.\nI showed Xiaodong my work so far, most of it completed in January which included proof-of-concept data engineering steps to extract, transform and load data from a relational database to a graph structure. I showed the steps I had taken, the challenges encountered and the possible opportunities of graph data structures which I am hoping to explore in more detail.\nWe discussed project scope and outcomes and recognised that I need to ensure that I keep within scope.\nThe next few weeks will require attention on other matters (work, taught modules, etc.) but I will look to pick up project work soon.\nIf possible, Xiaodong was going to investigate Neo4j and graph to get a bit of context.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "X: Fortnightly Update - 10 June 2024"
    ]
  },
  {
    "objectID": "appendix-supervision3.html",
    "href": "appendix-supervision3.html",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "",
    "text": "This week has been primarily based on getting the ETL pipeline working.\nSome thought and planning about the overall project",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#summary",
    "href": "appendix-supervision3.html#summary",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "",
    "text": "This week has been primarily based on getting the ETL pipeline working.\nSome thought and planning about the overall project",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#accomplishments",
    "href": "appendix-supervision3.html#accomplishments",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "Accomplishments",
    "text": "Accomplishments\n\nProject Management\n\nPlanned next 6 weeks - high level\n\nData Collection:\n\nN/A\n\nAnalysis / Wrangling:\n\nN/A\n\nModel Development:\n\nThe ETL is (hopefully) production ready.\n\nResults:\n\nFunctioning ETL which:\n\nExtracts -&gt; Filters -&gt; Cleans -&gt; Transforms -&gt; Anonymises -&gt; Uploads to Google Drive -&gt; Loads to Neo4j\nConfigurable, scalable, modular\nLogging, Error-handling",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#next-steps",
    "href": "appendix-supervision3.html#next-steps",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "Next Steps",
    "text": "Next Steps\n\nWeekly Goal: What is goal of week?\n\nProject Management\n\nPlan next six weeks - with tasks\nsee project-structure\n\nData:\n\nClear out database and repopulate\nRun ETL for some programme - e.g. UG maths, PG Data Sci, PG AI, PG cyber\nConsider Unit Tests\n\nAnalysis / Wrangling:\n\nDevelop and explore Cypher quality and violation queries\nsee queries\n\nModeling:\n\nConsider different model of time as nodes (but probably will not develop)\n\nWriting\n\nAfter ETL confirmation - fully documentation of code\n\nAlso write up into QMD doc\nVisualiation - Data Flow Diagram?\n\nModel section write up\n\nSQL data model\nGraph data model\nVisualisations\nsee code-critique\n\nDraft Intro\n\nProof of concept\nResearch question\nDomain",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#issuesblockers",
    "href": "appendix-supervision3.html#issuesblockers",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "Issues/Blockers",
    "text": "Issues/Blockers\n\nTechnical:\n\nMain limitations are free Neo4j Aura software and other restrictions\n\nMethodological:\n\nNeed to think about what I want to deliver and how it looks",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#post-meeting-notes",
    "href": "appendix-supervision3.html#post-meeting-notes",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "Post-Meeting Notes",
    "text": "Post-Meeting Notes\n\nKey Decisions: What were the main takeaways from meeting?\n\nI need to focus on making progress and think about what the final produce will be.\n\nAction Items: What tasks do you need to complete?\n\nsee above - write up sections, even if draft",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-supervision3.html#additional-notes",
    "href": "appendix-supervision3.html#additional-notes",
    "title": "Z: Fornightly Update - 2024-07-08",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nLiterature Review: Relevant papers read.\nExperiments: If applicable, describe any experiments conducted.\nCode: Embedding example snippets of code.",
    "crumbs": [
      "Home",
      "Appendices",
      "Supervision",
      "Notes Example 3"
    ]
  },
  {
    "objectID": "appendix-toc.html",
    "href": "appendix-toc.html",
    "title": "Appendix: Table of Contents",
    "section": "",
    "text": "This project contains several appendices that provide additional information, resources and code.\nThe appendices are listed below:\nA. Random Graph Generator B. Technology Stack C. Configuration D. Anonymisation E. ETL Summary F. ETL Code G. Config and Misc H. Extract-SQL I. Extract J. Google Drive Load K. Transform L. Neo4j Load M. Cypher Queries N. Creating Nodes and Relationships O. Deleting Nodes and Relationships P. General Queries Q. Count Queries R. Hard (timetabling) Constraints S. Student Clashes T. Soft Constraints U. Rooms and Spaces V. Perspectives W. Supervision X. Notes Example 1 Y. Notes Example 2 Z. Notes Example 3"
  },
  {
    "objectID": "01-04-project.html",
    "href": "01-04-project.html",
    "title": "Project Aims and Scope",
    "section": "",
    "text": "The primary aim of this project is to investigate the viability of using graph data structures for enhanced timetabling analytics and reporting.\n\nObjectives\n\n\nDesigning an extensible, system-agnostic graph data model for university timetables\nDeveloping a configurable ETL (extract, transform, load) pipeline to transition from relational to graph database representations of timetables\nDiscussing how graph-based approaches to timetabling analysis could contribute to measuring and improving timetable quality.\n\nIn order to achieve these objectives, I implement and evaluate a set of proof-of-concept analytical metrics which leverage the graph data model whilst discussing performance capabilities (and limitations) against traditional relational approaches.\nIt is important to note that this project is positioned as a proof-of-concept and exploratory study.\nI will not:\n\nreinvent a full-scale timetabling system\nattempt to optimise real-time timetable generation\n\nI will:\n\nfocus on the data engineering aspects of transitioning from relational to graph data models\ndemonstrate the potential of graph-based approaches in the analysis of university timetables\nprovide a foundation for future analytical work.\n\nLet’s graph!\n\n\n\n\n\n\n\nG\n\n\n\n4zj99lz2\n\n\n\n\n666n359b\n\n\n\n\nu0upp5hj\n\n\n\n\n666n359b-&gt;u0upp5hj\n\n\n\n\n\n4s3xzocl\n\n\n\n\nk8yaaeoi\n\n\n\n\n4s3xzocl-&gt;k8yaaeoi\n\n\n\n\n\n9bbhq0e8\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n5cymetka\n\n\n\n\ne4qukjnm\n\n\n\n\n5cymetka-&gt;e4qukjnm\n\n\n\n\n\nhx0hp6dr\n\n\n\n\nhx0hp6dr-&gt;hx0hp6dr\n\n\n\n\n\nk8yaaeoi-&gt;k8yaaeoi\n\n\n\n\n\n14ae1pfz\n\n\n\n\nk8yaaeoi-&gt;14ae1pfz\n\n\n\n\n\njrtct42d\n\n\n\n\nglv43ych\n\n\n\n\nxpzdirk1\n\n\n\n\nglv43ych-&gt;xpzdirk1\n\n\n\n\n\n06x4bmzo\n\n\n\n\nglv43ych-&gt;06x4bmzo\n\n\n\n\n\nmis1btmm\n\n\n\n\nglv43ych-&gt;mis1btmm\n\n\n\n\n\nce1516j0\n\n\n\n\nglv43ych-&gt;ce1516j0\n\n\n\n\n\ngzxrlpod\n\n\n\n\nc114j2tw\n\n\n\n\ngzxrlpod-&gt;c114j2tw\n\n\n\n\n\n8bpimk72\n\n\n\n\n8bpimk72-&gt;k8yaaeoi\n\n\n\n\n\n8bpimk72-&gt;14ae1pfz\n\n\n\n\n\nur4yo3tx\n\n\n\n\n8bpimk72-&gt;ur4yo3tx\n\n\n\n\n\nonkctu6x\n\n\n\n\nonkctu6x-&gt;onkctu6x\n\n\n\n\n\n85hi4ca2\n\n\n\n\nonkctu6x-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep\n\n\n\n\nonkctu6x-&gt;x0n5twep\n\n\n\n\n\nc1sdjw8j\n\n\n\n\nc1sdjw8j-&gt;hx0hp6dr\n\n\n\n\n\nc1sdjw8j-&gt;u0upp5hj\n\n\n\n\n\nxh1xin78\n\n\n\n\nxztqibcz\n\n\n\n\nxh1xin78-&gt;xztqibcz\n\n\n\n\n\nvr6qql5z\n\n\n\n\nvr6qql5z-&gt;666n359b\n\n\n\n\n\nwxhhelge\n\n\n\n\nvr6qql5z-&gt;wxhhelge\n\n\n\n\n\n3ytehise\n\n\n\n\nvr6qql5z-&gt;3ytehise\n\n\n\n\n\n2m5wokx2\n\n\n\n\nmfp9is99\n\n\n\n\n2m5wokx2-&gt;mfp9is99\n\n\n\n\n\nv4x5duoh\n\n\n\n\nv4x5duoh-&gt;2m5wokx2\n\n\n\n\n\nfovglxww\n\n\n\n\nv4x5duoh-&gt;fovglxww\n\n\n\n\n\nv4x5duoh-&gt;xztqibcz\n\n\n\n\n\n89w9dxkj\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n1chkeldv\n\n\n\n\n89w9dxkj-&gt;1chkeldv\n\n\n\n\n\n89w9dxkj-&gt;mis1btmm\n\n\n\n\n\nxpzdirk1-&gt;gzxrlpod\n\n\n\n\n\nfovglxww-&gt;fovglxww\n\n\n\n\n\nfsp8cdjo\n\n\n\n\nfovglxww-&gt;fsp8cdjo\n\n\n\n\n\nfovglxww-&gt;mfp9is99\n\n\n\n\n\njnazmo3s\n\n\n\n\njnazmo3s-&gt;e4qukjnm\n\n\n\n\n\n72a2d6xd\n\n\n\n\nsj35t4ss\n\n\n\n\n72a2d6xd-&gt;sj35t4ss\n\n\n\n\n\nlfwuczry\n\n\n\n\n35yrnabr\n\n\n\n\nlfwuczry-&gt;35yrnabr\n\n\n\n\n\n8cgoonso\n\n\n\n\nlfwuczry-&gt;8cgoonso\n\n\n\n\n\nhv8k4g84\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nhcqaqm1o\n\n\n\n\nhcqaqm1o-&gt;666n359b\n\n\n\n\n\nqy17mwag\n\n\n\n\nhcqaqm1o-&gt;qy17mwag\n\n\n\n\n\n53bgisfb\n\n\n\n\nhcqaqm1o-&gt;53bgisfb\n\n\n\n\n\n1k9klz8v\n\n\n\n\neacs5e9j\n\n\n\n\n1k9klz8v-&gt;eacs5e9j\n\n\n\n\n\nj812m8am\n\n\n\n\n1k9klz8v-&gt;j812m8am\n\n\n\n\n\nieb7bdce\n\n\n\n\n1k9klz8v-&gt;ieb7bdce\n\n\n\n\n\nbav7hkue\n\n\n\n\nc114j2tw-&gt;bav7hkue\n\n\n\n\n\ne7injpqb\n\n\n\n\nc114j2tw-&gt;e7injpqb\n\n\n\n\n\nhj4rycy9\n\n\n\n\ns3okac0a\n\n\n\n\nhj4rycy9-&gt;s3okac0a\n\n\n\n\n\n85hi4ca2-&gt;hj4rycy9\n\n\n\n\n\n85hi4ca2-&gt;9bbhq0e8\n\n\n\n\n\nakih91pi\n\n\n\n\n85hi4ca2-&gt;akih91pi\n\n\n\n\n\nwxhhelge-&gt;72a2d6xd\n\n\n\n\n\nwxhhelge-&gt;53bgisfb\n\n\n\n\n\niw0pfpmk\n\n\n\n\nwt9rtovp\n\n\n\n\niw0pfpmk-&gt;wt9rtovp\n\n\n\n\n\ndtmww06j\n\n\n\n\niw0pfpmk-&gt;dtmww06j\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nw084j1ko\n\n\n\n\nx0n5twep-&gt;w084j1ko\n\n\n\n\n\ny0sap20o\n\n\n\n\ny0sap20o-&gt;y0sap20o\n\n\n\n\n\ndxppp23s\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\npveq6y3l\n\n\n\n\ny0sap20o-&gt;pveq6y3l\n\n\n\n\n\nzefyvzdp\n\n\n\n\npzodwhlw\n\n\n\n\nhxejkaog\n\n\n\n\npzodwhlw-&gt;hxejkaog\n\n\n\n\n\n9bbhq0e8-&gt;dxppp23s\n\n\n\n\n\nm0ggeeei\n\n\n\n\n9bbhq0e8-&gt;m0ggeeei\n\n\n\n\n\n06x4bmzo-&gt;c114j2tw\n\n\n\n\n\nv850hpzb\n\n\n\n\n06x4bmzo-&gt;v850hpzb\n\n\n\n\n\n8bx5ks88\n\n\n\n\n06x4bmzo-&gt;8bx5ks88\n\n\n\n\n\nzyz74rqy\n\n\n\n\nzyz74rqy-&gt;hx0hp6dr\n\n\n\n\n\nzyz74rqy-&gt;c1sdjw8j\n\n\n\n\n\n2cicgpkb\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nc21k3g41\n\n\n\n\nzyz74rqy-&gt;c21k3g41\n\n\n\n\n\n6e82t0az\n\n\n\n\nsi800de6\n\n\n\n\nsi800de6-&gt;fovglxww\n\n\n\n\n\nsi800de6-&gt;si800de6\n\n\n\n\n\n4rv1fae9\n\n\n\n\n4rv1fae9-&gt;v4x5duoh\n\n\n\n\n\ndogjcfze\n\n\n\n\n4rv1fae9-&gt;dogjcfze\n\n\n\n\n\n4rv1fae9-&gt;dtmww06j\n\n\n\n\n\n4rv1fae9-&gt;3ytehise\n\n\n\n\n\nqvvjbgxm\n\n\n\n\nqy17mwag-&gt;vr6qql5z\n\n\n\n\n\nkjba91s3\n\n\n\n\nqy17mwag-&gt;kjba91s3\n\n\n\n\n\n9ifdi52l\n\n\n\n\ns3tdzgmu\n\n\n\n\n9ifdi52l-&gt;s3tdzgmu\n\n\n\n\n\n14ae1pfz-&gt;eacs5e9j\n\n\n\n\n\nhxejkaog-&gt;s3tdzgmu\n\n\n\n\n\nlijfw7my\n\n\n\n\nlijfw7my-&gt;4zj99lz2\n\n\n\n\n\neacs5e9j-&gt;k8yaaeoi\n\n\n\n\n\nur4yo3tx-&gt;hxejkaog\n\n\n\n\n\n1tdjps87\n\n\n\n\nur4yo3tx-&gt;1tdjps87\n\n\n\n\n\nl9axxrot\n\n\n\n\nur4yo3tx-&gt;l9axxrot\n\n\n\n\n\nfsp8cdjo-&gt;v4x5duoh\n\n\n\n\n\nfsp8cdjo-&gt;fovglxww\n\n\n\n\n\n9xro7toh\n\n\n\n\n4pocwaxo\n\n\n\n\nakih91pi-&gt;4pocwaxo\n\n\n\n\n\nakih91pi-&gt;3ytehise\n\n\n\n\n\nv850hpzb-&gt;89w9dxkj\n\n\n\n\n\nv850hpzb-&gt;qvvjbgxm\n\n\n\n\n\nv850hpzb-&gt;e7injpqb\n\n\n\n\n\ne4qukjnm-&gt;u0upp5hj\n\n\n\n\n\n1bkcwtuf\n\n\n\n\n1bkcwtuf-&gt;c114j2tw\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;lijfw7my\n\n\n\n\n\ndvhx7p1e\n\n\n\n\n1bkcwtuf-&gt;dvhx7p1e\n\n\n\n\n\nu0upp5hj-&gt;hcqaqm1o\n\n\n\n\n\nu0upp5hj-&gt;wxhhelge\n\n\n\n\n\nu0upp5hj-&gt;c21k3g41\n\n\n\n\n\nj33a36gy\n\n\n\n\nj33a36gy-&gt;9ifdi52l\n\n\n\n\n\nj33a36gy-&gt;j812m8am\n\n\n\n\n\newthkxdz\n\n\n\n\nefvvduxt\n\n\n\n\nefvvduxt-&gt;onkctu6x\n\n\n\n\n\nefvvduxt-&gt;4pocwaxo\n\n\n\n\n\ntwne0skc\n\n\n\n\ntwne0skc-&gt;xpzdirk1\n\n\n\n\n\ntwne0skc-&gt;1bkcwtuf\n\n\n\n\n\n2cicgpkb-&gt;l9axxrot\n\n\n\n\n\nozdu79aw\n\n\n\n\nozdu79aw-&gt;hx0hp6dr\n\n\n\n\n\nozdu79aw-&gt;1tdjps87\n\n\n\n\n\nj812m8am-&gt;j33a36gy\n\n\n\n\n\nsj35t4ss-&gt;72a2d6xd\n\n\n\n\n\nsj35t4ss-&gt;53bgisfb\n\n\n\n\n\n1tdjps87-&gt;8bpimk72\n\n\n\n\n\n1tdjps87-&gt;8cgoonso\n\n\n\n\n\nqcsg1epp\n\n\n\n\nqcsg1epp-&gt;k8yaaeoi\n\n\n\n\n\nkjba91s3-&gt;53bgisfb\n\n\n\n\n\nmis1btmm-&gt;c114j2tw\n\n\n\n\n\nmis1btmm-&gt;efvvduxt\n\n\n\n\n\n35yrnabr-&gt;4zj99lz2\n\n\n\n\n\n35yrnabr-&gt;y0sap20o\n\n\n\n\n\n35yrnabr-&gt;dvhx7p1e\n\n\n\n\n\nwmnsxqhi\n\n\n\n\nwmnsxqhi-&gt;wxhhelge\n\n\n\n\n\ntyoit3iq\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;fovglxww\n\n\n\n\n\ntyoit3iq-&gt;1chkeldv\n\n\n\n\n\n8cgoonso-&gt;j33a36gy\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\nez7k5sb5\n\n\n\n\nez7k5sb5-&gt;72a2d6xd\n\n\n\n\n\nez7k5sb5-&gt;9ifdi52l\n\n\n\n\n\nieb7bdce-&gt;xh1xin78\n\n\n\n\n\n4pocwaxo-&gt;85hi4ca2\n\n\n\n\n\n4pocwaxo-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz\n\n\n\n\n4pocwaxo-&gt;qez5iiiz\n\n\n\n\n\nmfp9is99-&gt;wt9rtovp\n\n\n\n\n\nmfp9is99-&gt;1chkeldv\n\n\n\n\n\nyezs4jbg\n\n\n\n\nyezs4jbg-&gt;tyoit3iq\n\n\n\n\n\nyezs4jbg-&gt;w084j1ko\n\n\n\n\n\nqez5iiiz-&gt;onkctu6x\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\ns3okac0a-&gt;5cymetka\n\n\n\n\n\ns3okac0a-&gt;vr6qql5z\n\n\n\n\n\ns3okac0a-&gt;72a2d6xd\n\n\n\n\n\n1pk6uryz\n\n\n\n\n1pk6uryz-&gt;xh1xin78\n\n\n\n\n\n1pk6uryz-&gt;2m5wokx2\n\n\n\n\n\n1pk6uryz-&gt;v4x5duoh\n\n\n\n\n\n1pk6uryz-&gt;fovglxww\n\n\n\n\n\n1pk6uryz-&gt;iw0pfpmk\n\n\n\n\n\n1pk6uryz-&gt;1chkeldv\n\n\n\n\n\nhv8k4g84-&gt;89w9dxkj\n\n\n\n\n\n83ayee0w\n\n\n\n\n83ayee0w-&gt;y0sap20o\n\n\n\n\n\n83ayee0w-&gt;bav7hkue\n\n\n\n\n\n83ayee0w-&gt;lijfw7my\n\n\n\n\n\ndxppp23s-&gt;glv43ych\n\n\n\n\n\ndxppp23s-&gt;m0ggeeei\n\n\n\n\n\ndogjcfze-&gt;1chkeldv\n\n\n\n\n\ndogjcfze-&gt;dtmww06j\n\n\n\n\n\n53bgisfb-&gt;666n359b\n\n\n\n\n\n53bgisfb-&gt;72a2d6xd\n\n\n\n\n\n53bgisfb-&gt;9ifdi52l\n\n\n\n\n\n53bgisfb-&gt;xztqibcz\n\n\n\n\n\nb463lw81\n\n\n\n\n53bgisfb-&gt;b463lw81\n\n\n\n\n\ne7injpqb-&gt;bav7hkue\n\n\n\n\n\ndvhx7p1e-&gt;5cymetka\n\n\n\n\n\ndvhx7p1e-&gt;8bx5ks88\n\n\n\n\n\npveq6y3l-&gt;4s3xzocl\n\n\n\n\n\npveq6y3l-&gt;pveq6y3l\n\n\n\n\n\nm0ggeeei-&gt;4s3xzocl\n\n\n\n\n\nm0ggeeei-&gt;qez5iiiz\n\n\n\n\n\nl9axxrot-&gt;pzodwhlw\n\n\n\n\n\nl9axxrot-&gt;akih91pi\n\n\n\n\n\nl9axxrot-&gt;c21k3g41\n\n\n\n\n\nc21k3g41-&gt;c1sdjw8j\n\n\n\n\n\nc21k3g41-&gt;ozdu79aw\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nb463lw81-&gt;hcqaqm1o\n\n\n\n\n\nb463lw81-&gt;wxhhelge\n\n\n\n\n\nce1516j0-&gt;4s3xzocl\n\n\n\n\n\nce1516j0-&gt;83ayee0w\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nw084j1ko-&gt;onkctu6x\n\n\n\n\n\nw084j1ko-&gt;dxppp23s\n\n\n\n\n\ns3tdzgmu-&gt;vr6qql5z\n\n\n\n\n\ns3tdzgmu-&gt;ez7k5sb5\n\n\n\n\n\ns3tdzgmu-&gt;b463lw81\n\n\n\n\n\n\n\n\n\n\nA randomly generated graph for visual purposes only.\nSee Appendix for graph generator code",
    "crumbs": [
      "Home",
      "Project Introduction",
      "Project Aims and Scope"
    ]
  },
  {
    "objectID": "03-05-transform.html",
    "href": "03-05-transform.html",
    "title": "Transformation",
    "section": "",
    "text": "TRANSFORM picks up where EXTRACT finished by using the extracted csv files as the source.\ntransform\n\n\n\nsource_files\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nvalidate_data\n\nValidating Data\n\n\n\nsource_files-&gt;validate_data\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;validate_data\n\n\n\n\n\nclean_data\n\nCleaning Data\n\n\n\nconfig-&gt;clean_data\n\n\n\n\n\nadd_department\n\nAdding 'Department' to Nodes\n\n\n\nconfig-&gt;add_department\n\n\n\n\n\nanonymise_data\n\n𝗔𝗡𝗢𝗡𝗬𝗠𝗜𝗦𝗜𝗡𝗚\nPersonal Data\n\n\n\nconfig-&gt;anonymise_data\n\n\n\n\n\naugment_rooms\n\nAugmenting Rooms\nwith Archibus Data\n\n\n\nconfig-&gt;augment_rooms\n\n\n\n\n\ncreate_relationships\n\nCreating\nRelationship Tables\n\n\n\nconfig-&gt;create_relationships\n\n\n\n\n\nvalidate_data-&gt;clean_data\n\n\n\n\n\nclean_data-&gt;add_department\n\n\n\n\n\nadd_department-&gt;anonymise_data\n\n\n\n\n\nanonymise_data-&gt;augment_rooms\n\n\n\n\n\naugment_rooms-&gt;create_relationships\n\n\n\n\n\nprocessed_files\n\nProcessed CSV Files\n\n\n\ncreate_relationships-&gt;processed_files\nConfiguration allows the user to control which nodes and relationships are included and how they are processed. There are options to specify validation, cleaning, data linking, anonymisation and relationship details.\nIt is also possible to specify datatypes. Neo4j assumes string datatype unless it is well-formatted or pre-determined. Config allows the user to specify specific datatypes like dates, times, point, Boolean, etc.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#all-data",
    "href": "03-05-transform.html#all-data",
    "title": "Transformation",
    "section": "All data",
    "text": "All data\n\nValidation - basic validation of the data is performed. Validation is extensible and can be expanded, as requirements are identified.\nCleaned - basic cleaning of all data is performed by stripping empty space and removing non-printable characters, etc. using regex. The cleaning functionality is expandable.\n\nWith clean data, the transformation proper starts:",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#nodes-and-relationships",
    "href": "03-05-transform.html#nodes-and-relationships",
    "title": "Transformation",
    "section": "Nodes and relationships",
    "text": "Nodes and relationships\n\nAdd Organisational Unit - where appropriate, the University Organisational Unit (e.g. College, School, Department) is added to the node. This will be picked up as a property during load.\n\nData Augmentation - Room data is augmented with additional properties from the location master database, including latitude, longitude, square meterage, etc. Data augmentation is extensible.\nAnonymisation - Personal data is anonymised. An anonymisation function was developed to remove and replace any personally identifiable information (PII). The pipeline extracts minimal PII but this is safely anonymised. The functional also adds fake emails. See Appendix for additional details\nRelationships - Based on requirements in the configuration, relationships are extracted including optional relationship properties.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-06b-load.html",
    "href": "03-06b-load.html",
    "title": "Neo4j Load",
    "section": "",
    "text": "With accessible csv files, the final module of the ETL pipeline creates (or updates) nodes and relationships in the Neo4j instance.\n\n\n\n\n\n\n\nneo4j_load\n\n\n\ngdrive_files\n\nGoogle Drive Files\n(nodes, relationships folders)\n\n\n\ngdrive_api\n\nConnect to Google Drive API\n\n\n\ngdrive_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nneo4j_connection\n\nConnect to Neo4j\n(with credentials)\n\n\n\nkeyring-&gt;neo4j_connection\n\n\n\n\n\nlist_files\n\nList Files\n(from folders)\n\n\n\ngdrive_api-&gt;list_files\n\n\n\n\n\ndetermine_nodes\n\nDetermine Nodes\n(to create)\n\n\n\nlist_files-&gt;determine_nodes\n\n\n\n\n\ndetermine_relationships\n\nDetermine Relationships\n(to create)\n\n\n\nlist_files-&gt;determine_relationships\n\n\n\n\n\ncreate_schema\n\nCreate Schema\n(dynamic or custom)\n\n\n\nlist_files-&gt;create_schema\n\n\nOptionally Create\n\n\n\ncreate_nodes\n\nCreate Nodes\n\n\n\nneo4j_connection-&gt;create_nodes\n\n\n\n\n\ncreate_relationships\n\nCreate Relationships\n\n\n\nneo4j_connection-&gt;create_relationships\n\n\n\n\n\ndetermine_nodes-&gt;create_nodes\n\n\n\n\n\ndetermine_relationships-&gt;create_relationships\n\n\n\n\n\ncreate_schema-&gt;create_nodes\n\n\n\n\n\ncreate_schema-&gt;create_relationships\n\n\n\n\n\nset_node_properties\n\nSet Node Properties\n(with datatypes)\n\n\n\ncreate_nodes-&gt;set_node_properties\n\n\n\n\n\nset_relationship_properties\n\nSet Relationship Properties\n\n\n\ncreate_relationships-&gt;set_relationship_properties\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nset_node_properties-&gt;process_done\n\n\n\n\n\nset_relationship_properties-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nThere are two authentication requirements:\n\nGoogle Drive to get node and relationship files and data.\n\nNeo4j Aura instance is connected to with Keyring encrypted credentials.\n\nNodes and relationships are dynamically processed by using a file-pattern matching approach. However, this can be overridden within configuration, if needed.\nAlso in configuration is the option to create a database schema. There are three options:\n\nNo schema\nDynamic (default) - creates unique constraints based on nodes\nCustom - allows the user to specify specific constraints prior to loading.\n\nAt this point, the ETL loads data on a row-by-row basis, reading the public csv files. Columns become properties with data types cross-referenced from a data-mapping dictionary in the configuration.\nIf there have been no errors - we should have data in our Neo4j Aura instance!\n\n\n\nData successfully loaded for “MSc Artificial Intelligence (I400)”",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Neo4j Load"
    ]
  },
  {
    "objectID": "appendix-anonymise.html",
    "href": "appendix-anonymise.html",
    "title": "D: Anonymisation",
    "section": "",
    "text": "The following code snippet is shows how I anonymised personal data in a DataFrame using the Faker library.\nThe code generates fake names, emails, and IDs for staff or student data based on the unique IDs in the extract DataFrame. The anonymised data is then merged back with the original DataFrame, and the original columns are removed.\n\n\n\nPre-anonymisation Extract\n\n\n\n\n\n\nPost-anonymisation Extract\n\n\n \n\n\nClick to show code\nimport random\nimport hashlib\nfrom faker import Faker\nimport pandas as pd\n\n\ndef anonymise_data(df):\n    \"\"\"\n    anonymises cols in df by generating fake names, emails, and IDs.\n    \"\"\"\n    process_logger.info(\"Starting anonymisation\")\n    process_logger.info(f\"Columns in dataframe: {df.columns.tolist()}\")\n    \n    # staff or student data\n    if 'staffSplusID' in df.columns:\n        process_logger.info(\"Processing staff data\")\n        id_col = 'staffID'\n        prefix = 'staff'\n        columns_to_remove = ['staffFullName', 'staffLastName', 'staffForenames', 'staffID']\n    elif 'stuSplusID' in df.columns:\n        process_logger.info(\"Processing student data\")\n        id_col = 'studentID'\n        prefix = 'stu'\n        columns_to_remove = ['stuFullName', 'stuLastName', 'stuForenames', 'studentID']\n    else:\n        process_logger.error(\"Neither 'staffSplusID' nor 'stuSplusID' found in columns.\")\n        return df  # Return original dataframe if required columns are missing\n\n    # dictionary to store anonymised data\n    anon_data = {}\n    \n    # generate anonymised data for each unique ID\n    for unique_id in df[id_col].unique():\n        # create a seed based on the unique_id\n        seed = int(hashlib.md5(str(unique_id).encode()).hexdigest(), 16) & 0xFFFFFFFF\n        fake = Faker()\n        fake.seed_instance(seed)\n        random.seed(seed)\n\n        first_name = fake.first_name()\n        last_name = fake.last_name()\n        full_name = f\"{first_name} {last_name}\"\n        email = f\"{first_name.lower()}.{last_name.lower()}@fakemail.ac.uk\"\n        anon_id = f\"{prefix}-{random.randint(10000000, 99999999):08d}\"\n        \n        anon_data[unique_id] = {\n            f'{prefix}FirstName_anon': first_name,\n            f'{prefix}LastName_anon': last_name,\n            f'{prefix}FullName_anon': full_name,\n            f'{prefix}Email_anon': email,\n            f'{prefix}ID_anon': anon_id\n        }\n    \n    # create a new df with anonymised data\n    df_anon = pd.DataFrame.from_dict(anon_data, orient='index')\n    \n    # reset the index and rename it to match the original ID column\n    df_anon = df_anon.reset_index().rename(columns={'index': id_col})\n    \n    try:\n        # Merge anonymised data with the original DataFrame\n        df_result = pd.merge(df, df_anon, on=id_col)\n        \n        # Rmove columns that should be anonymised\n        columns_to_remove = [col for col in columns_to_remove if col in df_result.columns]\n        df_result = df_result.drop(columns=columns_to_remove)\n        \n        process_logger.info(\"Anonymisation completed successfully\")\n        return df_result\n\n    except Exception as e:\n        process_logger.error(f\"Error during anonymisation: {str(e)}\")\n        return df  # return original df if error",
    "crumbs": [
      "Home",
      "Appendices",
      "Anonymisation"
    ]
  },
  {
    "objectID": "appendix-random-graph.html",
    "href": "appendix-random-graph.html",
    "title": "A: Random Graph Generator",
    "section": "",
    "text": "The function below generates a random graph (dot file) using Graphviz.\nTo render, ensure that graphviz is installed or save to file and render within documents using Quarto or similar.\n\n\nClick to show code\nimport graphviz\nimport random\nimport string\nfrom collections import defaultdict\n\ndef generate_random_graph(num_nodes=50, num_edges=100, num_clusters=5, colors=None):\n    \"\"\"Generates a random Graphviz graph with clusters and random colours.\n\n    Args:\n        num_nodes: Number of nodes in the graph.\n        num_edges: Number of edges in the graph.\n        num_clusters: Number of clusters to create.\n        colors: List of colours to use for clusters (optional). If not provided, random colours will be used.\n    \"\"\"\n\n    dot = graphviz.Digraph(\"G\")\n    dot.attr(fontname=\"Helvetica,Arial,sans-serif\")\n    dot.attr(layout=\"neato\")\n    dot.attr(start=\"random\")\n    dot.attr(overlap=\"false\")\n    dot.attr(splines=\"true\")\n    dot.attr(size=\"8,8\")\n    #dot.attr(dpi=\"300\")\n\n    # nodes to clusters, random colours if not provided\n    cluster_assignments = {}\n    if colors is None:\n        colors = [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(num_clusters)] \n\n    for i in range(num_nodes):\n        cluster_assignments[i] = random.randint(0, num_clusters - 1)\n\n    # random node names, colouur assignment\n    nodes = []\n    for i in range(num_nodes):\n        node_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n        nodes.append(node_name)\n        cluster_id = cluster_assignments[i]\n        color = colors[cluster_id]\n        dot.node(node_name, label=\"\", shape=\"circle\", height=\"0.12\", width=\"0.12\", fontsize=\"1\", fillcolor=color, style=\"filled\")\n        \n\n    # random edges (with a higher probability of staying within clusters)\n    edges = []\n    for _ in range(num_edges):\n        src_cluster = random.randint(0, num_clusters - 1)\n        dst_cluster = src_cluster if random.random() &lt; 0.8 else random.randint(0, num_clusters - 1)  # 80% chance of staying in cluster\n        src_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == src_cluster])\n        dst_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == dst_cluster])\n        edges.append((src_node, dst_node))\n\n    #  edges to the graph\n    for edge in edges:\n        dot.edge(*edge)\n\n    return dot",
    "crumbs": [
      "Home",
      "Appendices",
      "Random Graph Generator"
    ]
  },
  {
    "objectID": "appendix-cypher4.html",
    "href": "appendix-cypher4.html",
    "title": "Q: Count Queries",
    "section": "",
    "text": "This page contains a selection of count queries used to explore the graph database. The queries are designed to provide insights into the data and relationships between nodes. They can be considered starter queries which can be amended depending on the requirements.\n\nCount all nodes - by label\nBelow are two queries returning the same results - counts of nodes by node label.\n// Count of nodes - row per node\n\nUNWIND [\"student\", \"staff\", \"room\", \"activity\"] AS label\nMATCH (n)\nWHERE label IN labels(n)\nRETURN label, count(n) AS count\n\n\n\nCount All Nodes\n\n\n// Count of nodes - single row\n\nMATCH (n:student)\nWITH count(n) AS studentCount\nMATCH (n:staff)\nWITH studentCount, count(n) AS lecturerCount\nMATCH (n:room)\nWITH studentCount, lecturerCount, count(n) AS roomCount\nMATCH (n:activity)\nRETURN studentCount, lecturerCount, roomCount, count(n) AS activityCount\n\n\n\nCount All Nodes\n\n\n\n\nCount all relationships - by type\nThe query below returns counts of relationships. We can see that there are a significant amount of (student)-[]-&gt;(activity) relationships due to how we structured activity in the graph - that is, a separate node for each instance.\n// Count of relationships\n\nMATCH ()-[r:ATTENDS]-&gt;()\nWITH count(r) AS attendsCount\nMATCH ()-[r:TEACHES]-&gt;()\nWITH attendsCount, count(r) AS teachesCount\nMATCH ()-[r:OCCUPIES]-&gt;() \nWITH attendsCount, teachesCount, count(r) AS occupiesCount\nMATCH ()-[r:BELONGS_TO]-&gt;()\nRETURN attendsCount, teachesCount, occupiesCount, count(r) AS belongsCount\n\n\n\nCount All Relationships\n\n\n\n\nActivity counts\nIn this graph, an activity is an instance of an activity, that is, a unique combination of name, date, start, end, location, staff. It means a lot of activities!\n// Count of activities\n\nMATCH (a:activity)\nRETURN count(a) AS totalActivities;\n// Count of activities on a day\n\nMATCH (a:activity)\nWHERE a.actDayName = \"Wednesday\"\nRETURN DISTINCT count(a) AS wednesdayActivities\n\nMATCH (a:activity)\nRETURN DISTINCT a.actDayName AS dayName, count(a) AS activityCount\n\n\n\n\n\n\nActivity counts by time\nThe query below connects to the graph via python and returns the result - that is, the number of activities which start at 17:00.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\n// Activity count by time (start)\n\nMATCH (a:activity)\nWHERE a.actStartTime = localtime(\"17:00:00\")\n//AND a.actDayName = \"Wednesday\"\nRETURN count(a) AS activitiesStartingAt5pm\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\nfor record in result:\n    print(record)\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000027FA1066C50&gt;\nRunning query...\n\n&lt;Record activitiesStartingAt5pm=172&gt;\n\n\n\n\nStaff activity count\nThis query returns the first 5 rows of the query which counts activities by member of staff.\n\n\nClick to show code\nfrom connect_to_neo4j_db import connect_to_neo4j\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\n# connect to Neo4j\ndriver = connect_to_neo4j()\n\n# session\nsession = driver.session()\n\n# run query\nquery = \"\"\"\n// Staff activity count\n\nMATCH (st:staff)-[r:TEACHES]-&gt;(a:activity)\nRETURN st.staffFullName_anon AS staffName, count(a) AS activityCount\nORDER BY activityCount DESC\n\"\"\"\nprint(\"Running query...\\n\")\nresult = session.run(query)\n\n# list to hold records\nrecords = []\nfor record in result:\n    records.append(record)\n\n# df of first 5 records\ndf = pd.DataFrame(records[:5], columns=[\"staffName\", \"activityCount\"])\n\n# print\nprint(df)\n\n# close the session and driver\nsession.close()\ndriver.close()\n\n\nConnecting to Neo4j database....\nConnected to Neo4j database successfully! Driver: &lt;neo4j._sync.driver.Neo4jDriver object at 0x0000027FA10CE6D0&gt;\nRunning query...\n\n        staffName  activityCount\n0  Debbie Nichols            145\n1  Marc Hernandez            127\n2    Eileen Allen            126\n3    Steven Perez            124\n4  Justin Alvarez            112",
    "crumbs": [
      "Home",
      "Appendices",
      "Cypher Code",
      "Count Queries"
    ]
  },
  {
    "objectID": "03-04-extract.html",
    "href": "03-04-extract.html",
    "title": "Extraction",
    "section": "",
    "text": "extract\n\n\n\nA\n\n\nSQL Database\n(students, staff, programmes,\nactivities, rooms, etc.)\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nA-&gt;keyring\n\n\nRequires\n\n\n\nB\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nextract\n\n𝗘𝗫𝗧𝗥𝗔𝗖𝗧\n\n\n\nextract-&gt;B\n\n\n\n\n\nkeyring-&gt;extract\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;extract\n\n\nSQL Scripts\n\n\n\nconfig-&gt;keyring\n\n\nCredentials\n\n\n\n\n\n\n\n\n\nExtract\n\n\nEXTRACT starts by securely connecting to the specified SQL database using encrypted credentials stored with keyring. The combination of configuration and SQL scripts determine which data will be extracted by filtering based on programme(s) of study and specifying which nodes, relationships and properties to extract. Additional options include specifying chunk size if extracting signficant amounts of data, for example.\nThe process performs basic validation at every step ensuring secure connection before running SQL SELECT statements and storing extracted data as local csv files.\n\nSQL example\nSELECT DISTINCT a.[Id] AS actSplusID,\n     CONCAT(a.[Id], '-', adt.[Week], '-', adt.[Day]) AS actGraphID,\n     a.[Name] AS actName,\n     a.[Description] AS actDescription,\n     a.[DepartmentId] AS actDeptSPlusID,\n     adt.[StartDateTime] AS actStartDateTime,\n     adt.[EndDateTime] AS actEndDateTime,\n     adt.[Week] AS actWeekNum,\n     adt.[Occurrence] AS actOccurrence,\n     a.[ModuleId] AS actModSplusID,\n     a.[ScheduledDay] AS actScheduledDay,\n     a.[StartDate] AS actFirstActivityDate,\n     a.[EndDate] AS actLastActivityDate,\n     a.[PlannedSize] AS actPlannedSize,\n     a.[RealSize] AS actRealSize,\n     a.[Duration] AS actDuration,\n     a.[DurationInMinutes] AS actDurationInMinutes,\n     a.[NumberOfOccurrences] AS actNumberOfOccurrences,\n     a.[WeekPattern] AS actWeekPattern,\n     a.[ActivityTypeId] AS actActivityTypeSplusID,\n     a.[WhenScheduled] AS actWhenScheduled,\n     a.[IsJtaParent],\n     a.[IsJtaChild],\n     a.[IsVariantParent],\n     a.[IsVariantChild]\nFROM ##TempActivity a\nINNER JOIN ##TempActivityDateTime adt ON a.[Id] = adt.[ActivityID];\n\n\nSnippet: extract_data.py\n\n\nClick to show code\n# extract_main.py\nfrom logger_config import extract_logger\nfrom extract_data import main as extract_main\nfrom config import EXTRACT_DIR, HOSTKEYS, CHUNK_SIZE\nfrom utils import execution_times\n\ndef run_extraction():\n    extract_logger.info(\"Starting data extraction process\")\n    extract_logger.info(f\"Output Directory: {EXTRACT_DIR}\")\n    extract_logger.info(f\"Hostkeys: {HOSTKEYS}\")\n    extract_logger.info(f\"Chunksize: {CHUNK_SIZE}\")\n\n    try:\n        extract_main()\n    except Exception as e:\n        extract_logger.exception(\"An error occurred during data extraction:\")\n    finally:\n        extract_logger.info(\"Data extraction completed.\")\n\n   \n    # Log the execution times\n    extract_logger.info(\"Extraction Time Summary:\")\n    for func_name, exec_time in execution_times.items():\n        extract_logger.info(f\"Function {func_name} took {exec_time:.2f} seconds\")\n\n\nif __name__ == \"__main__\":\n    run_extraction()",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Extract"
    ]
  }
]