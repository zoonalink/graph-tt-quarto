[
  {
    "objectID": "06-conclusion.html",
    "href": "06-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations"
  },
  {
    "objectID": "06-conclusion.html#conclusion-500-words",
    "href": "06-conclusion.html#conclusion-500-words",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations"
  },
  {
    "objectID": "04-timetable-metrics.html",
    "href": "04-timetable-metrics.html",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts"
  },
  {
    "objectID": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "href": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts"
  },
  {
    "objectID": "02-data-engineering.html",
    "href": "02-data-engineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "High-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling\n\n\n\n\n\nBrief overview of SQL extraction techniques\n\n\n\n\n\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nChallenges and solutions with Neo4j Aura\nCloud vs.¬†desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nPotential future enhancements, developments"
  },
  {
    "objectID": "02-data-engineering.html#data-engineering-an-end-to-end-solution-1500-2000-words",
    "href": "02-data-engineering.html#data-engineering-an-end-to-end-solution-1500-2000-words",
    "title": "Data Engineering",
    "section": "",
    "text": "High-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling\n\n\n\n\n\nBrief overview of SQL extraction techniques\n\n\n\n\n\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nChallenges and solutions with Neo4j Aura\nCloud vs.¬†desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nPotential future enhancements, developments"
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Background on university timetabling challenges\nMotivation for exploring graph-based approaches\nProject scope and objectives",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "01-introduction.html#intro-500-words",
    "href": "01-introduction.html#intro-500-words",
    "title": "Introduction",
    "section": "",
    "text": "Background on university timetabling challenges\nMotivation for exploring graph-based approaches\nProject scope and objectives",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "03-graph-data-model.html",
    "href": "03-graph-data-model.html",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "03-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "href": "03-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "05-future-opportunities.html",
    "href": "05-future-opportunities.html",
    "title": "Future Opportunities",
    "section": "",
    "text": "Discussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources"
  },
  {
    "objectID": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "href": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "title": "Future Opportunities",
    "section": "",
    "text": "Discussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Graph Data Models for Timetabling Insights",
    "section": "",
    "text": "Supervisor: Xiaodong Li\nProgramme: MSc Data Science\n\n\n\n\n\n\n\nG\n\n\n\n4zj99lz2\n\n\n\n\n666n359b\n\n\n\n\nu0upp5hj\n\n\n\n\n666n359b-&gt;u0upp5hj\n\n\n\n\n\n4s3xzocl\n\n\n\n\nk8yaaeoi\n\n\n\n\n4s3xzocl-&gt;k8yaaeoi\n\n\n\n\n\n9bbhq0e8\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n5cymetka\n\n\n\n\ne4qukjnm\n\n\n\n\n5cymetka-&gt;e4qukjnm\n\n\n\n\n\nhx0hp6dr\n\n\n\n\nhx0hp6dr-&gt;hx0hp6dr\n\n\n\n\n\nk8yaaeoi-&gt;k8yaaeoi\n\n\n\n\n\n14ae1pfz\n\n\n\n\nk8yaaeoi-&gt;14ae1pfz\n\n\n\n\n\njrtct42d\n\n\n\n\nglv43ych\n\n\n\n\nxpzdirk1\n\n\n\n\nglv43ych-&gt;xpzdirk1\n\n\n\n\n\n06x4bmzo\n\n\n\n\nglv43ych-&gt;06x4bmzo\n\n\n\n\n\nmis1btmm\n\n\n\n\nglv43ych-&gt;mis1btmm\n\n\n\n\n\nce1516j0\n\n\n\n\nglv43ych-&gt;ce1516j0\n\n\n\n\n\ngzxrlpod\n\n\n\n\nc114j2tw\n\n\n\n\ngzxrlpod-&gt;c114j2tw\n\n\n\n\n\n8bpimk72\n\n\n\n\n8bpimk72-&gt;k8yaaeoi\n\n\n\n\n\n8bpimk72-&gt;14ae1pfz\n\n\n\n\n\nur4yo3tx\n\n\n\n\n8bpimk72-&gt;ur4yo3tx\n\n\n\n\n\nonkctu6x\n\n\n\n\nonkctu6x-&gt;onkctu6x\n\n\n\n\n\n85hi4ca2\n\n\n\n\nonkctu6x-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep\n\n\n\n\nonkctu6x-&gt;x0n5twep\n\n\n\n\n\nc1sdjw8j\n\n\n\n\nc1sdjw8j-&gt;hx0hp6dr\n\n\n\n\n\nc1sdjw8j-&gt;u0upp5hj\n\n\n\n\n\nxh1xin78\n\n\n\n\nxztqibcz\n\n\n\n\nxh1xin78-&gt;xztqibcz\n\n\n\n\n\nvr6qql5z\n\n\n\n\nvr6qql5z-&gt;666n359b\n\n\n\n\n\nwxhhelge\n\n\n\n\nvr6qql5z-&gt;wxhhelge\n\n\n\n\n\n3ytehise\n\n\n\n\nvr6qql5z-&gt;3ytehise\n\n\n\n\n\n2m5wokx2\n\n\n\n\nmfp9is99\n\n\n\n\n2m5wokx2-&gt;mfp9is99\n\n\n\n\n\nv4x5duoh\n\n\n\n\nv4x5duoh-&gt;2m5wokx2\n\n\n\n\n\nfovglxww\n\n\n\n\nv4x5duoh-&gt;fovglxww\n\n\n\n\n\nv4x5duoh-&gt;xztqibcz\n\n\n\n\n\n89w9dxkj\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n1chkeldv\n\n\n\n\n89w9dxkj-&gt;1chkeldv\n\n\n\n\n\n89w9dxkj-&gt;mis1btmm\n\n\n\n\n\nxpzdirk1-&gt;gzxrlpod\n\n\n\n\n\nfovglxww-&gt;fovglxww\n\n\n\n\n\nfsp8cdjo\n\n\n\n\nfovglxww-&gt;fsp8cdjo\n\n\n\n\n\nfovglxww-&gt;mfp9is99\n\n\n\n\n\njnazmo3s\n\n\n\n\njnazmo3s-&gt;e4qukjnm\n\n\n\n\n\n72a2d6xd\n\n\n\n\nsj35t4ss\n\n\n\n\n72a2d6xd-&gt;sj35t4ss\n\n\n\n\n\nlfwuczry\n\n\n\n\n35yrnabr\n\n\n\n\nlfwuczry-&gt;35yrnabr\n\n\n\n\n\n8cgoonso\n\n\n\n\nlfwuczry-&gt;8cgoonso\n\n\n\n\n\nhv8k4g84\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nhcqaqm1o\n\n\n\n\nhcqaqm1o-&gt;666n359b\n\n\n\n\n\nqy17mwag\n\n\n\n\nhcqaqm1o-&gt;qy17mwag\n\n\n\n\n\n53bgisfb\n\n\n\n\nhcqaqm1o-&gt;53bgisfb\n\n\n\n\n\n1k9klz8v\n\n\n\n\neacs5e9j\n\n\n\n\n1k9klz8v-&gt;eacs5e9j\n\n\n\n\n\nj812m8am\n\n\n\n\n1k9klz8v-&gt;j812m8am\n\n\n\n\n\nieb7bdce\n\n\n\n\n1k9klz8v-&gt;ieb7bdce\n\n\n\n\n\nbav7hkue\n\n\n\n\nc114j2tw-&gt;bav7hkue\n\n\n\n\n\ne7injpqb\n\n\n\n\nc114j2tw-&gt;e7injpqb\n\n\n\n\n\nhj4rycy9\n\n\n\n\ns3okac0a\n\n\n\n\nhj4rycy9-&gt;s3okac0a\n\n\n\n\n\n85hi4ca2-&gt;hj4rycy9\n\n\n\n\n\n85hi4ca2-&gt;9bbhq0e8\n\n\n\n\n\nakih91pi\n\n\n\n\n85hi4ca2-&gt;akih91pi\n\n\n\n\n\nwxhhelge-&gt;72a2d6xd\n\n\n\n\n\nwxhhelge-&gt;53bgisfb\n\n\n\n\n\niw0pfpmk\n\n\n\n\nwt9rtovp\n\n\n\n\niw0pfpmk-&gt;wt9rtovp\n\n\n\n\n\ndtmww06j\n\n\n\n\niw0pfpmk-&gt;dtmww06j\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nw084j1ko\n\n\n\n\nx0n5twep-&gt;w084j1ko\n\n\n\n\n\ny0sap20o\n\n\n\n\ny0sap20o-&gt;y0sap20o\n\n\n\n\n\ndxppp23s\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\npveq6y3l\n\n\n\n\ny0sap20o-&gt;pveq6y3l\n\n\n\n\n\nzefyvzdp\n\n\n\n\npzodwhlw\n\n\n\n\nhxejkaog\n\n\n\n\npzodwhlw-&gt;hxejkaog\n\n\n\n\n\n9bbhq0e8-&gt;dxppp23s\n\n\n\n\n\nm0ggeeei\n\n\n\n\n9bbhq0e8-&gt;m0ggeeei\n\n\n\n\n\n06x4bmzo-&gt;c114j2tw\n\n\n\n\n\nv850hpzb\n\n\n\n\n06x4bmzo-&gt;v850hpzb\n\n\n\n\n\n8bx5ks88\n\n\n\n\n06x4bmzo-&gt;8bx5ks88\n\n\n\n\n\nzyz74rqy\n\n\n\n\nzyz74rqy-&gt;hx0hp6dr\n\n\n\n\n\nzyz74rqy-&gt;c1sdjw8j\n\n\n\n\n\n2cicgpkb\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nc21k3g41\n\n\n\n\nzyz74rqy-&gt;c21k3g41\n\n\n\n\n\n6e82t0az\n\n\n\n\nsi800de6\n\n\n\n\nsi800de6-&gt;fovglxww\n\n\n\n\n\nsi800de6-&gt;si800de6\n\n\n\n\n\n4rv1fae9\n\n\n\n\n4rv1fae9-&gt;v4x5duoh\n\n\n\n\n\ndogjcfze\n\n\n\n\n4rv1fae9-&gt;dogjcfze\n\n\n\n\n\n4rv1fae9-&gt;dtmww06j\n\n\n\n\n\n4rv1fae9-&gt;3ytehise\n\n\n\n\n\nqvvjbgxm\n\n\n\n\nqy17mwag-&gt;vr6qql5z\n\n\n\n\n\nkjba91s3\n\n\n\n\nqy17mwag-&gt;kjba91s3\n\n\n\n\n\n9ifdi52l\n\n\n\n\ns3tdzgmu\n\n\n\n\n9ifdi52l-&gt;s3tdzgmu\n\n\n\n\n\n14ae1pfz-&gt;eacs5e9j\n\n\n\n\n\nhxejkaog-&gt;s3tdzgmu\n\n\n\n\n\nlijfw7my\n\n\n\n\nlijfw7my-&gt;4zj99lz2\n\n\n\n\n\neacs5e9j-&gt;k8yaaeoi\n\n\n\n\n\nur4yo3tx-&gt;hxejkaog\n\n\n\n\n\n1tdjps87\n\n\n\n\nur4yo3tx-&gt;1tdjps87\n\n\n\n\n\nl9axxrot\n\n\n\n\nur4yo3tx-&gt;l9axxrot\n\n\n\n\n\nfsp8cdjo-&gt;v4x5duoh\n\n\n\n\n\nfsp8cdjo-&gt;fovglxww\n\n\n\n\n\n9xro7toh\n\n\n\n\n4pocwaxo\n\n\n\n\nakih91pi-&gt;4pocwaxo\n\n\n\n\n\nakih91pi-&gt;3ytehise\n\n\n\n\n\nv850hpzb-&gt;89w9dxkj\n\n\n\n\n\nv850hpzb-&gt;qvvjbgxm\n\n\n\n\n\nv850hpzb-&gt;e7injpqb\n\n\n\n\n\ne4qukjnm-&gt;u0upp5hj\n\n\n\n\n\n1bkcwtuf\n\n\n\n\n1bkcwtuf-&gt;c114j2tw\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;lijfw7my\n\n\n\n\n\ndvhx7p1e\n\n\n\n\n1bkcwtuf-&gt;dvhx7p1e\n\n\n\n\n\nu0upp5hj-&gt;hcqaqm1o\n\n\n\n\n\nu0upp5hj-&gt;wxhhelge\n\n\n\n\n\nu0upp5hj-&gt;c21k3g41\n\n\n\n\n\nj33a36gy\n\n\n\n\nj33a36gy-&gt;9ifdi52l\n\n\n\n\n\nj33a36gy-&gt;j812m8am\n\n\n\n\n\newthkxdz\n\n\n\n\nefvvduxt\n\n\n\n\nefvvduxt-&gt;onkctu6x\n\n\n\n\n\nefvvduxt-&gt;4pocwaxo\n\n\n\n\n\ntwne0skc\n\n\n\n\ntwne0skc-&gt;xpzdirk1\n\n\n\n\n\ntwne0skc-&gt;1bkcwtuf\n\n\n\n\n\n2cicgpkb-&gt;l9axxrot\n\n\n\n\n\nozdu79aw\n\n\n\n\nozdu79aw-&gt;hx0hp6dr\n\n\n\n\n\nozdu79aw-&gt;1tdjps87\n\n\n\n\n\nj812m8am-&gt;j33a36gy\n\n\n\n\n\nsj35t4ss-&gt;72a2d6xd\n\n\n\n\n\nsj35t4ss-&gt;53bgisfb\n\n\n\n\n\n1tdjps87-&gt;8bpimk72\n\n\n\n\n\n1tdjps87-&gt;8cgoonso\n\n\n\n\n\nqcsg1epp\n\n\n\n\nqcsg1epp-&gt;k8yaaeoi\n\n\n\n\n\nkjba91s3-&gt;53bgisfb\n\n\n\n\n\nmis1btmm-&gt;c114j2tw\n\n\n\n\n\nmis1btmm-&gt;efvvduxt\n\n\n\n\n\n35yrnabr-&gt;4zj99lz2\n\n\n\n\n\n35yrnabr-&gt;y0sap20o\n\n\n\n\n\n35yrnabr-&gt;dvhx7p1e\n\n\n\n\n\nwmnsxqhi\n\n\n\n\nwmnsxqhi-&gt;wxhhelge\n\n\n\n\n\ntyoit3iq\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;fovglxww\n\n\n\n\n\ntyoit3iq-&gt;1chkeldv\n\n\n\n\n\n8cgoonso-&gt;j33a36gy\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\nez7k5sb5\n\n\n\n\nez7k5sb5-&gt;72a2d6xd\n\n\n\n\n\nez7k5sb5-&gt;9ifdi52l\n\n\n\n\n\nieb7bdce-&gt;xh1xin78\n\n\n\n\n\n4pocwaxo-&gt;85hi4ca2\n\n\n\n\n\n4pocwaxo-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz\n\n\n\n\n4pocwaxo-&gt;qez5iiiz\n\n\n\n\n\nmfp9is99-&gt;wt9rtovp\n\n\n\n\n\nmfp9is99-&gt;1chkeldv\n\n\n\n\n\nyezs4jbg\n\n\n\n\nyezs4jbg-&gt;tyoit3iq\n\n\n\n\n\nyezs4jbg-&gt;w084j1ko\n\n\n\n\n\nqez5iiiz-&gt;onkctu6x\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\ns3okac0a-&gt;5cymetka\n\n\n\n\n\ns3okac0a-&gt;vr6qql5z\n\n\n\n\n\ns3okac0a-&gt;72a2d6xd\n\n\n\n\n\n1pk6uryz\n\n\n\n\n1pk6uryz-&gt;xh1xin78\n\n\n\n\n\n1pk6uryz-&gt;2m5wokx2\n\n\n\n\n\n1pk6uryz-&gt;v4x5duoh\n\n\n\n\n\n1pk6uryz-&gt;fovglxww\n\n\n\n\n\n1pk6uryz-&gt;iw0pfpmk\n\n\n\n\n\n1pk6uryz-&gt;1chkeldv\n\n\n\n\n\nhv8k4g84-&gt;89w9dxkj\n\n\n\n\n\n83ayee0w\n\n\n\n\n83ayee0w-&gt;y0sap20o\n\n\n\n\n\n83ayee0w-&gt;bav7hkue\n\n\n\n\n\n83ayee0w-&gt;lijfw7my\n\n\n\n\n\ndxppp23s-&gt;glv43ych\n\n\n\n\n\ndxppp23s-&gt;m0ggeeei\n\n\n\n\n\ndogjcfze-&gt;1chkeldv\n\n\n\n\n\ndogjcfze-&gt;dtmww06j\n\n\n\n\n\n53bgisfb-&gt;666n359b\n\n\n\n\n\n53bgisfb-&gt;72a2d6xd\n\n\n\n\n\n53bgisfb-&gt;9ifdi52l\n\n\n\n\n\n53bgisfb-&gt;xztqibcz\n\n\n\n\n\nb463lw81\n\n\n\n\n53bgisfb-&gt;b463lw81\n\n\n\n\n\ne7injpqb-&gt;bav7hkue\n\n\n\n\n\ndvhx7p1e-&gt;5cymetka\n\n\n\n\n\ndvhx7p1e-&gt;8bx5ks88\n\n\n\n\n\npveq6y3l-&gt;4s3xzocl\n\n\n\n\n\npveq6y3l-&gt;pveq6y3l\n\n\n\n\n\nm0ggeeei-&gt;4s3xzocl\n\n\n\n\n\nm0ggeeei-&gt;qez5iiiz\n\n\n\n\n\nl9axxrot-&gt;pzodwhlw\n\n\n\n\n\nl9axxrot-&gt;akih91pi\n\n\n\n\n\nl9axxrot-&gt;c21k3g41\n\n\n\n\n\nc21k3g41-&gt;c1sdjw8j\n\n\n\n\n\nc21k3g41-&gt;ozdu79aw\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nb463lw81-&gt;hcqaqm1o\n\n\n\n\n\nb463lw81-&gt;wxhhelge\n\n\n\n\n\nce1516j0-&gt;4s3xzocl\n\n\n\n\n\nce1516j0-&gt;83ayee0w\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nw084j1ko-&gt;onkctu6x\n\n\n\n\n\nw084j1ko-&gt;dxppp23s\n\n\n\n\n\ns3tdzgmu-&gt;vr6qql5z\n\n\n\n\n\ns3tdzgmu-&gt;ez7k5sb5\n\n\n\n\n\ns3tdzgmu-&gt;b463lw81\n\n\n\n\n\n\n\n\n\n\nThis is a randomly generated graph for visual purposes only.\nSee Appendix for graph generator code",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "03-data-engineering.html",
    "href": "03-data-engineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "TODO\n\n\n\n\nlook for update notes\nfix yml so that the pages are grouped:\n\nOverview\nApproach\nETL\nReflection\n\nRewrite 3.3.2 - into paragraphs and reference ETL yaml in appendix"
  },
  {
    "objectID": "03-data-engineering.html#overview-of-the-data-pipeline",
    "href": "03-data-engineering.html#overview-of-the-data-pipeline",
    "title": "Data Engineering",
    "section": "3.1 Overview of the Data Pipeline",
    "text": "3.1 Overview of the Data Pipeline\nThe data engineering pipeline is designed to efficiently and securely transfer selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j), enabling advanced analytics and insights.\nThis section provides an overview of the pipeline architecture, key design principles, and implementation approach.\n\n3.1.1 High-level Architecture\nThe data pipeline consists of several key stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: The CSV files are processed, cleaned, transformed, merged, and anonymised using Python code.\nIntermediate Storage: Processed CSVs are saved locally and uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. ùóòùó´ùóßùó•ùóîùóñùóß: filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. ùóßùó•ùóîùó°ùó¶ùóôùó¢ùó•ùó†: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. ùêîùêèùêãùêéùêÄùêÉ\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. ùóüùó¢ùóîùóó: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure¬†1\n\n\n\n\n\n3.1.2 Key Design Principles\nThis pipeline represents a comprehensive approach to data engineering, incorporating several best practices in data handling, processing, and database management.\n\n\n\nDesign Principles\n\n\nThe data pipeline is built on several core design principles. I started with a strong sense of what I wanted to achieve - a modular, scalable, secure and configurable design - however, what exactly this meant was discovered in the development process. Given that the project has several limitations including resources, technology and is also time-constrained, it was important to make the final result one which can be built upon and potentially be developed within operational contexts. However, the project is also a proof-of-concept and as such, some design opportunities were eschewed in favour of simplicity and progress.\n\nSecurity and Data Protection\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\nModularity, Scalability and Automation\n\n\nDistinct, interoperable modules (extract, transform, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\nError Handling and Logging\n\n\nRobust error handling mechanisms\nComprehensive logging for troubleshooting and auditing\n\n\n\nUser configurable\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling\n\n\n\n\n3.1.3 Implementation Approach\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nKey aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. See Appendix for more details.\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that componentsare behaving as expected.\n\n\n\n3.1.4 Upcoming Sections\nThe following sections will delve into the specific implementation details of each stage in the pipeline, demonstrating how these principles are put into practice.\nI will explore the iterative development process, configuration management, extraction techniques, transformation processes, loading strategies, and automation workflows. Finally, I will reflect on lessons learned and potential future enhancements to the data engineering components."
  },
  {
    "objectID": "02-graph-data-model.html",
    "href": "02-graph-data-model.html",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)",
    "crumbs": [
      "Home",
      "Graph Data Model"
    ]
  },
  {
    "objectID": "02-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "href": "02-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)",
    "crumbs": [
      "Home",
      "Graph Data Model"
    ]
  },
  {
    "objectID": "03-data-engineering.html#notes-to-delete",
    "href": "03-data-engineering.html#notes-to-delete",
    "title": "Data Engineering",
    "section": "NOTES TO DELETE",
    "text": "NOTES TO DELETE\n\nHigh-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling"
  },
  {
    "objectID": "03-data-engineering.html#iterative-development-approach",
    "href": "03-data-engineering.html#iterative-development-approach",
    "title": "Data Engineering",
    "section": "3.2 Iterative Development Approach",
    "text": "3.2 Iterative Development Approach\nI followed an interative, agile-inspired approach when developing the data pipeline, despite being a team of one. This allowed for flexibility, continuous improvement and the opportunity to adapt to new insights during the process. The bulk of my effort was spent prototyping, testing and reviewing with each iteration resulting in a new challenge, issue, opportunity or occasionally, success.\n\n\n\nIterative Development Approach\n\n\n\n3.2.1 Initial Planning and Requirements Gathering\nThe development cycle began with initial high-level planning and requirements gathering, where I imagined how each stage should work, trying to bear in mind future-proofing and repeatability principles.\nI defined core functionality for each module (extraction, transformation, loading) adn outlined initial technical requirements and constraints. The planning documentation was maintained in Quarto and markdown files in a centralised repository for project information.\n\n\n3.2.2 Prototyping\nFollowing the initial planning, rapid prototyping was undertaken for each module:\n\nSQL prototyping for data extraction queries\nPython prototyping for data transformation and processing logic\nNeo4j prototyping for graph database schema and loading procedures\n\nThis stage allowed for quick exploration of different approaches and early identification of potential challenges as well as giving me the confidence to continue with my exploration.\n\n\n3.2.3 Component-Based Development and Testing\nDevelopment proceeded with a focus on individual components:\n\nEach module (extraction, transformation, loading) was developed separately\nAn iterative, component-based testing approach was employed\nWhile formal unit tests were not always created, each component was thoroughly tested for functionality\n\nThis approach allowed for rapid progress while maintaining a focus on component-level quality. It was during this phase that I started expanding configuration, logging and error-handling options.\n\n\n3.2.4 Integration and Review\nAs components reached a stable state, they were integrated and reviewed:\n\nComponents were combined to form larger functional units\nIntegrated functionality was occasioanlly demonstrated to subject matter experts (operational timetablers)\nFeedback was gathered on functionality, usability, and alignment with requirements\n\n\n\n3.2.5 Feedback Integration and Iteration\nInsights gained from reviews and ongoing development were fed back into the process:\n\nNew requirements or modifications were documented, for example updates to SQL SELECT statements and data model interpretations.\nThe planning stage was revisited to incorporate new information.\nThe development cycle was repeated, focusing on areas needing improvement or new functionality.\nDecisions were made to park some development opportunities for the future.\n\n\n\n3.2.6 Version Validation and Documentation\nAt key milestones, when a stable version was achieved:\n\nEnd-to-end validation of the entire pipeline was performed.\nResults were documented in notebooks, including opportunities for improvement.\nAny issues identified were logged for the next iteration.\n\n\n\n3.2.7 Continuous Learning and Adaptation\nThroughout the development process, learning and adaptation became central to the project‚Äôs evolution. Each iteration brought new insights, often through trial and error. Early challenges included the need to modularise components before they became unmanageable, resisting the temptation to make overly ambitious changes, and recognising when refactoring was necessary. These experiences underscored the importance of incremental progress and consistent testing in maintaining project stability and direction.\nThis iterative journey was far from linear. There were many moments of frustration, periods of painstaking troubleshooting, and the constant urge to overdeliver, sometimes exceeding the original proof-of-concept scope. Yet, with each stumble and course correction, the process itself became more refined, transforming into a powerful tool for identifying and resolving issues.\nWhile the core MVP (minimum viable product) requirements remained relatively stable (I set them after all), the iterative approach empowered me to seize opportunities for enhancement. Each chance to modularise, parameterise, or fine-tune sparked an almost compulsive drive for improvement, pushing the pipeline beyond its initial scope. This dedication to continuous refinement, while time-consuming, ultimately fostered a robust, flexible solution that could adapt gracefully to unforeseen challenges and embrace future opportunities.\nIt also meant the the proportion of overall emphasis shifted slightly from exploring Neo4j insights towards the development of a comprenhensive data engineering solution - entirely by virtue of where I spent my time.\nThe iterative approach proved to be more than just a development methodology. It facilitated personal growth, enhanced technical skills, and improved project management capabilities. This process emphasized the importance of persistence, learning from mistakes, and continual improvement in creating an effective data engineering solution. While challenging at times, this approach ultimately led to a more robust and flexible final product."
  },
  {
    "objectID": "03-data-engineering.html#configuration-and-logging",
    "href": "03-data-engineering.html#configuration-and-logging",
    "title": "Data Engineering",
    "section": "3.3 Configuration and Logging",
    "text": "3.3 Configuration and Logging\n\n3.3.1 Approach\nThe configuration and logging approach has been based on centralising configuration parameters into a python script and it has been organised into sections to manage the different aspects of the ETL pipeline:\n[CHANGE IF YAML BASED APPROACHED FINISH, ELSE UPDATE FUTURE DEV]\n\n\n3.3.2 Main Configuration options\nThe configuration for the ETL pipeline is designed to be flexible and customizable through a YAML file (etl_config.yaml). This configuration file allows users to specify a range of options to tailor the pipeline to their specific needs.\nGeneral settings include a list of program codes (hostkeys) used for filtering data and naming folders, as well as a customizable folder name for organization. File paths and directories can be configured to point to the locations of source data files, Google Drive folders for storing processed data, and Google service account credentials. Data processing parameters include the chunk size for SQL extraction and templates for output file names.\nThe configuration also provides options for customizing the schema of the Neo4j database and setting the batch size for data loading. For each node and relationship type, detailed configuration options are available, such as file patterns, column mappings, and data types. Additionally, users can customize the handling and display of specific data types through mappings. Finally, separate loggers for different stages of the ETL process and a configurable log level provide detailed logging capabilities.\n\nGeneral\n\nhostkeys: list of programme codes for filtering at extract and naming folders\nfolder_name:name for organising directories; default = hostkeys\n\nFilepaths and Directories\n\nroot_dir: project root directory; default = current working directory\nnodes_folder_url, relationships_folder_url: Google Drive URLs to override dynamic data location, if needed\ngdrive_root_folder_url, gdrive_folder_name: Google Drive for storing processed data - shared root folder\ngoogle_credentials_path: Path to the Google service account credentials file.\ndepartment_source, archibus_source: Paths to source data files to augment extracted data.\n\nData Processing\n\nchunk_size: number of rows to process during SQL extraction\ntemp_table_sql_files: SQL script files\nnode_output_filename_template, rel_output_filename_template: Templates for output file names.\n\nneo4j\n\nSchema configuration options (dynamic or custom).\nbatch_size: Batch size for loading data into Neo4j.\n\nnodes, relationships:\n\nDetailed configuration for each node and relationship type, including file patterns, column mappings, data types, and more.\n\ndata_type_mapping, display_name_mapping\n\nMappings to customise how specific data types are handled and displayed.\n\nLogging\n\nSeparate loggers for:\n\nextract\nprocess\nload\ngdrive\n\nLog level configurable: Controls the verbosity of logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)\nCustom timeit function to log time elapsed\n\n\nSee Appendix for Configuration YAML"
  },
  {
    "objectID": "03-data-engineering.html#extraction-process",
    "href": "03-data-engineering.html#extraction-process",
    "title": "Data Engineering",
    "section": "3.4 Extraction Process",
    "text": "3.4 Extraction Process\n\nBrief overview of SQL extraction techniques\nData Sources\nCode snippet\nExtract (from SQL Database): SQL queries are executed to extract both node and relationship data, which are saved into separate CSV files.\n\nExtraction (from SQL Database): - The extraction process uses the provided SQL queries and connection details to retrieve data. - Data Validation: - Before Extraction: The configuration might include validation rules to check SQL query syntax or data source availability. - After Extraction: The extracted data is validated against rules defined in the configuration (e.g., checking for missing values, data type validation).\n\nnotes to include:\n\ninformation security\ncontrolled access\nsecure university databases, windows system user, servers\nanonymisation protocol - minimal personal information, still want operatinalisable tool\nreusability, maintainability, testability\nmore data -&gt; scale\nmore data -&gt; properties, relationships\nsql code changes\ngoogle drive apis\nerror handling\nconsistent logging\nvalidation data\n\n\n\n3.5 Transformation and Processing\n\nData cleaning and preproessing\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n3.4 Loading to Graph Database\n\nDatabase schema\nLoading process\nChallenges and solutions with Neo4j Aura\nCloud vs.¬†desktop considerations\n\n\n\n3.5 Automation and Workflow\n\nEnd-to-end automated process for specific programme data\n\n\n\n3.6 Refelction and Lessons Learned\n\nReflection on the agile approach and discoveries made during development\nBest practices discovered\nChallenges over come\nPotential future enhancements, developments"
  },
  {
    "objectID": "03-01-overview.html",
    "href": "03-01-overview.html",
    "title": "Data Engineering Overview",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "03-01-overview.html#overview-of-the-data-pipeline",
    "href": "03-01-overview.html#overview-of-the-data-pipeline",
    "title": "Data Engineering Overview",
    "section": "3.1 Overview of the Data Pipeline",
    "text": "3.1 Overview of the Data Pipeline\nThe data engineering pipeline is designed to efficiently and securely transfer selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j), enabling advanced analytics and insights.\nThis section provides an overview of the pipeline architecture, fundamental design principles, implementation approach and key learning takeaways.\n\nHigh-level Architecture\nThe data pipeline consists of these core stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: The CSV files are processed, cleaned, transformed, merged, and anonymised using Python code.\nIntermediate Storage: Processed CSVs are saved locally and uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. ùóòùó´ùóßùó•ùóîùóñùóß: filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. ùóßùó•ùóîùó°ùó¶ùóôùó¢ùó•ùó†: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. ùêîùêèùêãùêéùêÄùêÉ\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. ùóüùó¢ùóîùóó: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure¬†1\n\n\n\n\n\nDesign Principles\nThis pipeline represents a comprehensive approach to data engineering, incorporating several best practices in data handling, processing, and database management.\n\n\n\nDesign Principles\n\n\nThe data pipeline is built on several core design principles. I started with a strong sense of what I wanted to achieve - a modular, scalable, secure and configurable design - however, what exactly this meant was discovered during the development process.\nGiven that my project bound by time and word-limits and has additional resource and technology constraints, it was important to make the final artefact one which can be built upon after submission, including potential further development within operational contexts.\nHowever, the project is also a proof-of-concept and as such, some design opportunities were eschewed in favour of simplicity and progress.\n\n Security and Data Protection\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\nModularity, Scalability and Automation\n\n\nDistinct, interoperable modules (extract, transform, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\nError Handling and Logging\n\n\nRobust error handling mechanisms\nComprehensive logging for troubleshooting and auditing\n\n\n\nUser configurable\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling\n\n\n\n\n3.1.3 Implementation Approach\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nKey aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. See Appendix for more details.\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that componentsare behaving as expected.\n\n\n\n3.1.4 Upcoming Sections\nThe following sections will delve into the specific implementation details of each stage in the pipeline, demonstrating how these principles are put into practice.\nI will explore the iterative development process, configuration management, extraction techniques, transformation processes, loading strategies, and automation workflows. Finally, I will reflect on lessons learned and potential future enhancements to the data engineering components."
  },
  {
    "objectID": "03-06-load.html",
    "href": "03-06-load.html",
    "title": "Load",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "03-04-extract.html",
    "href": "03-04-extract.html",
    "title": "Extraction",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "03-04-extract.html#extraction-process",
    "href": "03-04-extract.html#extraction-process",
    "title": "Extraction",
    "section": "3.4 Extraction Process",
    "text": "3.4 Extraction Process\n\nBrief overview of SQL extraction techniques\nData Sources\nCode snippet\nExtract (from SQL Database): SQL queries are executed to extract both node and relationship data, which are saved into separate CSV files.\n\nExtraction (from SQL Database): - The extraction process uses the provided SQL queries and connection details to retrieve data. - Data Validation: - Before Extraction: The configuration might include validation rules to check SQL query syntax or data source availability. - After Extraction: The extracted data is validated against rules defined in the configuration (e.g., checking for missing values, data type validation)."
  },
  {
    "objectID": "03-02-approach.html",
    "href": "03-02-approach.html",
    "title": "Data Engineering Approach",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "03-02-approach.html#iterative-development-approach",
    "href": "03-02-approach.html#iterative-development-approach",
    "title": "Data Engineering Approach",
    "section": "3.2 Iterative Development Approach",
    "text": "3.2 Iterative Development Approach\nI followed an interative, agile-inspired approach when developing the data pipeline, despite being a team of one. This allowed for flexibility, continuous improvement and the opportunity to adapt to new insights during the process. The bulk of my effort was spent prototyping, testing and reviewing with each iteration resulting in a new challenge, issue, opportunity or occasionally, success.\n\n\n\nIterative Development Approach\n\n\n\n3.2.1 Initial Planning and Requirements Gathering\nThe development cycle began with initial high-level planning and requirements gathering, where I imagined how each stage should work, trying to bear in mind future-proofing and repeatability principles.\nI defined core functionality for each module (extraction, transformation, loading) adn outlined initial technical requirements and constraints. The planning documentation was maintained in Quarto and markdown files in a centralised repository for project information.\n\n\n3.2.2 Prototyping\nFollowing the initial planning, rapid prototyping was undertaken for each module:\n\nSQL prototyping for data extraction queries\nPython prototyping for data transformation and processing logic\nNeo4j prototyping for graph database schema and loading procedures\n\nThis stage allowed for quick exploration of different approaches and early identification of potential challenges as well as giving me the confidence to continue with my exploration.\n\n\n3.2.3 Component-Based Development and Testing\nDevelopment proceeded with a focus on individual components:\n\nEach module (extraction, transformation, loading) was developed separately\nAn iterative, component-based testing approach was employed\nWhile formal unit tests were not always created, each component was thoroughly tested for functionality\n\nThis approach allowed for rapid progress while maintaining a focus on component-level quality. It was during this phase that I started expanding configuration, logging and error-handling options.\n\n\n3.2.4 Integration and Review\nAs components reached a stable state, they were integrated and reviewed:\n\nComponents were combined to form larger functional units\nIntegrated functionality was occasioanlly demonstrated to subject matter experts (operational timetablers)\nFeedback was gathered on functionality, usability, and alignment with requirements\n\n\n\n3.2.5 Feedback Integration and Iteration\nInsights gained from reviews and ongoing development were fed back into the process:\n\nNew requirements or modifications were documented, for example updates to SQL SELECT statements and data model interpretations.\nThe planning stage was revisited to incorporate new information.\nThe development cycle was repeated, focusing on areas needing improvement or new functionality.\nDecisions were made to park some development opportunities for the future.\n\n\n\n3.2.6 Version Validation and Documentation\nAt key milestones, when a stable version was achieved:\n\nEnd-to-end validation of the entire pipeline was performed.\nResults were documented in notebooks, including opportunities for improvement.\nAny issues identified were logged for the next iteration.\n\n\n\n3.2.7 Continuous Learning and Adaptation\nThroughout the development process, learning and adaptation became central to the project‚Äôs evolution. Each iteration brought new insights, often through trial and error. Early challenges included the need to modularise components before they became unmanageable, resisting the temptation to make overly ambitious changes, and recognising when refactoring was necessary. These experiences underscored the importance of incremental progress and consistent testing in maintaining project stability and direction.\nThis iterative journey was far from linear. There were many moments of frustration, periods of painstaking troubleshooting, and the constant urge to overdeliver, sometimes exceeding the original proof-of-concept scope. Yet, with each stumble and course correction, the process itself became more refined, transforming into a powerful tool for identifying and resolving issues.\nWhile the core MVP (minimum viable product) requirements remained relatively stable (I set them after all), the iterative approach empowered me to seize opportunities for enhancement. Each chance to modularise, parameterise, or fine-tune sparked an almost compulsive drive for improvement, pushing the pipeline beyond its initial scope. This dedication to continuous refinement, while time-consuming, ultimately fostered a robust, flexible solution that could adapt gracefully to unforeseen challenges and embrace future opportunities.\nIt also meant the the proportion of overall emphasis shifted slightly from exploring Neo4j insights towards the development of a comprenhensive data engineering solution - entirely by virtue of where I spent my time.\nThe iterative approach proved to be more than just a development methodology. It facilitated personal growth, enhanced technical skills, and improved project management capabilities. This process emphasized the importance of persistence, learning from mistakes, and continual improvement in creating an effective data engineering solution. While challenging at times, this approach ultimately led to a more robust and flexible final product."
  },
  {
    "objectID": "03-03-config.html",
    "href": "03-03-config.html",
    "title": "Configuration and Logging",
    "section": "",
    "text": "TODO\n\n\n\n\nrewrite detailed options into a summary format\nupdate if YAML config implemented."
  },
  {
    "objectID": "03-03-config.html#configuration-and-logging",
    "href": "03-03-config.html#configuration-and-logging",
    "title": "Configuration and Logging",
    "section": "3.3 Configuration and Logging",
    "text": "3.3 Configuration and Logging\n\n3.3.1 Approach\nThe configuration and logging approach has been based on centralising configuration parameters into a python script and it has been organised into sections to manage the different aspects of the ETL pipeline.\n\n\n3.3.2 Main Configuration options\nThe configuration for the ETL pipeline is designed to be flexible and customizable through a YAML file (etl_config.yaml). This configuration file allows users to specify a range of options to tailor the pipeline to their specific needs.\nGeneral settings include a list of program codes (hostkeys) used for filtering data and naming folders, as well as a customizable folder name for organization. File paths and directories can be configured to point to the locations of source data files, Google Drive folders for storing processed data, and Google service account credentials. Data processing parameters include the chunk size for SQL extraction and templates for output file names.\nThe configuration also provides options for customizing the schema of the Neo4j database and setting the batch size for data loading. For each node and relationship type, detailed configuration options are available, such as file patterns, column mappings, and data types. Additionally, users can customize the handling and display of specific data types through mappings. Finally, separate loggers for different stages of the ETL process and a configurable log level provide detailed logging capabilities.\n\nGeneral\n\nhostkeys: list of programme codes for filtering at extract and naming folders\nfolder_name:name for organising directories; default = hostkeys\n\nFilepaths and Directories\n\nroot_dir: project root directory; default = current working directory\nnodes_folder_url, relationships_folder_url: Google Drive URLs to override dynamic data location, if needed\ngdrive_root_folder_url, gdrive_folder_name: Google Drive for storing processed data - shared root folder\ngoogle_credentials_path: Path to the Google service account credentials file.\ndepartment_source, archibus_source: Paths to source data files to augment extracted data.\n\nData Processing\n\nchunk_size: number of rows to process during SQL extraction\ntemp_table_sql_files: SQL script files\nnode_output_filename_template, rel_output_filename_template: Templates for output file names.\n\nneo4j\n\nSchema configuration options (dynamic or custom).\nbatch_size: Batch size for loading data into Neo4j.\n\nnodes, relationships:\n\nDetailed configuration for each node and relationship type, including file patterns, column mappings, data types, and more.\n\ndata_type_mapping, display_name_mapping\n\nMappings to customise how specific data types are handled and displayed.\n\nLogging\n\nSeparate loggers for:\n\nextract\nprocess\nload\ngdrive\n\nLog level configurable: Controls the verbosity of logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)\nCustom timeit function to log time elapsed\n\n\nSee Appendix for Configuration YAML"
  },
  {
    "objectID": "03-05-transform.html",
    "href": "03-05-transform.html",
    "title": "Transformation",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Github gists ??\nGithub repo?"
  },
  {
    "objectID": "appendix.html#technology-stack",
    "href": "appendix.html#technology-stack",
    "title": "Appendix",
    "section": "Technology Stack",
    "text": "Technology Stack\nTO ADD:\n\nTechnology Stack\n\nPython used\nPython packages used and why\nNeo4j used\nGoogle API\nVS code\nQuarto\nSQL"
  },
  {
    "objectID": "appendix.html#config",
    "href": "appendix.html#config",
    "title": "Appendix",
    "section": "Config",
    "text": "Config\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) Override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) Override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\""
  },
  {
    "objectID": "appendix.html#random-graph-generator",
    "href": "appendix.html#random-graph-generator",
    "title": "Appendix",
    "section": "Random Graph Generator",
    "text": "Random Graph Generator\nThe function below generates a random graph (dot file) using Graphviz.\nTo render, ensure that graphviz is installed or save to file and render within documents using Quarto or similar.\n\n\nClick to show code\nimport graphviz\nimport random\nimport string\nfrom collections import defaultdict\n\ndef generate_random_graph(num_nodes=50, num_edges=100, num_clusters=5, colors=None):\n    \"\"\"Generates a random Graphviz graph with clusters and random colours.\n\n    Args:\n        num_nodes: Number of nodes in the graph.\n        num_edges: Number of edges in the graph.\n        num_clusters: Number of clusters to create.\n        colors: List of colours to use for clusters (optional). If not provided, random colours will be used.\n    \"\"\"\n\n    dot = graphviz.Digraph(\"G\")\n    dot.attr(fontname=\"Helvetica,Arial,sans-serif\")\n    dot.attr(layout=\"neato\")\n    dot.attr(start=\"random\")\n    dot.attr(overlap=\"false\")\n    dot.attr(splines=\"true\")\n    dot.attr(size=\"8,8\")\n    #dot.attr(dpi=\"300\")\n\n    # nodes to clusters, random colours if not provided\n    cluster_assignments = {}\n    if colors is None:\n        colors = [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(num_clusters)] \n\n    for i in range(num_nodes):\n        cluster_assignments[i] = random.randint(0, num_clusters - 1)\n\n    # random node names, colouur assignment\n    nodes = []\n    for i in range(num_nodes):\n        node_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n        nodes.append(node_name)\n        cluster_id = cluster_assignments[i]\n        color = colors[cluster_id]\n        dot.node(node_name, label=\"\", shape=\"circle\", height=\"0.12\", width=\"0.12\", fontsize=\"1\", fillcolor=color, style=\"filled\")\n        \n\n    # random edges (with a higher probability of staying within clusters)\n    edges = []\n    for _ in range(num_edges):\n        src_cluster = random.randint(0, num_clusters - 1)\n        dst_cluster = src_cluster if random.random() &lt; 0.8 else random.randint(0, num_clusters - 1)  # 80% chance of staying in cluster\n        src_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == src_cluster])\n        dst_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == dst_cluster])\n        edges.append((src_node, dst_node))\n\n    #  edges to the graph\n    for edge in edges:\n        dot.edge(*edge)\n\n    return dot"
  },
  {
    "objectID": "appendix-random-graph.html",
    "href": "appendix-random-graph.html",
    "title": "Random Graph Generator",
    "section": "",
    "text": "The function below generates a random graph (dot file) using Graphviz.\nTo render, ensure that graphviz is installed or save to file and render within documents using Quarto or similar.\n\n\nClick to show code\nimport graphviz\nimport random\nimport string\nfrom collections import defaultdict\n\ndef generate_random_graph(num_nodes=50, num_edges=100, num_clusters=5, colors=None):\n    \"\"\"Generates a random Graphviz graph with clusters and random colours.\n\n    Args:\n        num_nodes: Number of nodes in the graph.\n        num_edges: Number of edges in the graph.\n        num_clusters: Number of clusters to create.\n        colors: List of colours to use for clusters (optional). If not provided, random colours will be used.\n    \"\"\"\n\n    dot = graphviz.Digraph(\"G\")\n    dot.attr(fontname=\"Helvetica,Arial,sans-serif\")\n    dot.attr(layout=\"neato\")\n    dot.attr(start=\"random\")\n    dot.attr(overlap=\"false\")\n    dot.attr(splines=\"true\")\n    dot.attr(size=\"8,8\")\n    #dot.attr(dpi=\"300\")\n\n    # nodes to clusters, random colours if not provided\n    cluster_assignments = {}\n    if colors is None:\n        colors = [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(num_clusters)] \n\n    for i in range(num_nodes):\n        cluster_assignments[i] = random.randint(0, num_clusters - 1)\n\n    # random node names, colouur assignment\n    nodes = []\n    for i in range(num_nodes):\n        node_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n        nodes.append(node_name)\n        cluster_id = cluster_assignments[i]\n        color = colors[cluster_id]\n        dot.node(node_name, label=\"\", shape=\"circle\", height=\"0.12\", width=\"0.12\", fontsize=\"1\", fillcolor=color, style=\"filled\")\n        \n\n    # random edges (with a higher probability of staying within clusters)\n    edges = []\n    for _ in range(num_edges):\n        src_cluster = random.randint(0, num_clusters - 1)\n        dst_cluster = src_cluster if random.random() &lt; 0.8 else random.randint(0, num_clusters - 1)  # 80% chance of staying in cluster\n        src_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == src_cluster])\n        dst_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == dst_cluster])\n        edges.append((src_node, dst_node))\n\n    #  edges to the graph\n    for edge in edges:\n        dot.edge(*edge)\n\n    return dot"
  },
  {
    "objectID": "appendix-config.html",
    "href": "appendix-config.html",
    "title": "Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\""
  },
  {
    "objectID": "appendix-config.html#config",
    "href": "appendix-config.html#config",
    "title": "Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\""
  },
  {
    "objectID": "appendix-tech-stack.html",
    "href": "appendix-tech-stack.html",
    "title": "Technology Stack",
    "section": "",
    "text": "TO ADD:\n\nTechnology Stack\n\nPython used\nPython packages used and why\nNeo4j used\nGoogle API\nVS code\nQuarto\nSQL"
  },
  {
    "objectID": "appendix-tech-stack.html#technology-stack",
    "href": "appendix-tech-stack.html#technology-stack",
    "title": "Technology Stack",
    "section": "",
    "text": "TO ADD:\n\nTechnology Stack\n\nPython used\nPython packages used and why\nNeo4j used\nGoogle API\nVS code\nQuarto\nSQL"
  }
]