[
  {
    "objectID": "06-conclusion.html",
    "href": "06-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project’s impact and potential for timetabling processes\nFuture work and recommendations"
  },
  {
    "objectID": "06-conclusion.html#conclusion-500-words",
    "href": "06-conclusion.html#conclusion-500-words",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project’s impact and potential for timetabling processes\nFuture work and recommendations"
  },
  {
    "objectID": "04-timetable-metrics.html",
    "href": "04-timetable-metrics.html",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts"
  },
  {
    "objectID": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "href": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts"
  },
  {
    "objectID": "02-data-engineering.html",
    "href": "02-data-engineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "High-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling\n\n\n\n\n\nBrief overview of SQL extraction techniques\n\n\n\n\n\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nChallenges and solutions with Neo4j Aura\nCloud vs. desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nPotential future enhancements, developments"
  },
  {
    "objectID": "02-data-engineering.html#data-engineering-an-end-to-end-solution-1500-2000-words",
    "href": "02-data-engineering.html#data-engineering-an-end-to-end-solution-1500-2000-words",
    "title": "Data Engineering",
    "section": "",
    "text": "High-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling\n\n\n\n\n\nBrief overview of SQL extraction techniques\n\n\n\n\n\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nChallenges and solutions with Neo4j Aura\nCloud vs. desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nPotential future enhancements, developments"
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Background on university timetabling challenges\nMotivation for exploring graph-based approaches\nProject scope and objectives"
  },
  {
    "objectID": "01-introduction.html#intro-500-words",
    "href": "01-introduction.html#intro-500-words",
    "title": "Introduction",
    "section": "",
    "text": "Background on university timetabling challenges\nMotivation for exploring graph-based approaches\nProject scope and objectives"
  },
  {
    "objectID": "03-graph-data-model.html",
    "href": "03-graph-data-model.html",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "03-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "href": "03-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "05-future-opportunities.html",
    "href": "05-future-opportunities.html",
    "title": "Future Opportunities",
    "section": "",
    "text": "Discussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources"
  },
  {
    "objectID": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "href": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "title": "Future Opportunities",
    "section": "",
    "text": "Discussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Graph Data Models for Timetabling Insights",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "03-data-engineering.html",
    "href": "03-data-engineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "The data engineering pipeline is designed to efficiently and securely transfer selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j), enabling advanced analytics and insights.\nThis section provides an overview of the pipeline architecture, key design principles, and implementation approach.\n\n\nThe data pipeline consists of several key stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: The CSV files are processed, cleaned, transformed, merged, and anonymised using Python code.\nIntermediate Storage: Processed CSVs are saved locally and uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. 𝗘𝗫𝗧𝗥𝗔𝗖𝗧: filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. 𝗧𝗥𝗔𝗡𝗦𝗙𝗢𝗥𝗠: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. 𝐔𝐏𝐋𝐎𝐀𝐃\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. 𝗟𝗢𝗔𝗗: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure 1\n\n\n\n\n\n\nThis pipeline represents a comprehensive approach to data engineering, incorporating several best practices in data handling, processing, and database management.\n\n\n\nDesign Principles\n\n\nThe data pipeline is built on several core design principles. I started with a strong sense of what I wanted to achieve - a modular, scalable, secure and configurable design - however, what exactly this meant was discovered in the development process. Given that the project has several limitations including resources, technology and is also time-constrained, it was important to make the final result one which can be built upon and potentially be developed within operational contexts. However, the project is also a proof-of-concept and as such, some design opportunities were eschewed in favour of simplicity and progress.\n\n\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\n\n\n\nDistinct, interoperable modules (extract, transform, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\n\n\n\nRobust error handling mechanisms\nComprehensive logging for troubleshooting and auditing\n\n\n\n\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling\n\n\n\n\n\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nKey aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. ADD LINK TO TECH APPENDIX\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that componentsare behaving as expected.\n\n\n\n\nThe following sections will delve into the specific implementation details of each stage in the pipeline, demonstrating how these principles are put into practice.\nI will explore the iterative development process, configuration management, extraction techniques, transformation processes, loading strategies, and automation workflows. Finally, I will reflect on lessons learned and potential future enhancements to the data engineering components.\n\n\n\n\nDescription of the iterative process\nTesting methodology\n\n\n\n\nIterative Development Approach\n\n\n\n\n\n\ninformation security\ncontrolled access\nsecure university databases, windows system user, servers\nanonymisation protocol - minimal personal information, still want operatinalisable tool\nreusability, maintainability, testability\nmore data -&gt; scale\nmore data -&gt; properties, relationships\nsql code changes\ngoogle drive apis\nerror handling\nconsistent logging\nvalidation data\n\n\n\n\n\nConfiguration Management\nLogging strategy and implementation\n\n\n\n\n\nBrief overview of SQL extraction techniques\nData Sources\nCode snippet\n\n\n\n\n\nData cleaning and preproessing\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nDatabase schema\nLoading process\nChallenges and solutions with Neo4j Aura\nCloud vs. desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nBest practices discovered\nChallenges over come\nPotential future enhancements, developments"
  },
  {
    "objectID": "03-data-engineering.html#overview-of-the-data-pipeline",
    "href": "03-data-engineering.html#overview-of-the-data-pipeline",
    "title": "Data Engineering",
    "section": "",
    "text": "The data engineering pipeline is designed to efficiently and securely transfer selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j), enabling advanced analytics and insights.\nThis section provides an overview of the pipeline architecture, key design principles, and implementation approach.\n\n\nThe data pipeline consists of several key stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: The CSV files are processed, cleaned, transformed, merged, and anonymised using Python code.\nIntermediate Storage: Processed CSVs are saved locally and uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. 𝗘𝗫𝗧𝗥𝗔𝗖𝗧: filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. 𝗧𝗥𝗔𝗡𝗦𝗙𝗢𝗥𝗠: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. 𝐔𝐏𝐋𝐎𝐀𝐃\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. 𝗟𝗢𝗔𝗗: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure 1\n\n\n\n\n\n\nThis pipeline represents a comprehensive approach to data engineering, incorporating several best practices in data handling, processing, and database management.\n\n\n\nDesign Principles\n\n\nThe data pipeline is built on several core design principles. I started with a strong sense of what I wanted to achieve - a modular, scalable, secure and configurable design - however, what exactly this meant was discovered in the development process. Given that the project has several limitations including resources, technology and is also time-constrained, it was important to make the final result one which can be built upon and potentially be developed within operational contexts. However, the project is also a proof-of-concept and as such, some design opportunities were eschewed in favour of simplicity and progress.\n\n\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\n\n\n\nDistinct, interoperable modules (extract, transform, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\n\n\n\nRobust error handling mechanisms\nComprehensive logging for troubleshooting and auditing\n\n\n\n\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling\n\n\n\n\n\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nKey aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. ADD LINK TO TECH APPENDIX\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that componentsare behaving as expected.\n\n\n\n\nThe following sections will delve into the specific implementation details of each stage in the pipeline, demonstrating how these principles are put into practice.\nI will explore the iterative development process, configuration management, extraction techniques, transformation processes, loading strategies, and automation workflows. Finally, I will reflect on lessons learned and potential future enhancements to the data engineering components.\n\n\n\n\nDescription of the iterative process\nTesting methodology\n\n\n\n\nIterative Development Approach\n\n\n\n\n\n\ninformation security\ncontrolled access\nsecure university databases, windows system user, servers\nanonymisation protocol - minimal personal information, still want operatinalisable tool\nreusability, maintainability, testability\nmore data -&gt; scale\nmore data -&gt; properties, relationships\nsql code changes\ngoogle drive apis\nerror handling\nconsistent logging\nvalidation data\n\n\n\n\n\nConfiguration Management\nLogging strategy and implementation\n\n\n\n\n\nBrief overview of SQL extraction techniques\nData Sources\nCode snippet\n\n\n\n\n\nData cleaning and preproessing\nDetailed discussion of the Python-based transformation process\nHighlight of the anonymisation function\nDiscussion on safeguarding personal identifiable information\n\n\n\n\n\nDatabase schema\nLoading process\nChallenges and solutions with Neo4j Aura\nCloud vs. desktop considerations\n\n\n\n\n\nEnd-to-end automated process for specific programme data\n\n\n\n\n\nReflection on the agile approach and discoveries made during development\nBest practices discovered\nChallenges over come\nPotential future enhancements, developments"
  },
  {
    "objectID": "02-graph-data-model.html",
    "href": "02-graph-data-model.html",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "02-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "href": "02-graph-data-model.html#graph-data-model-for-timetabling-1000-words",
    "title": "Graph Data Model",
    "section": "",
    "text": "Visual representation of both models - mermaid, or simmilar\n\n\n\n\n\n\n\n\nRoom properties example (lat, long)\nPotential for additional data integration (curriculum, student outcomes, etc.)"
  },
  {
    "objectID": "03-data-engineering.html#notes-to-delete",
    "href": "03-data-engineering.html#notes-to-delete",
    "title": "Data Engineering",
    "section": "NOTES TO DELETE",
    "text": "NOTES TO DELETE\n\nHigh-level architecture (data flow diagram)\nKey features: modularity, configurability, scalability, error handling"
  }
]