[
  {
    "objectID": "03-04-extract.html",
    "href": "03-04-extract.html",
    "title": "Extraction",
    "section": "",
    "text": "TODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nextract\n\n\n\nA\n\n\nSQL Database\n(students, staff, programmes,\nactivities, rooms, etc.)\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nA-&gt;keyring\n\n\nRequires\n\n\n\nB\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nextract\n\nùóòùó´ùóßùó•ùóîùóñùóß\n\n\n\nextract-&gt;B\n\n\n\n\n\nkeyring-&gt;extract\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;extract\n\n\nSQL Scripts\n\n\n\nconfig-&gt;keyring\n\n\nCredentials\n\n\n\n\n\n\n\n\n\nExtract\n\nThe extraction process starts by securely connecting to the specified SQL database using encrypted credentials stored with keyring. The combination of configuration and SQL scripts determine which data will be extracted by filtering based on programme(s) of study and specifying which nodes, relationships and properties to extract. Additional options include specifying chunk size if extracting signficicant amounts of data, for example.\nThe process performs basic validation at every step ensuring secure connection before running SQL SELECT statements and storing extracted data as csv files locally.\n\nSQL example\n\n\nClick to show code\nSELECT DISTINCT a.[Id] AS actSplusID,\n     CONCAT(a.[Id], '-', adt.[Week], '-', adt.[Day]) AS actGraphID,\n     a.[Name] AS actName,\n     a.[Description] AS actDescription,\n     a.[DepartmentId] AS actDeptSPlusID,\n     adt.[StartDateTime] AS actStartDateTime,\n     adt.[EndDateTime] AS actEndDateTime,\n     adt.[Week] AS actWeekNum,\n     adt.[Occurrence] AS actOccurrence,\n     a.[ModuleId] AS actModSplusID,\n     a.[ScheduledDay] AS actScheduledDay,\n     a.[StartDate] AS actFirstActivityDate,\n     a.[EndDate] AS actLastActivityDate,\n     a.[PlannedSize] AS actPlannedSize,\n     a.[RealSize] AS actRealSize,\n     a.[Duration] AS actDuration,\n     a.[DurationInMinutes] AS actDurationInMinutes,\n     a.[NumberOfOccurrences] AS actNumberOfOccurrences,\n     a.[WeekPattern] AS actWeekPattern,\n     a.[ActivityTypeId] AS actActivityTypeSplusID,\n     a.[WhenScheduled] AS actWhenScheduled,\n     a.[IsJtaParent],\n     a.[IsJtaChild],\n     a.[IsVariantParent],\n     a.[IsVariantChild]\nFROM ##TempActivity a\nINNER JOIN ##TempActivityDateTime adt ON a.[Id] = adt.[ActivityID];\n\n\n\n\nextract_main.py snippet\n\n\nClick to show code\n# extract_main.py\nfrom logger_config import extract_logger\nfrom extract_data import main as extract_main\nfrom config import EXTRACT_DIR, HOSTKEYS, CHUNK_SIZE\nfrom utils import execution_times\n\ndef run_extraction():\n    extract_logger.info(\"Starting data extraction process\")\n    extract_logger.info(f\"Output Directory: {EXTRACT_DIR}\")\n    extract_logger.info(f\"Hostkeys: {HOSTKEYS}\")\n    extract_logger.info(f\"Chunksize: {CHUNK_SIZE}\")\n\n    try:\n        extract_main()\n    except Exception as e:\n        extract_logger.exception(\"An error occurred during data extraction:\")\n    finally:\n        extract_logger.info(\"Data extraction completed.\")\n\n   \n    # Log the execution times\n    extract_logger.info(\"Extraction Time Summary:\")\n    for func_name, exec_time in execution_times.items():\n        extract_logger.info(f\"Function {func_name} took {exec_time:.2f} seconds\")\n\n\nif __name__ == \"__main__\":\n    run_extraction()",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Extract"
    ]
  },
  {
    "objectID": "02-01-comparison.html",
    "href": "02-01-comparison.html",
    "title": "Graph vs Relational Data Models",
    "section": "",
    "text": "TODO\n\n\n\n\nfind graph, rel quotes\nadd refernces\nupdate images and text\nHaving established the context and motivation for this project, let us now focus on the core of the proposed solution: the graph data model for university timetabling. As outlined in the project aims, my hypothesis is that graph-based approaches have the potential to offer new insights and efficiencies in timetable analysis and optimisation. This section will explore the theoretical underpinnings of graph data structures and their application to the complex domain of university timetabling.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#relational-models",
    "href": "02-01-comparison.html#relational-models",
    "title": "Graph vs Relational Data Models",
    "section": "Relational Models",
    "text": "Relational Models\n\nTables, Joins and the Limits of Interconnectedness\nRelational databases, using SQL as their query language, have long been the go-to for managing data, including timetabling information. They structure data into tables, where rows represent entities (like rooms, staff, or students) and columns represent their attributes (name, capacity, email, etc.). Relationships between these entities are established through foreign keys, forming links between tables. This often involves intermediary ‚Äúrelationship‚Äù tables to handle the many-to-many nature of timetabling data (e.g., a student attends many activities, and an activity has many students). (Khan et al., 2023;Sokolova, G√≥mez and Borisoglebskaya, 2020)\nWhile robust and well-understood, relational databases start to show their limitations when dealing with the highly interconnected nature of timetables:\n\nJoin Complexity: Even seemingly simple queries, like ‚Äúfind students attending a specific lecturer‚Äôs class in a particular building,‚Äù require joining multiple tables (see below). As queries become more nuanced, the number of joins increases, often impacting performance, especially with large datasets.\nRigidity: Relational databases rely on a predefined schema, making them less adaptable to evolving needs. Adding new entities or relationships, a common occurrence in dynamic timetabling environments, often requires schema modifications, potentially disrupting existing queries and applications.\n\n\n\nExample Simple Entity Relationship Diagram\n\n\n\nExample Query in SQL\n-- Assuming \"BuildingName\" is in V_BUILDING and linked to V_LOCATION\n-- find students attending a specific lecturer's class in a particular building\n-- requires 6 JOINS\n\n\nSELECT DISTINCT \n    ss.[FirstName], \n    ss.[LastName],\n    ss.[Email]\nFROM [RDB_MAIN2223].[rdowner].[V_STUDENTSET] ss\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STUDENTSET] acts ON ss.[Id] = acts.[StudentSetId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY] a ON acts.[ActivityId] = a.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_LOCATION] al ON a.[Id] = al.[ActivityId]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_LOCATION] l ON al.[LocationId] = l.[Id]\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_BUILDING] b ON l.[BuildingId] = b.[Id] # Assuming a BuildingId column in V_LOCATION\nINNER JOIN [RDB_MAIN2223].[rdowner].[V_ACTIVITY_STAFF] ast ON a.[Id] = ast.[ActivityId]\nWHERE ast.[StaffId] = 'StaffID'  \n  AND b.[Name] = 'BuildingName';",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#graph-models",
    "href": "02-01-comparison.html#graph-models",
    "title": "Graph vs Relational Data Models",
    "section": "Graph Models",
    "text": "Graph Models\n\nEmbracing interconnectedness\nIn contrast to the rigid table structure of relational databases, graph databases offer a more intuitive and flexible approach for representing interconnected data like timetables. They utilise:\n\nNodes: Represent entities (e.g., activity, room, lecturer, student).\nEdges: Represent relationships between nodes (e.g., ‚Äútaught by,‚Äù ‚Äúenrolled in,‚Äù ‚Äúscheduled at‚Äù).\n\n\n\n\nSimple Graph Data Model\n\n\nThis node-and-edge structure inherently reflects how timetabling elements connect. Instead of relying on cumbersome joins, relationships are directly encoded in the data model itself. This results in several advantages:\n\nNatural Representation: Graph databases visually and conceptually mirror the relationships inherent in timetables, making them easier to understand and query.\nRelationship-Centric Queries: Graph databases are optimised for traversing and analysing relationships. Queries that would require multiple joins in a relational database often become significantly simpler and faster in a graph database.\nFlexibility: The schema-less or schema-optional nature of most graph databases allows for greater flexibility in data modeling. New entities or relationships can be added effortlessly without impacting existing structures or queries (Nan and Bai, 2019;Webber, Eifrem and Robinson, 2013).\n\nExample Query in Cypher\n// Assuming properties on nodes\n// Much simpler query pattern\n\nMATCH (s:Student)-[:ATTENDS]-&gt;(a:Activity)&lt;-[:TEACHES]-(st:Staff), \n      (a:Activity)-[:TAKES_PLACE_IN]-&gt;(r:Room)\nWHERE st.last_name = \"LecturerLastName\" AND r.building = \"BuildingName\"\nRETURN s.first_name, s.last_name, s.email",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "02-01-comparison.html#key-differences-and-implications",
    "href": "02-01-comparison.html#key-differences-and-implications",
    "title": "Graph vs Relational Data Models",
    "section": "Key Differences and Implications",
    "text": "Key Differences and Implications\n\n\n\n\n\n\n\n\n\nFeature\nRelational Model\nGraph Model\n\n\n\n\nData Structure\nTables with rows and columns\nNodes and edges\n\n\nSchema\nRigid, predefined\nFlexible, schema-less or schema-optional\n\n\nRelationship Handling\nForeign keys, joins\nDirect connections (edges)\n\n\nQuery Performance\nCan be slow for relationship-heavy queries\nOptimised for traversing relationships, potentially faster\n\n\nData Modeling\nLess intuitive for interconnected data\nNaturally represents complex relationships\n\n\nAdaptability\nLess adaptable to schema changes\nMore flexible, accommodates evolving data needs\n\n\n\n\nThese advantages position graph databases as a powerful tool for uncovering insights hidden within complex, interconnected datasets like university timetables. The following section will detail a graph data model specifically designed to leverage these strengths for enhanced timetable analysis.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph vs Relational Data Models"
    ]
  },
  {
    "objectID": "appendix-random-graph.html",
    "href": "appendix-random-graph.html",
    "title": "Random Graph Generator",
    "section": "",
    "text": "The function below generates a random graph (dot file) using Graphviz.\nTo render, ensure that graphviz is installed or save to file and render within documents using Quarto or similar.\n\n\nClick to show code\nimport graphviz\nimport random\nimport string\nfrom collections import defaultdict\n\ndef generate_random_graph(num_nodes=50, num_edges=100, num_clusters=5, colors=None):\n    \"\"\"Generates a random Graphviz graph with clusters and random colours.\n\n    Args:\n        num_nodes: Number of nodes in the graph.\n        num_edges: Number of edges in the graph.\n        num_clusters: Number of clusters to create.\n        colors: List of colours to use for clusters (optional). If not provided, random colours will be used.\n    \"\"\"\n\n    dot = graphviz.Digraph(\"G\")\n    dot.attr(fontname=\"Helvetica,Arial,sans-serif\")\n    dot.attr(layout=\"neato\")\n    dot.attr(start=\"random\")\n    dot.attr(overlap=\"false\")\n    dot.attr(splines=\"true\")\n    dot.attr(size=\"8,8\")\n    #dot.attr(dpi=\"300\")\n\n    # nodes to clusters, random colours if not provided\n    cluster_assignments = {}\n    if colors is None:\n        colors = [\"#%06x\" % random.randint(0, 0xFFFFFF) for _ in range(num_clusters)] \n\n    for i in range(num_nodes):\n        cluster_assignments[i] = random.randint(0, num_clusters - 1)\n\n    # random node names, colouur assignment\n    nodes = []\n    for i in range(num_nodes):\n        node_name = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n        nodes.append(node_name)\n        cluster_id = cluster_assignments[i]\n        color = colors[cluster_id]\n        dot.node(node_name, label=\"\", shape=\"circle\", height=\"0.12\", width=\"0.12\", fontsize=\"1\", fillcolor=color, style=\"filled\")\n        \n\n    # random edges (with a higher probability of staying within clusters)\n    edges = []\n    for _ in range(num_edges):\n        src_cluster = random.randint(0, num_clusters - 1)\n        dst_cluster = src_cluster if random.random() &lt; 0.8 else random.randint(0, num_clusters - 1)  # 80% chance of staying in cluster\n        src_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == src_cluster])\n        dst_node = random.choice([node for i, node in enumerate(nodes) if cluster_assignments[i] == dst_cluster])\n        edges.append((src_node, dst_node))\n\n    #  edges to the graph\n    for edge in edges:\n        dot.edge(*edge)\n\n    return dot",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Random Graph Generator"
    ]
  },
  {
    "objectID": "03-06b-load.html",
    "href": "03-06b-load.html",
    "title": "Neo4j Load",
    "section": "",
    "text": "TODO\n\n\n\n\nadd log files?\nadd screenshot?\nadd neo4j loaded data screenshot\n\n\n\nWith accessible csv files, the final module of the ETL pipeline creates (or updates) nodes and relationships in the Neo4j instance.\n\n\n\n\n\n\n\nneo4j_load\n\n\n\ngdrive_files\n\nGoogle Drive Files\n(nodes, relationships folders)\n\n\n\ngdrive_api\n\nConnect to Google Drive API\n\n\n\ngdrive_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nkeyring\n\n\n\nKeyring\n(Credentials)\n\n\n\nneo4j_connection\n\nConnect to Neo4j\n(with credentials)\n\n\n\nkeyring-&gt;neo4j_connection\n\n\n\n\n\nlist_files\n\nList Files\n(from folders)\n\n\n\ngdrive_api-&gt;list_files\n\n\n\n\n\ndetermine_nodes\n\nDetermine Nodes\n(to create)\n\n\n\nlist_files-&gt;determine_nodes\n\n\n\n\n\ndetermine_relationships\n\nDetermine Relationships\n(to create)\n\n\n\nlist_files-&gt;determine_relationships\n\n\n\n\n\ncreate_schema\n\nCreate Schema\n(dynamic or custom)\n\n\n\nlist_files-&gt;create_schema\n\n\nOptionally Create\n\n\n\ncreate_nodes\n\nCreate Nodes\n\n\n\nneo4j_connection-&gt;create_nodes\n\n\n\n\n\ncreate_relationships\n\nCreate Relationships\n\n\n\nneo4j_connection-&gt;create_relationships\n\n\n\n\n\ndetermine_nodes-&gt;create_nodes\n\n\n\n\n\ndetermine_relationships-&gt;create_relationships\n\n\n\n\n\ncreate_schema-&gt;create_nodes\n\n\n\n\n\ncreate_schema-&gt;create_relationships\n\n\n\n\n\nset_node_properties\n\nSet Node Properties\n(with datatypes)\n\n\n\ncreate_nodes-&gt;set_node_properties\n\n\n\n\n\nset_relationship_properties\n\nSet Relationship Properties\n\n\n\ncreate_relationships-&gt;set_relationship_properties\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nset_node_properties-&gt;process_done\n\n\n\n\n\nset_relationship_properties-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nThere are two authentication requirements:\n\nGoogle Drive to get node and relationship files and data.\n\nNeo4j Aura instance is connected to with Keyring encrypted credentials.\n\nThe process automatically processes nodes and relationships based on files in the specified folders by using a file-pattern matching approach. However, this can be overridden within configuration.\nAlso in configuration is the option to create a database schema. There are three options:\n\nNo schema\nDynamic (default) - creates unique constraints based on nodes\nCustom - allows the user to specify specific constraints prior to loading.\n\nAt this point, the ETL loads data on a row-by-row basis, reading the public csv files. Columns become properties with data types cross-referenced from a data-mapping dictionary in the configuration.\nIf there have been no errors - we should have data in our Neo4j Aura instance!",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Neo4j Load"
    ]
  },
  {
    "objectID": "03-05-transform.html",
    "href": "03-05-transform.html",
    "title": "Transformation",
    "section": "",
    "text": "TODO\nThe transformation section of the ETL pipeline picks up from where extract finished by using the extracted csv files as the source.\ntransform\n\n\n\nsource_files\n\nCSV Files\n(./{hostkeys}/extract)\n\n\n\nvalidate_data\n\nValidating Data\n\n\n\nsource_files-&gt;validate_data\n\n\n\n\n\nconfig\n\nConfiguration\n\n\n\nconfig-&gt;validate_data\n\n\n\n\n\nclean_data\n\nCleaning Data\n\n\n\nconfig-&gt;clean_data\n\n\n\n\n\nadd_department\n\nAdding 'Department' to Nodes\n\n\n\nconfig-&gt;add_department\n\n\n\n\n\nanonymise_data\n\nùóîùó°ùó¢ùó°ùó¨ùó†ùóúùó¶ùóúùó°ùóö\nPersonal Data\n\n\n\nconfig-&gt;anonymise_data\n\n\n\n\n\naugment_rooms\n\nAugmenting Rooms\nwith Archibus Data\n\n\n\nconfig-&gt;augment_rooms\n\n\n\n\n\ncreate_relationships\n\nCreating\nRelationship Tables\n\n\n\nconfig-&gt;create_relationships\n\n\n\n\n\nvalidate_data-&gt;clean_data\n\n\n\n\n\nclean_data-&gt;add_department\n\n\n\n\n\nadd_department-&gt;anonymise_data\n\n\n\n\n\nanonymise_data-&gt;augment_rooms\n\n\n\n\n\naugment_rooms-&gt;create_relationships\n\n\n\n\n\nprocessed_files\n\nProcessed CSV Files\n\n\n\ncreate_relationships-&gt;processed_files\nThe configuration files allows the user to specify which columns should be used as the unique identifier when determining uniqueness, creating relationships between nodes and linking to additional datasets. It is also possible to specify datatypes - the load process will automatically load properties as string unless it is well formatted or the datatype is predetermined. The config file allows the user to specify how to handle certain datatypes like dates, times, boolean, etc.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#all-data",
    "href": "03-05-transform.html#all-data",
    "title": "Transformation",
    "section": "All data",
    "text": "All data\n\nValidation - basic validation of the data is performed. Validation is extensible and can be expanded, as requirements are identified.\nCleaned - basic cleaning of all data is performed by stripping empty space and removing non-printable characters, etc. using regex. The cleaning functionality can be expanded.\n\nWith clean data, the transformation proper starts:",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "03-05-transform.html#nodes-and-relationships",
    "href": "03-05-transform.html#nodes-and-relationships",
    "title": "Transformation",
    "section": "Nodes and relationships",
    "text": "Nodes and relationships\n\nAdd Organisational Unit - where appropriate, the University Organisational Unit (e.g.¬†College, School, Department) is added to the node. This will be picked up as a property during load.\n\nData Augmentation - Room data is augmented with additional properties from the location master database, including latitude, longitude, square meterage, etc. Data augmentation is extensible.\nAnonymisation - Personal data is anonymised. An anonymisation function was developed to remove and replace any personally identifiable information (PII). The pipeline extracts minimal PII but this is safely anonymised. The functional also adds fake emails. See Appendix for Anonymisation\nRelationships - Based on requirements in the configuration, relationships are extracted including optional relationship properties.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Transform"
    ]
  },
  {
    "objectID": "appendix-tech-stack.html",
    "href": "appendix-tech-stack.html",
    "title": "Technology Stack",
    "section": "",
    "text": "TO ADD:\n\nTechnology Stack\n\nPython used\nPython packages used and why\nNeo4j used\nGoogle API\nVS code\nQuarto\nSQL\n\ngraphviz\narrows\nmermaid\ngit hub, git",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Technology Stack"
    ]
  },
  {
    "objectID": "appendix-tech-stack.html#technology-stack",
    "href": "appendix-tech-stack.html#technology-stack",
    "title": "Technology Stack",
    "section": "",
    "text": "TO ADD:\n\nTechnology Stack\n\nPython used\nPython packages used and why\nNeo4j used\nGoogle API\nVS code\nQuarto\nSQL\n\ngraphviz\narrows\nmermaid\ngit hub, git",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Technology Stack"
    ]
  },
  {
    "objectID": "appendix-config.html",
    "href": "appendix-config.html",
    "title": "Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\"",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Configuration"
    ]
  },
  {
    "objectID": "appendix-config.html#config",
    "href": "appendix-config.html#config",
    "title": "Configuration YAML",
    "section": "",
    "text": "The below is an example of configuration options configured in more human readable YAML format.\n\n# ETL Pipeline Configuration\n\ngeneral:\n  hostkeys: \n    - INB112\n    # - N420\n  folder_name: '' # default to hostkey if empty\n\nfile_paths:\n  root_dir: '.'  # default to current working directory\n  nodes_folder_url: # (Optional) override for dynamic lookup) eg \"https://drive.google.com/drive/folders/1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\"\n  relationships_folder_url: # (Optional) override for dynamic lookup) eg.\"https://drive.google.com/drive/folders/1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\"\n  gdrive_root_folder_url: \"1iWkeTubJ0xZ6I728emoj9BkqZm7dL2fq\"\n  gdrive_folder_name: # Leave commented out to use default (hostkey)\n  google_credentials_path: 'credentials/graph-diss-dbbdbb5e5d00.json'\n  department_source: 'node-dept-all.csv'\n  archibus_source: 'archibus.csv'\n\ndata_processing:\n  chunk_size: 20000\n  temp_tables_sql_file: \"create_temp_tables.sql\"\n  node_output_filename_template: \"node-{node}-processed.csv\"\n  rel_output_filename_template: \"rel-{relationship}-processed.csv\"\n\nneo4j:\n  #max_connection_retries: 5\n  #max_transaction_retry_time: 30\n  schema:\n    apply: True\n    type: 'dynamic' # Options: 'dynamic', 'custom'\n    custom_path: ''\n  batch_size: 1000\n\nlogging:\n  log_level: \"INFO\" # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL \n\nnodes:\n  department: \n    filename_pattern: \"node-dept-all*.csv\"\n    dept_join_col: null \n    node_suffix: 'dept'\n    node_id: \"deptSplusID\"\n  module: \n    filename_pattern: \"node-module-by-pos-temp*.csv\"\n    dept_join_col: \"modSplusDeptID\"\n    node_suffix: \"mod\"\n    node_id: \"modSplusID\"             \n  room: \n    filename_pattern: \"node-room-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: 'room'\n    node_id: \"roomSplusID\"   \n  programme: \n    filename_pattern: \"node-pos-by-pos-temp*.csv\"\n    dept_join_col: \"posSplusDeptID\"\n    node_suffix: \"pos\"\n    node_id: \"posSplusID\"\n  activityType: \n    filename_pattern: \"node-activitytype-by-pos-temp*.csv\"\n    dept_join_col: 'actTypeDeptSplusID'\n    node_suffix: 'actType'\n    node_id: 'actTypeSplusID'\n  staff: \n    filename_pattern: \"node-staff-by-pos-temp*.csv\"\n    dept_join_col: \"staffDeptSplusID\"\n    node_suffix: \"staff\"\n    dtype:\n      staffSplusID: str\n      staffID: str \n    node_id: \"staffSplusID\"\n  student: \n    filename_pattern: \"node-student-by-pos-temp*.csv\"\n    dept_join_col: \"stuDeptSplusID\"\n    node_suffix: \"stu\"\n    dtype: \n      stuSplusID: str\n      studentID: str\n    node_id: \"stuSplusID\"\n  activity: \n    filename_pattern: \"node-activity-by-pos-temp*.csv\"\n    dept_join_col: null\n    node_suffix: null\n    dtype:\n      actSplusID: str\n      actTypeSplusID: str \n      actRoomSplusID: str\n      actStaffSplusID: str \n      actStuSplusID: str \n      actStartDateTime: str\n      actEndDateTime: str\n      actFirstActivityDate: str\n      actLastActivityDate: str\n      actWhenScheduled: str\n    node_id: \"actGraphID\" \n\nrelationships:\n  activity_module: \n    filename_pattern: \"rel-activity-module-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"modSplusID\"\n    relationship: \"BELONGS_TO\"\n  activity_room: \n    filename_pattern: \"rel-activity-room-by-pos-temp*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"roomSplusID\"\n    relationship: \"OCCUPIES\"\n  activity_staff: \n    filename_pattern: \"rel-activity-staff-by-pos-temp*.csv\"\n    node1_col: \"staffSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"TEACHES\"\n  activity_student: \n    filename_pattern: \"rel-activity-student-by-pos-temp*.csv\"\n    node1_col: \"stuSplusID\"\n    node2_col: \"actSplusID\"\n    relationship: \"ATTENDS\"\n  activity_activityType: \n    filename_pattern: \"relActivityActType*.csv\"\n    node1_col: \"actSplusID\"\n    node2_col: \"actActivityTypeSplusID\"\n    relationship: \"HAS_TYPE\"\n  module_programme: \n    filename_pattern: \"rel-mod-pos-by-pos-temp*.csv\"\n    node1_col: \"modSplusID\"\n    node2_col: \"posSplusID\"\n    relationship: \"BELONGS_TO\"\n    properties: \n      - \"modType\"\n\ndata_type_mapping:\n  activity:\n    actStartDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actEndDateTime: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actFirstActivityDate: ['date2', '%Y-%m-%d']\n    actLastActivityDate: ['date2', '%Y-%m-%d']\n    actPlannedSize: 'int'\n    actRealSize: 'int'\n    actDuration: 'int'\n    actDurationInMinutes: 'int'\n    actNumberOfOccurrences: 'int'\n    actWhenScheduled: ['datetime', '%Y-%m-%d %H:%M:%S']\n    actStartDate: ['date', '%Y-%m-%d']\n    actEndDate: ['date', '%Y-%m-%d']\n    actStartTime: 'time'\n    actEndTime: 'time'\n    actScheduledDay: 'int'\n  room:\n    roomCapacity: 'int'\n\ndisplay_name_mapping:\n  activity: \"actName\"",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Configuration"
    ]
  },
  {
    "objectID": "05-future-opportunities.html",
    "href": "05-future-opportunities.html",
    "title": "Future Opportunities",
    "section": "",
    "text": "Unveiling Hidden Patterns & Improving Student Experience: Problem: Timetable inefficiencies often remain hidden in relational data, impacting student experience. Graph Solution: Graph analysis can uncover patterns like students with excessive travel time between classes, those lacking adequate breaks, or those facing scheduling conflicts due to part-time work. This empowers universities to optimize timetables for improved student well-being and academic performance. Stakeholder-Centric Analysis & Enhanced Decision Making: Problem: Traditional timetabling often prioritizes one factor (e.g., room utilization) over others, neglecting holistic needs. Graph Solution: Graphs allow simultaneous modeling of student preferences (class times, travel distance), faculty constraints, and institutional priorities (resource allocation). This enables data-driven decisions that balance stakeholder needs and improve overall satisfaction. What-If Scenarios & Agile Timetable Management: Problem: Evaluating the impact of timetable changes in relational systems is cumbersome, hindering proactive planning. Graph Solution: Graph databases excel at simulating ‚Äúwhat-if‚Äù scenarios. Adding hypothetical courses, adjusting room capacities, or modifying faculty availability becomes straightforward. This agility allows for rapid evaluation of multiple scenarios, enabling institutions to anticipate challenges and adapt timetables dynamically. Visual Exploration & Fostering Collaboration: Problem: Communicating complex timetable data to diverse stakeholders (students, faculty, administrators) is challenging. Graph Solution: Graph visualizations make complex relationships intuitive and accessible, fostering shared understanding. This transparency promotes collaboration, reduces misunderstandings, and facilitates informed decision-making.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#opportunities",
    "href": "05-future-opportunities.html#opportunities",
    "title": "Future Opportunities",
    "section": "",
    "text": "Unveiling Hidden Patterns & Improving Student Experience: Problem: Timetable inefficiencies often remain hidden in relational data, impacting student experience. Graph Solution: Graph analysis can uncover patterns like students with excessive travel time between classes, those lacking adequate breaks, or those facing scheduling conflicts due to part-time work. This empowers universities to optimize timetables for improved student well-being and academic performance. Stakeholder-Centric Analysis & Enhanced Decision Making: Problem: Traditional timetabling often prioritizes one factor (e.g., room utilization) over others, neglecting holistic needs. Graph Solution: Graphs allow simultaneous modeling of student preferences (class times, travel distance), faculty constraints, and institutional priorities (resource allocation). This enables data-driven decisions that balance stakeholder needs and improve overall satisfaction. What-If Scenarios & Agile Timetable Management: Problem: Evaluating the impact of timetable changes in relational systems is cumbersome, hindering proactive planning. Graph Solution: Graph databases excel at simulating ‚Äúwhat-if‚Äù scenarios. Adding hypothetical courses, adjusting room capacities, or modifying faculty availability becomes straightforward. This agility allows for rapid evaluation of multiple scenarios, enabling institutions to anticipate challenges and adapt timetables dynamically. Visual Exploration & Fostering Collaboration: Problem: Communicating complex timetable data to diverse stakeholders (students, faculty, administrators) is challenging. Graph Solution: Graph visualizations make complex relationships intuitive and accessible, fostering shared understanding. This transparency promotes collaboration, reduces misunderstandings, and facilitates informed decision-making.",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#challenges",
    "href": "05-future-opportunities.html#challenges",
    "title": "Future Opportunities",
    "section": "Challenges",
    "text": "Challenges\nData Migration & Integration: A Necessary Hurdle: Challenge: Migrating from existing relational systems to a graph database requires careful planning and data transformation. Mitigation: Employing robust ETL (Extract, Transform, Load) processes and leveraging graph database import tools can streamline the migration process. Prioritizing incremental migration, starting with core entities, can minimize disruption. Tooling and Expertise: Bridging the Skills Gap: Challenge: The graph database ecosystem, while maturing, might require specialized skills compared to traditional SQL. Mitigation: Investing in staff training, collaborating with experts, and leveraging online resources can address the skills gap. Open-source graph databases like Neo4j offer ample learning material and community support. Performance at Scale: Ensuring Responsiveness with Large Datasets: Challenge: Graph databases, while generally performant for connected data, might face challenges with extremely large universities and complex queries. Mitigation: Employing performance tuning techniques like indexing, caching, and query optimization can enhance scalability. Exploring specialized graph database solutions designed for high-volume transactional systems might be necessary in extreme cases. ‚ÄúSoft‚Äù Constraint Modeling: Quantifying Subjective Preferences: Challenge: Graphs excel at explicit relationships but struggle with subjective preferences (e.g., student aversion to late classes). Mitigation: Combine graph analysis with techniques like sentiment analysis on student feedback or preference elicitation surveys. This hybrid approach allows incorporating both explicit relationships and quantified subjective factors.\n2.4 Data Augmentation Opportunities\nData Augmentation Opportunities: You touch on this briefly; expanding this section could be very compelling. Example: Integrating room location data (latitude/longitude) with student address data could allow for powerful analyses of commute patterns and potential inequities. Ethical Considerations: As your project deals with student data, briefly mention the importance of data privacy, anonymization, and responsible use of insights.\n2.5 Challenges and Considerations\nPotential limitations of the graph approach Data migration considerations Performance considerations for large-scale timetabling systems",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "href": "05-future-opportunities.html#future-opportunities-and-potential-insights-500-words",
    "title": "Future Opportunities",
    "section": "Future Opportunities and Potential Insights (500 words)",
    "text": "Future Opportunities and Potential Insights (500 words)\n\nDiscussion of potential analyses (module combinations, student clustering, etc.)\nIntegration of additional data sources",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Future Opportunities"
    ]
  },
  {
    "objectID": "03-07-reflection.html",
    "href": "03-07-reflection.html",
    "title": "Reflections",
    "section": "",
    "text": "From the outset, I recognised that this data engineering project was ambitious in both scale and scope. However, the reality of its magnitude became increasingly apparent as development progressed. Despite my initial awareness, I found myself continually expanding the project‚Äôs boundaries, often pushing for a ‚Äúgold-plated‚Äù solution rather than acknowledging when certain aspects were ‚Äúgood enough.‚Äù This tendency towards scope creep, while driven by a desire for excellence, has significantly increased the project‚Äôs complexity and time requirements.\nThe learning curve has been exceptionally steep. I‚Äôve had to rapidly acquire proficiency in a diverse range of technologies and tools: Python, Neo4j, Google APIs, Quarto, and GraphViz. This intensive learning process, while challenging, has been incredibly rewarding, expanding my technical toolkit far beyond my initial expectations. However, it has also contributed to the project‚Äôs expanding scope, as each new skill acquired opened up possibilities for further enhancements.\nUnexpected challenges have been a constant companion throughout this process. From deleted servers and access issues to discrepancies between development environments (such as missing user certificates), I‚Äôve encountered a wide array of unforeseen obstacles. These issues have necessitated the development of strong troubleshooting skills and a flexible approach to problem-solving. While often frustrating, these challenges have also provided valuable learning opportunities, pushing me to deepen my understanding of the systems and technologies I‚Äôm working with.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-07-reflection.html#lessons-learned",
    "href": "03-07-reflection.html#lessons-learned",
    "title": "Reflections",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nScope management is crucial: Work on recognising when a solution is ‚Äúgood enough‚Äù and resist the urge to continually expand scope. Set clear boundaries at the start and be prepared to reassess and adjust plans when necessary.\nEmbrace modularisation from the beginning: Avoid the temptation to create oversized code blocks. Maintain a list of ‚Äúfuture enhancements‚Äù to prevent immediate implementation of every idea.\nBalance documentation with development: Document sufficiently during the development process, but save comprehensive documentation for appropriate milestones. This approach maintains progress while ensuring proper record-keeping.\nView obstacles as learning opportunities: Embrace continuous learning and see challenges as chances to grow. Invest time in understanding the right technologies and approaches, particularly focusing on modularisation.\nCelebrate incremental progress: Recognise and appreciate small achievements throughout the development process. This helps maintain motivation and provides a clearer sense of overall progress.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Reflection"
    ]
  },
  {
    "objectID": "03-02-approach.html",
    "href": "03-02-approach.html",
    "title": "Data Engineering Approach",
    "section": "",
    "text": "TODO\n\n\n\n\nAdd agile reference\n\n\n\nI followed an interative, agile-inspired approach when developing the data pipeline, despite being a team of one. This allowed for flexibility, continuous improvement and the opportunity to adapt to new insights during the process. The bulk of my effort was spent prototyping, testing and reviewing with each iteration resulting in a new challenge, issue, opportunity or occasionally, success.\n\n\n\nIterative Development Approach\n\n\n\nInitial Planning and Requirements Gathering\nThe development cycle began with initial high-level planning and requirements gathering, where I imagined how each stage should work, trying to bear in mind future-proofing and repeatability principles.\nI defined core functionality for each module (extraction, transformation, loading) and outlined initial technical requirements and constraints. The planning documentation was maintained in Quarto and markdown files in a centralised repository for project information.\n\n\nPrototyping\nFollowing the initial planning, rapid prototyping was undertaken for each module:\n\nSQL prototyping for data extraction queries\nPython prototyping for data transformation and processing logic\nNeo4j prototyping for graph database schema and loading procedures\n\nThis stage allowed for quick exploration of different approaches and early identification of potential challenges as well as giving me the confidence to continue with my exploration.\n\n\nComponent-Based Development and Testing\nDevelopment proceeded with a focus on individual components:\n\nEach module (extraction, transformation, loading) was developed separately with a view to distinct ‚Äúhandovers‚Äù\nAn iterative, component-based testing approach was employed\nWhile formal unit tests were not always created, each component was thoroughly tested for functionality\n\nThis approach allowed for continuous progress while maintaining a focus on component-level quality. It was during this phase that I started expanding configuration, logging and error-handling options.\n\n\nIntegration -&gt; Review -&gt; Demo -&gt; Feedback -&gt; Repeat\nAs components reached a (more) stable state, they were integrated and reviewed:\n\nComponents were combined to form larger functional units\nIntegrated functionality was occasionally demonstrated to subject matter experts (operational timetablers, timetable manager, data manager)\nFeedback was gathered on functionality, usability, and alignment with requirements\n\nInsights gained from reviews, demonstrations and ongoing development were continuously fed back into the process. New requirements or modifications were documented, for example updates to SQL SELECT statements and data model interpretations.\nWith each new piece of information or change, decisions were required and made - but not always the right ones.\n\n\nVersion Validation and Documentation\nAt pivotal junctures, e.g., when a stable version was achieved:\n\nEnd-to-end validation of the entire pipeline was performed.\nResults were documented in notebooks, including opportunities for improvement.\nAny issues (or opportunities) identified were logged for the next iteration.\n\n\n\nContinuous Learning and Adaptation\nThroughout the development process, learning and adaptation became central to the project‚Äôs evolution. Each iteration brought new insights, often through trial and error and certainly through unintended consequences or unforeseen complications. Early challenges included the need to modularise components before they became unmanageable, resisting the temptation to make overly ambitious changes or indeed resisting the temptation to carry on when it would have been better to pause and shore up progress. With practice, I became better at recognising when refactoring was necessary. These experiences underscored the importance of incremental progress and consistent testing in maintaining project stability and direction.\nThis iterative journey was far from linear. There were many moments of frustration, periods of painstaking troubleshooting, and the constant urge to overdeliver, often exceeding the original proof-of-concept scope. Yet, with each stumble, the process itself became more refined, transforming into a powerful tool for identifying and resolving issues.\nWhile the core MVP (minimum viable product) requirements remained relatively stable (I set them after all!), the iterative approach empowered me to seize opportunities for enhancement. Each chance to modularise, parameterise, or fine-tune sparked an almost compulsive drive for improvement, pushing the pipeline beyond its initial scope.\nThis dedication to continuous refinement, while time-consuming, ultimately resulted in a robust, flexible solution that can adapt gracefully to unforeseen challenges and serve as the starting point for future opportunities.\nHowever, this rigorous development process naturally led to a greater focus on data engineering rather than delving into the potential insights offered by Neo4j, simply due to the allocation of time and resources.\nThe iterative approach proved to be more than just a development methodology. It facilitated personal growth, enhanced technical skills, and improved project management capabilities.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Approach"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html",
    "href": "02-02-graph-timetable.html",
    "title": "Graph Data Model for Timebling",
    "section": "",
    "text": "TODO\n\n\n\n\nwrite up section - review notes\nnew page of difficulties with time - see poc2 time\nappendix with queries (queries.ipynb (graph-project.ipynb)) - copy all queries onto one appendix see graph-tt\nHaving discussed some advantages of graph databases for representing interconnected data, this section delves into the specifics of a proposed graph data model tailored for university timetabling. This model serves as the foundation for exploring graph-based analysis and its potential to unlock valuable insights.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#an-iterative-approach",
    "href": "02-02-graph-timetable.html#an-iterative-approach",
    "title": "Graph Data Model for Timebling",
    "section": "An Iterative Approach",
    "text": "An Iterative Approach\nThe design of this graph data model was iterative: design, build, test, review, and revise -&gt; ‚Ä¶and repeat. Using Neo4j Desktop and an Aura cloud instance, the first model was small scope in scope, incorporating minimal nodes and properties. This iterative approach allows for flexibility and refinement based on real-world data and evolving requirements.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "href": "02-02-graph-timetable.html#core-nodes---building-blocks",
    "title": "Graph Data Model for Timebling",
    "section": "Core Nodes - Building Blocks",
    "text": "Core Nodes - Building Blocks\nAt its core, the timetable model revolves around four key entities represented as nodes:\n\n\n\n\nNode\nProperty\nDescription\nData Type\n\n\n\n\nStudent\nfirstName\nLegal first name\nstring\n\n\n\nlastName\nLegal last name\nstring\n\n\n\nstudentID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nLecturer\nfirstName\nFirst name\nstring\n\n\n\nlastName\nLast name\nstring\n\n\n\nstaffID\nUniversity identifier\ninteger\n\n\n\nsplusID\nTimetable URN\nstring\n\n\nRoom\nname\nRoom name\nstring\n\n\n\nsplusID\nTimetable URN\ninteger\n\n\nActivity\nname\nActivity name\nstring\n\n\n\ndescription\nActivity description\nstring\n\n\n\nstartTime\nScheduled start time\ndatetime\n\n\n\nendTime\nScheduled end time\ndatetime\n\n\n\ndate\nDate of activity\ndate\n\n\n\n\n\n\n\nNeo4j Interface showing basic nodes and properties",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "href": "02-02-graph-timetable.html#relationships---connecting-the-dots",
    "title": "Graph Data Model for Timebling",
    "section": "Relationships - Connecting the Dots",
    "text": "Relationships - Connecting the Dots\nThese core nodes are interconnected through relationships that reflect the dynamics of a timetable:\n\nStudent-[IS_ALLOCATED_TO]-&gt;Activity\nStaff-[TEACHES_ON]-&gt;Activity\nActivity-[TAKES_PLACE_IN]-&gt;Room\n\n\n\n\nCore Nodes and Properties",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graph Data Model for Timetabling"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html",
    "href": "01-03-good-timetable.html",
    "title": "What is a ‚Äúgood‚Äù timetable?",
    "section": "",
    "text": "TODO\n\n\n\n\nadd dowland, norman to references\nOne of the most challenging aspects of university timetabling is defining what constitutes a ‚Äúgood‚Äù timetable. Despite best efforts, it is virtually impossible to find universal satisfaction with a university timetable. The quality of a timetable is inherently subjective and varies among stakeholders depending on their preferences and competing demands on their time.\nBased on surveys across various institutions, students typically prioritise (Dowland, 2018; Norman, 2022):\nHowever, individual stakeholders often have conflicting priorities:\nThis divergence in preferences and the complex interplay of constraints make it challenging to define and achieve a universally ‚Äúgood‚Äù timetable (Lindahl et al., 2018). It is this complexity that sets the stage for ongoing research and improvement in the field of university timetabling.",
    "crumbs": [
      "Home",
      "Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "01-03-good-timetable.html#examples",
    "href": "01-03-good-timetable.html#examples",
    "title": "What is a ‚Äúgood‚Äù timetable?",
    "section": "Examples",
    "text": "Examples\n\n\n\n \nConsider the above timetables:\n\nThe first timetable is evenly spread over five days. Tuesday afternoons are heavily scheduled; Fridays have one two-hour activity over lunch.\nThe second timetable has two days free of activities (Wednesday and Thursday). Monday has a single activity at 18:00-19:00.\nThe third timetable has activities on five days. There are large gaps between activities on Tuesday and Wednesday.\n\nWhich timetable is better? Is any of them a ‚Äògood‚Äô timetable? The answer is it depends! or none of them!\n\n\n\n\nEvenly spread over 5 days. ¬†Tuesday afternoons are heavily scheduled; Fridays have one two-hour activity over lunch.\n\n\nTwo days free of activity (Wednesday and Thursday). ¬†Monday has a single activity at 18:00-19:00\n\n\nActivities on five days. ¬†There are large gaps between activities on Tuesday and Wednesday.",
    "crumbs": [
      "Home",
      "Introduction",
      "What is a Good Timetable?"
    ]
  },
  {
    "objectID": "01-01a-introduction.html",
    "href": "01-01a-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "TODO\n\n\n\n\nadd references and quote\nconsider adding ‚Äòoperational challenges - see 01-01a‚Äô\n\n\n\nUniversity timetabling is the complex process of scheduling curriculum elements (modules, programmes), rooms, and resources within an academic institution. At its core, it involves collecting and combining time slots, rooms, students, and other resources while satisfying a multitude of constraints and preferences to achieve a viable outcome. However, the reality of timetabling is far more intricate than this simple definition suggests.\nUniversity timetabling is the intricate puzzle of scheduling courses, rooms, and resources within the constraints of an academic calendar. It involves coordinating thousands of students, hundreds of modules, and diverse faculty needs to create a functioning schedule. This process is far from simple, as timetablers must juggle numerous hard constraints (e.g., room capacities, pre-assigned times) and soft constraints (e.g., staff preferences, student travel times) to reach a workable solution. The scale of this task, combined with interdependencies between scheduling decisions, makes university timetabling one of the most challenging administrative tasks in higher education.\nTimetabling shapes the daily experiences of students and staff, influences resource utilisation, and plays a significant role in institutional efficiency. The complexity of this task stems from various factors:\n\nScale: Tens of thousands of students, countless modules, and limited resources create a logistical nightmare.\nConstraints: Juggling hard limits (room capacities) and soft preferences (college desires) is a constant balancing act.\nInterdependencies: Changes in one part of the schedule can have cascading effects throughout the entire timetable.\nDiversity of Needs: Different organisational units (colleges, faculties, schools, departments) have varying requirements and preferences.\nOptimisation Goals: Timetablers must balance efficiency, fairness, and quality of education.\n\nWhile traditional approaches focus heavily on generating feasible timetables ‚Äì ensuring no clashes or rule violations ‚Äì this project explores a different facet: how analysing those timetables can lead to deeper insights and ultimately, improved quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": " ",
    "section": "",
    "text": "Abdipoor, S., Yaakob, R., Goh, S.L. and Abdullah, S. (2023) Meta-heuristic approaches for the University Course Timetabling Problem. Intelligent Systems with Applications [online]. 19, p.¬†200253. Available from: https://www.sciencedirect.com/science/article/pii/S2667305323000789 [Accessed 25 July 2024].\nAnon. (no date) CPB Projects [online]. Available from: https://www.cpbprojects.co.uk/solutions/timetabling-and-teaching-space [Accessed 25 July 2024a].\nBabaei, H., Karimpour, J. and Hadidi, A. (2015)‚ÄòA survey of approaches for university course timetabling problem‚Äô Computers & Industrial EngineeringApplications of Computational Intelligence and Fuzzy Logic to Manufacturing and Service Systems [online]. 86, pp.¬†43‚Äì59. Available from: https://www.sciencedirect.com/science/article/pii/S0360835214003714 [Accessed 28 July 2024].\nBellio, R., Ceschia, S., Di Gaspero, L., Schaerf, A. and Urli, T. (2016) Feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem. Computers & Operations Research [online]. 65, pp.¬†83‚Äì92. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0305054815001690 [Accessed 26 January 2024].\nBonutti, A., De Cesco, F., Di Gaspero, L. and Schaerf, A. (2012) Benchmarking curriculum-based course timetabling: formulations, data formats, instances, validation, visualization, and results. Annals of Operations Research [online]. 194 (1), pp.¬†59‚Äì70. Available from: https://doi.org/10.1007/s10479-010-0707-0 [Accessed 3 February 2024].\nBruggen, R. van (2014)‚ÄôLearning Neo4j: run blazingly fast queries on complex graph datasets with the power of the Neo4j graph database‚ÄôCommunity Experience Distilled. 1st edition. Birmingham, England: Packt Publishing.\nBurke, E., Mccollum, B., Meisels, A., Petrovic, S. and Qu, R. (2007) A graph-based hyper-heuristic for educational timetabling problems. European Journal of Operational Research [online]. 176, pp.¬†177‚Äì192.\nCeschia, S., Di Gaspero, L. and Schaerf, A. (2023) Educational timetabling: Problems, benchmarks, and state-of-the-art results. European Journal of Operational Research [online]. 308 (1), pp.¬†1‚Äì18. Available from: https://linkinghub.elsevier.com/retrieve/pii/S0377221722005641 [Accessed 25 January 2024].\nChicken, S., Fogg Rogers, L., Hobbs, L., Hunt-Fraisse, T. and Lewis, D. (2023) Amplifying the voices of neurodivergent students in relation to higher education assessment at UWE Bristol. [online]. Available from: https://uwe-repository.worktribe.com/output/10879555 [Accessed 25 July 2024].\nDammak, A., Elloumi, A. and Kamoun, H. (2007) An enterprise system component based on graph colouring for exam timetabling: A case study in a Tunisian university. Transforming Government: People, Process and Policy [online]. 1 (3), pp.¬†255‚Äì270. Available from: https://www.emerald.com/insight/content/doi/10.1108/17506160710778095/full/html [Accessed 19 February 2024].\nDon State Technical University, Rostov-on-Don, Russian Federation and Al-Gabri, W.M. (2017) Literature review for the topic of automation of scheduling classes and exams in higher education institutions. Vestnik of Don State Technical University [online]. 17 (1), pp.¬†132‚Äì143. Available from: https://vestnik.donstu.ru/jour/article/view/255 [Accessed 25 January 2024].\nDowland, D. (2018) Rubik‚Äôs cube or Battenburg? The university timetable Wonkhe. 11 January 2018 [online]. Available from: https://wonkhe.com/blogs/rubiks-cube-or-battenburg-the-university-timetable/ [Accessed 25 July 2024].\nFoung, D. and Chen, J. (2019) Discovering disciplinary differences: blending data sources to explore the student online behaviors in a University English course. Information Discovery and Delivery [online]. 47 (2), pp.¬†106‚Äì114. Available from: https://www.emerald.com/insight/content/doi/10.1108/IDD-10-2018-0053/full/html [Accessed 19 February 2024].\nHerres, B. and Schmitz, H. (2021) Decomposition of university course timetabling: A systematic study of subproblems and their complexities. Annals of Operations Research [online]. 302 (2), pp.¬†405‚Äì423. Available from: https://ezproxy.uwe.ac.uk/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=bsu&AN=150973572&site=ehost-live [Accessed 28 July 2024].\nHolm, D.S., Mikkelsen, R.√ò., S√∏rensen, M. and Stidsen, T.J.R. (2022) A graph-based MIP formulation of the International Timetabling Competition 2019. Journal of Scheduling [online]. 25 (4), pp.¬†405‚Äì428. Available from: https://doi.org/10.1007/s10951-022-00724-y [Accessed 2 November 2023].\nHutson, G. and Jackson, M. (2023) Graph Data Modeling in Python [online]. Packt Publishing. [Accessed 24 December 2023].\nJohnson, D. (1993) A Database Approach to Course Timetabling. The Journal of the Operational Research Society [online]. 44 (5), pp.¬†425‚Äì433. Available from: https://www.jstor.org.ezproxy.uwe.ac.uk/stable/2583909 [Accessed 28 July 2024].\nKhan, W., Kumar, T., Zhang, C., Raj, K., Roy, A.M. and Luo, B. (2023) SQL and NoSQL Database Software Architecture Performance Analysis and Assessments‚ÄîA Systematic Literature Review. Big Data and Cognitive Computing [online]. 7 (2), p.¬†97. Available from: https://www.mdpi.com/2504-2289/7/2/97 [Accessed 28 July 2024].\nLee, V., Nguyen, P.K. and Thomas, A. (2023) Graph-Powered Analytics and Machine Learning with TigerGraph [online]. [Accessed 4 November 2023].\nLemos, A., Melo, F.S., Monteiro, P.T. and Lynce, I. (2019) Room usage optimization in timetabling: A case study at Universidade de Lisboa. Operations Research Perspectives [online]. 6, p.¬†100092. Available from: https://linkinghub.elsevier.com/retrieve/pii/S2214716018301696 [Accessed 26 January 2024].\nLindahl, M., Mason, A.J., Stidsen, T. and S√∏rensen, M. (2018) A strategic view of University timetabling. European Journal of Operational Research [online]. 266 (1), pp.¬†35‚Äì45. Available from: https://www.sciencedirect.com/science/article/pii/S0377221717308433 [Accessed 28 July 2024].\nMandal, A.K. (2020) Development of an Interactive Tool based on Combining Graph Heuristic with Local Search for Examination Timetable Problem. International Journal of Advanced Computer Science and Applications [online]. 11 (3). Available from: https://www.proquest.com/docview/2655156280/abstract/33CF4A9244324D32PQ/1 [Accessed 18 July 2024].\nMirHassani, S.A. and Habibi, F. (2013) Solution approaches to the course timetabling problem. Artificial Intelligence Review [online]. 39 (2), pp.¬†133‚Äì149. Available from: http://link.springer.com/10.1007/s10462-011-9262-6 [Accessed 26 January 2024].\nM√ºhlenthaler, M. and Wanka, R. (2016) Fairness in academic course timetabling. Annals of Operations Research [online]. 239 (1), pp.¬†171‚Äì188. Available from: https://doi.org/10.1007/s10479-014-1553-2 [Accessed 3 February 2024].\nM√ºller, T. and Murray, K. (2010) Comprehensive approach to student sectioning. Annals of Operations Research [online]. 181 (1), pp.¬†249‚Äì269. Available from: http://link.springer.com/10.1007/s10479-010-0735-9 [Accessed 26 January 2024].\nNan 1, Z., Bai, X. 1 1 C. of I. and Economics, T.Y. (2019) The study on data migration from relational database to graph database. [online]. Available from: https://www.proquest.com/docview/2568058349?pq-origsite=primo [Accessed 4 November 2023].\nNegro, A. (2021) Graph-Powered Machine Learning [online]. O‚ÄôReilly Media, Inc.¬†[Accessed 4 November 2023].\nNeo4j (2023) The Neo4j Cypher Manual v5.\nNguyen, V.D. and Nguyen, T. (2021) An SHO-based approach to timetable scheduling: a case study. Journal of Information and Telecommunication [online]. 5 (4), pp.¬†421‚Äì439. Available from: https://doi.org/10.1080/24751839.2021.1935644 [Accessed 2 November 2023].\nNorman, R. and Williams, E. (no date) PSP Board Pack 220804 v1.3.pptx [online]. Available from: https://uweacuk-my.sharepoint.com/:p:/g/personal/richard2_norman_uwe_ac_uk/EWKNTqInQuRDoSaPmHCOp20B72Dp_2vYpJ__GbCzaY5tiA?email=Petter.Lovehagen%40uwe.ac.uk&e=4%3AwEUjpu&fromShare=true&at=31&CID=256ab09c-758c-97f4-07dd-ff61808257cf [Accessed 19 February 2024].\nOude Vrielink, R.A., Jansen, E.A., Hans, E.W. and Van Hillegersberg, J. (2019) Practices in timetabling in higher education institutions: a systematic review. Annals of Operations Research [online]. 275 (1), pp.¬†145‚Äì160. Available from: http://link.springer.com/10.1007/s10479-017-2688-8 [Accessed 2 November 2023].\nRudov√°, H., M√ºller, T. and Murray, K. (2011) Complex university course timetabling. Journal of Scheduling [online]. 14 (2), pp.¬†187‚Äì207. Available from: http://link.springer.com/10.1007/s10951-010-0171-3 [Accessed 26 January 2024].\nSanchez, C.A. (2015) An analytics based architecture and methodology for collaborative timetabling in higher education - ProQuest [online]. Available from: https://www.proquest.com/docview/1779550151?pq-origsite=primo&parentSessionId=GLad2hOIbVrF%2F0eiOKHGi%2BO%2BFOyV9GXuQTQCxSfgWNw%3D&sourcetype=Dissertations%20&%20Theses [Accessed 25 January 2024].\nScifo, E. (2023) Graph Data Science with Neo4j [online]. [Accessed 4 November 2023].\nSokolova, Marina V., Francisco J. G√≥mez, and Larisa N. Borisoglebskaya. ‚ÄòMigration from an SQL to a Hybrid SQL/NoSQL Data Model‚Äô. Journal of Management Analytics 7, no. 1 (March 2020): 1‚Äì11. https://doi.org/10.1080/23270012.2019.1700401.\nThomas, J.J., Khader, A.T. and Belaton, B. (2009) Visualization Techniques on the Examination Timetabling Pre-processing Data. In: Imaging and Visualization 2009 Sixth International Conference on Computer Graphics [online]Imaging and Visualization 2009 Sixth International Conference on Computer Graphics. pp.¬†454‚Äì458. Available from: https://ieeexplore.ieee.org/document/5298764 [Accessed 25 January 2024].\nWebber, J., Eifrem, E. and Robinson, I. (2013) Graph Databases [online]. [Accessed 4 November 2023].\nWikipedia contributors (2024) NP-hardness ‚Äî Wikipedia, the free encyclopedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=NP-hardness&oldid=1236371945.",
    "crumbs": [
      "Home",
      "Appendix & References",
      "References"
    ]
  },
  {
    "objectID": "01-02-background.html",
    "href": "01-02-background.html",
    "title": "Background and Motivation",
    "section": "",
    "text": "TODO\n\n\n\n\nreview - reduce?\nMy journey into the world of university timetabling began several years ago, first as a scheduler grappling with the complexities of timetable generation, and later transitioning into the role of a timetabling data manager. These experiences exposed me not only to the intricate data and systems involved but also to the intense pressures faced by timetabling teams. The constant scrutiny, stakeholder demands, and the near-impossibility of achieving universal satisfaction left an indelible mark, highlighting the critical need for better tools and metrics to understand and assess timetable quality ‚Äì a factor often overshadowed by the pursuit of mere feasibility.",
    "crumbs": [
      "Home",
      "Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "href": "01-02-background.html#research-gap-bridging-theory-and-practice",
    "title": "Background and Motivation",
    "section": "Research Gap: Bridging Theory and Practice",
    "text": "Research Gap: Bridging Theory and Practice\nMuch of the current research in university course timetabling (UCTTP) centres on optimisation algorithms, sophisticated techniques designed to efficiently generate feasible solutions given a set of constraints. This computationally-driven approach stems from the inherent difficulty of the UCTTP, often categorised as NP-hard1, meaning finding the absolute ‚Äúbest‚Äù timetable is exceptionally challenging (Babaei, Karimpour and Hadidi, 2015; Herres and Schmitz, 2021; Wikipedia contributors, 2024). Consequently, significant effort has been dedicated to developing algorithms like constraint programming (Holm et al., 2022) and local search techniques such as Tabu Search and simulated annealing (Oude Vrielink et al., 2019), aiming to create workable timetables within reasonable timeframes.\nHowever, this emphasis on computational optimisation often unfolds within the controlled environment of standardised datasets and predefined constraints. While crucial for advancing algorithmic development, these idealised scenarios may not fully capture the dynamic complexity of real-world university timetabling. Universities grapple with constantly shifting demands: fluctuating student popoulations, evolving school preferences, resource limitations, and the ever-present need to balance diverse stakeholder needs. These complexities extend beyond simply finding a feasible solution ‚Äì they necessitate tools to understand the trade-offs inherent in any timetable, enabling informed decisions about which ‚Äúgood‚Äù to prioritise (Lindahl, 2017). This is where I believe graph data structures offer unique potential. Timetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Graph databases excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable.\nWhile algorithms excel at generating solutions, there remains a significant gap in post-generation analysis ‚Äì the ability to delve into a timetable‚Äôs nuanced impacts on student and staff experience. Despite the acknowledged importance of factors like room allocation, teaching period distribution, and their effect on overall timetable quality, traditional optimisation-focused approaches often lack the tools to explore these relationships in depth (Ceschia, Di Gaspero and Schaerf, 2023; Lindahl, 2017; Rudov√°, M√ºller and Murray, 2011).",
    "crumbs": [
      "Home",
      "Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#motivation",
    "href": "01-02-background.html#motivation",
    "title": "Background and Motivation",
    "section": "Motivation",
    "text": "Motivation\nThis is where I believe graph data structures offer unique potential. Timetables are inherently about relationships: curriculum linked to lecturers, students connected through shared modules, rooms associated with specific times and capacities. Traditional data analysis methods often struggle to capture the richness of these interconnected elements. Graph databases, on the other hand, are specifically designed to excel in this domain, offering a way to unlock insights hidden within the complex web of a university timetable. This potential for deeper analysis, coupled with the limitations of optimisation-centric approaches, motivates this exploration of graph data structures for enhancing timetable understanding and, ultimately, improving timetable quality for all stakeholders.",
    "crumbs": [
      "Home",
      "Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-02-background.html#footnotes",
    "href": "01-02-background.html#footnotes",
    "title": "Background and Motivation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúIn¬†computational complexity theory, a computational problem¬†H¬†is called¬†NP-hard¬†if, for every problem¬†L¬†which can be solved in¬†non-deterministic polynomial-time, there is a¬†polynomial-time reduction¬†from¬†L¬†to¬†H.‚Äù (Wikipedia contributors, 2024)‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Introduction",
      "Background and Motivation"
    ]
  },
  {
    "objectID": "01-04-project.html",
    "href": "01-04-project.html",
    "title": "Project Aims and Scope",
    "section": "",
    "text": "TODO\n\n\n\n\nconsider changing scope section\n\n\n\nThe primary aim of this project is to investigate the viability of using graph data structures for enhanced timetabling analytics and reporting. By exploring this innovative approach, I hope to address some of the limitations of current timetabling methods and provide new insights into timetable quality assessment.\nKey objectives of this project include:\n\nDesigning an extensible, system-agnostic graph data model for university timetables\nDeveloping a mapping pipeline to transition from relational to graph database representations of timetables\nImplementing and evaluating a set of proof-of-concept analytical metrics leveraging the graph data model\nDiscussing the performance and capabilities of graph-based analytics against traditional relational database approaches\nExploring how graph-based approaches might contribute to measuring and improving timetable ‚Äòquality‚Äô from various stakeholder perspectives\n\nIt is important to note that this project is positioned as a proof of concept and exploratory study. I will not be implementing a full-scale timetabling system or focusing on real-time timetable generation or optimisation. Instead, the scope is limited to demonstrating the potential of graph-based approaches in enhancing understanding and analysis of university timetables.",
    "crumbs": [
      "Home",
      "Introduction",
      "Project Aims and Scope"
    ]
  },
  {
    "objectID": "02-03-graph-time.html",
    "href": "02-03-graph-time.html",
    "title": "Graphing Time",
    "section": "",
    "text": "TODO\n\n\n\n\nquote on normalisation (footnote)\nadd arrows for each model\nThe biggest challenge when modelling timetables into graph data structures involved temporal elements - that is, start and end times, dates, weeks, reccurences, durations, etc. Whether a data element is a node or a property of a node, or perhaps both is relatively trivial to model and test, and depends heavily on use cases. Time, on the other hand, is complicated, as I discovered when attempting to write aggregation queries on time elements.\nWhile the conceptual flexibility of graphs is appealing, finding the optimal balance between efficient representation, query performance, and data redundancy requires careful consideration. This section details some challenges encountered and the solutions explored in modeling time for a university timetable graph database.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#the-problem-of-normalised-time",
    "href": "02-03-graph-time.html#the-problem-of-normalised-time",
    "title": "Graphing Time",
    "section": "The Problem of Normalised Time",
    "text": "The Problem of Normalised Time\nTraditional relational databases often store timetable information in a highly normalised format, condensing recurring events into single rows with date ranges, week patterns, or lists of occurrences. While efficient for storage and basic display, this approach severely hinders analysis, especially when aiming to:\n\nIdentify Time-Based Patterns: Determining if students lack lunch breaks or experience excessive gaps between classes becomes difficult when time is fragmented across multiple fields.\nPerform Aggregations: Calculating total teaching hours for a lecturer across specific weeks or days requires complex queries and data transformations.\nModel Temporal Relationships: Representing relationships between activities based on their temporal proximity, such as students attending consecutive classes, becomes convoluted.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "02-03-graph-time.html#exploring-potential-solutions",
    "href": "02-03-graph-time.html#exploring-potential-solutions",
    "title": "Graphing Time",
    "section": "Exploring Potential Solutions",
    "text": "Exploring Potential Solutions\nTo address these challenges, several time modeling approaches were explored, each with its own trade-offs. Let‚Äôs explore with a fictional example for Introduction to Graph Databases :\n\nExample Source Data (Relational)\n\n\nName\nDay\nStartTime\nEndTime\nWeeks (11 weeks total)\n\n\n\n\nITGD\nWednesday\n09:00\n11:00\n1-3, 5-7, 9-13\n\n\n\n\nOption 1: Unique Activity Nodes\n\n\n\nUnique Activity Nodes Graph Example\n\n\nGraph Structure: 11 separate Activity nodes one for each occurrence (date)\n\nEach node has Date, StartTime, EndTime properties\nEssentially - this is ‚Äòun-normlising‚Äô the relational data\n\n(Activity {Name: \"ITGD\", Date: \"2024-01-03\", StartTime: \"09:00\", EndTime: \"11:00\"}) (Activity {Name: \"ITGD\", Date: \"2024-01-10\", StartTime: \"09:00\", EndTime: \"11:00\"}) ... (Activity {Name: \"ITGD\", Date: \"2024-03-20\", StartTime: \"09:00\", EndTime: \"11:00\"})\nPros:\n\nConceptual Simplicity: Easy to understand and implement.\nDirect Time Representation: Time is directly associated with each activity instance.\n\nCons:\n\nNode Proliferation: Leads to a high volume of nodes, potentially impacting performance with large datasets.\nComplex Time-Based Queries: Answering questions about time patterns or conflicts requires traversing numerous nodes and relationships.\n\n\n\nOption 2: Date and Time Nodes\n\n\n\nTime and Date Nodes\n\n\nGraph Structure: One Activity node for ‚ÄúIntro to Graph Databases‚Äù.\n\n11 Date nodes (for each Wednesday).\n2 Time nodes (09:00 and 11:00) - shared by ALL activities on those times!\nRelationships\n\nActivity -[:SCHEDULED_ON]-&gt; Date (11 relationships)\nDate -[:STARTS_AT]-&gt; Time (11 relationships to 09:00)\nDate -[:ENDS_AT]-&gt; Time (11 relationships to 11:00)\n\n\n(Activity {Name: \"ITGD\"}) -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"}) -[:STARTS_AT]-&gt; (Time {time: \"09:00\"})\n                                             (Date {date: \"2024-01-03\"}) -[:ENDS_AT]-&gt;   (Time {time: \"11:00\"})\n(Activity {Name: \"ITGD\"}) -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-10\"}) -[:STARTS_AT]-&gt; (Time {time: \"09:00\"}) // Same time node!\n                                             (Date {date: \"2024-01-10\"}) -[:ENDS_AT]-&gt;   (Time {time: \"11:00\"}) \n...\nKey point: Relationships encode which activity happens when.\nPros:\n\nIncreased Flexibility: Facilitates queries across time ranges and aggregations across time slots.\nReduced Redundancy: Avoids replicating time information for activities occurring on the same date and time.\nLower Node Count: Potentially fewer nodes overall compared to Option 1 as date and time nodes are shared with all activities in the database.\n\nCons:\n\nIncreased Model Complexity: Requires managing relationships between Activity, Date, and Time nodes.\nPotential Performance Overhead: Querying might involve traversing multiple relationships, impacting efficiency.\n\n\n\nOption 3: Date and Time Block Nodes\n\n\n\nTimeBlock and Date Nodes\n\n\nGraph Structure: Same as Option 2, but instead of individual Time nodes, we have TimeBlock nodes. - For example, if using 30-minute blocks, you would have a node for ‚Äú09:00-09:30‚Äù and another for ‚Äú09:30-10:00‚Äù, etc.\n(Activity {Name: \"ITGD\"}) -[:SCHEDULED_ON]-&gt; (Date {date: \"2024-01-03\"}) -[:OCCUPIES]-&gt; (TimeBlock {start: \"09:00\", end: \"09:30\"})\n                                                                          -[:OCCUPIES]-&gt; (TimeBlock {start: \"09:30\", end: \"10:00\"}) \n...\nPros:\n\nGranular Time Representation: Enables analysis at specific time intervals\nEasier Time Calculations: Duration is encoded and allows for easy calculations.\nReduced Node Count Compared to Option 2: Offers a balance between granularity and node proliferation.\n\nCons:\n\nPotential for Data Sparsity: Some time blocks might be sparsely populated, leading to storage inefficiencies.\nPotential for High Node Codes: Lots of TimeBlocks if using small intervals\n\n\n\nOption: Variations\nStartTime and Duration: This option simplifies the model by representing time using only StartTime and DurationInMinutes properties on the Activity node, omitting explicit EndTime nodes. This approach is suitable for duration based queries but it is limiting in that it is more difficult to query for events occurring at specific times, overlapping time ranges or on end-times.\nDynamic TimeBlocks: This variation does not precreate timeblocks based on a set interval (e.g.¬†30 minutes). They are created dynamically as required by the data and what already exists. For example, activities at 09:00-11:00, 10:30-11:30 and 11:00-12:00 would require these TimeBlocks:\n(Timeblock {name: \"09:00-11:00\", start: 09:00, end: 11:00, duration:120})\n(Timeblock {name: \"10:30-11:30\", start: 10:30, end: 11:30, duration:60})\n(Timeblock {name: \"11:00-12:00\", start: 11:00, end: 12:00, duration:60})\n\n\nSummary\n:\n\nOption summary\n\n\n\n\n\n\n\nOption\nPros\nCons\n\n\n\n\nUnique Activities\nSimple, direct\nHigh node count, complex time pattern queries\n\n\nDate & Time\nLower node count, good for time-based queries\nMore complex relationships\n\n\nDate & TimeBlock\nGranular, easier duration calculations\nPotentially high node count, sparsity if blocks are fine-grained\n\n\n\nGiven the proof-of-concept scope of this project, Option 1 (unique activity nodes for each date/time instance) was chosen. While this approach can lead to node proliferation, it offers the most straightforward implementation for exploring fundamental time-based queries and insights.\nFuture work could include evaluating the performance and scalability of different time modeling options, particularly:\n\nDynamic Node Creation (Option 2 or 3): This approach would create Time or TimeBlock nodes only when needed, potentially offering a good balance between flexibility and performance.\nDirect Performance Comparisons: Conducting benchmarks against specific use cases and datasets will provide valuable insights for choosing the optimal approach for large-scale deployments.\n\nModeling time effectively is crucial for unlocking the full potential of a graph database for university timetabling analysis. This section has outlined the challenges, explored potential solutions, and documented the chosen approach for this proof of concept.\nFurther exploration and optimisation of time modeling will be essential for developing robust, scalable, and insightful graph-based timetabling solutions. The next section will delve into the data engineering pipeline required to populate and maintain this model, bridging the gap between raw data and insightful analysis.",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Graphing Time"
    ]
  },
  {
    "objectID": "03-03-config.html",
    "href": "03-03-config.html",
    "title": "Configuration and Logging",
    "section": "",
    "text": "TODO\n\n\n\n\nrewrite detailed options into a summary format\nupdate if YAML config implemented.\nrevisit approach doc in final script folder\n\n\n\nThe configuration and logging approach was to centralise configuration parameters into a python scripts in order to allow the user to manage different aspects of the ETL pipeline. The configuration options are as a result of both initial design and discovery during development.\nWhen I set options during prototyping and testing, I considered whether these are worth parameterising in the ETL by weighing up cost versus value within a proof-of-concept scope. In general, my design is set up to run automatically or dynamically with well structured data, but I included options to override these settings.\n\nMain Configuration options\nAn YAML file containing configuration options can be viewed in the Appendix.\nGeneral settings include being able to filter which data to extract by way of a list of award codes. This is the default folder name for this ETL run, although default folder names, filepaths and directories can be overwritten and customised. This includes source data files and Google Drive folders for processed data.\nCredentials to SQL databases, Neo4j instances and Google drives are stored securely via environment variables or keyring secure storage.\nThere is an option to specify additional data sources, that is data which does not originate in the timetable database. This has been configured for optionally adding additional location data, e.g.¬†latitude and longitude, square meterage, etc.\n\n\nLogging\nI set up a mechanism which can create a separate loggers with ease. This can be expanded or contracted, as required - for example, during development I had sepaparate loggers for each stage of the ETL to allow me to understand errors. However, if the ETL was developed into a stable release, it can be configured to have one log.\nOptions include customising the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n\nExample Extract Log\nSetting up logger: extract\nLogger extract setup completed with 1 handlers.\nSetting up logger: process\nLogger process setup completed with 1 handlers.\nSetting up logger: load\nLogger load setup completed with 1 handlers.\n2024-07-04 11:26:01,124 - INFO - extract_main - Starting data extraction process\n2024-07-04 11:26:01,124 - INFO - extract_main - Output Directory: C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract\n2024-07-04 11:26:01,124 - INFO - extract_main - Hostkeys: ['INB112']\n2024-07-04 11:26:01,124 - INFO - extract_main - Chunksize: 20000\n2024-07-04 11:26:01,124 - INFO - extract_data - Starting data extraction process...\n2024-07-04 11:26:01,284 - INFO - Connected to SQL Server database.\n2024-07-04 11:26:01,284 - INFO - temp_table_loader - Creating global temporary tables...\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Processing SQL file: node-activity-by-pos-temp.sql\n2024-07-04 11:26:07,878 - INFO - extract_sql_file - Extracting data for node-activity-by-pos-temp with hostkey: INB112\nC:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\extract_sql_file.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  for chunk in pd.read_sql_query(query, conn, chunksize=CHUNK_SIZE):\n2024-07-04 11:26:07,960 - INFO - extract_sql_file - Saved 631 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activity-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Query for node-activity-by-pos-temp, INB112 took 0.09 seconds\n2024-07-04 11:26:07,966 - INFO - extract_sql_file - Processing SQL file: node-activityType-by-pos-temp.sql\n2024-07-04 11:26:07,968 - INFO - extract_sql_file - Extracting data for node-activityType-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Saved 16 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-activityType-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Query for node-activityType-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:07,985 - INFO - extract_sql_file - Processing SQL file: node-dept-all.sql\n2024-07-04 11:26:07,987 - INFO - extract_sql_file - Extracting data for node-dept-all with hostkey: INB112\n2024-07-04 11:26:07,990 - INFO - extract_sql_file - Saved 24 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-dept-all_INB112_1.csv\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Query for node-dept-all, INB112 took 0.01 seconds\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Processing SQL file: node-module-by-pos-temp.sql\n2024-07-04 11:26:07,995 - INFO - extract_sql_file - Extracting data for node-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Saved 42 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Query for node-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Processing SQL file: node-pos-by-pos-temp.sql\n2024-07-04 11:26:08,009 - INFO - extract_sql_file - Extracting data for node-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Saved 8 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Query for node-pos-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Processing SQL file: node-room-by-pos-temp.sql\n2024-07-04 11:26:08,020 - INFO - extract_sql_file - Extracting data for node-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Saved 44 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Query for node-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Processing SQL file: node-staff-by-pos-temp.sql\n2024-07-04 11:26:08,037 - INFO - extract_sql_file - Extracting data for node-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Saved 33 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Query for node-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Processing SQL file: node-student-by-pos-temp.sql\n2024-07-04 11:26:08,055 - INFO - extract_sql_file - Extracting data for node-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Saved 206 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/node-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Query for node-student-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Processing SQL file: rel-activity-module-by-pos-temp.sql\n2024-07-04 11:26:08,075 - INFO - extract_sql_file - Extracting data for rel-activity-module-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Saved 168 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-module-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Query for rel-activity-module-by-pos-temp, INB112 took 0.01 seconds\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Processing SQL file: rel-activity-room-by-pos-temp.sql\n2024-07-04 11:26:08,085 - INFO - extract_sql_file - Extracting data for rel-activity-room-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Saved 611 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-room-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Query for rel-activity-room-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Processing SQL file: rel-activity-staff-by-pos-temp.sql\n2024-07-04 11:26:08,105 - INFO - extract_sql_file - Extracting data for rel-activity-staff-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Saved 868 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-staff-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,120 - INFO - extract_sql_file - Query for rel-activity-staff-by-pos-temp, INB112 took 0.02 seconds\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Processing SQL file: rel-activity-student-by-pos-temp.sql\n2024-07-04 11:26:08,125 - INFO - extract_sql_file - Extracting data for rel-activity-student-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Saved 13423 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-activity-student-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,245 - INFO - extract_sql_file - Query for rel-activity-student-by-pos-temp, INB112 took 0.12 seconds\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Processing SQL file: rel-mod-pos-by-pos-temp.sql\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Extracting data for rel-mod-pos-by-pos-temp with hostkey: INB112\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Saved 82 rows to C:\\Users\\pho-lovehagen\\OneDrive - UWE Bristol\\Personal\\data-science\\graph-project\\code\\pipe3\\INB112\\extract/rel-mod-pos-by-pos-temp_INB112_1.csv\n2024-07-04 11:26:08,255 - INFO - extract_sql_file - Query for rel-mod-pos-by-pos-temp, INB112 took 0.00 seconds\n2024-07-04 11:26:08,265 - INFO - extract_data - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Data extraction completed.\n2024-07-04 11:26:08,285 - INFO - extract_main - Extraction Time Summary:\n2024-07-04 11:26:08,285 - INFO - extract_main - Function load_temp_tables took 6.59 seconds\n2024-07-04 11:26:08,285 - INFO - extract_main - Function main took 7.17 seconds\n\n\n\nExample Google Drive Log\n2024-07-11 13:28:22,339 - INFO - gdrive_upload - Starting Google Drive upload process.\n2024-07-11 13:28:22,789 - INFO - gdrive_upload - Found existing folder: INB112\n2024-07-11 13:28:23,115 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:23,366 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:23,576 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:24,810 - INFO - gdrive_upload - File node-activity-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:25,046 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:26,170 - INFO - gdrive_upload - File node-activityType-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:26,394 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:27,495 - INFO - gdrive_upload - File node-department-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:27,715 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:28,970 - INFO - gdrive_upload - File node-module-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:29,206 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:30,294 - INFO - gdrive_upload - File node-programme-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:30,515 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:31,595 - INFO - gdrive_upload - File node-room-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:31,846 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:32,895 - INFO - gdrive_upload - File node-staff-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:33,128 - INFO - gdrive_upload - Found existing folder: node\n2024-07-11 13:28:34,220 - INFO - gdrive_upload - File node-student-processed.csv uploaded to Google Drive folder ID: 1Rc3vQCF6CwxV3yNjfUTWXv61BgYD1j_3\n2024-07-11 13:28:34,220 - WARNING - gdrive_upload - Skipping file with unrecognized prefix: process.log\n2024-07-11 13:28:34,416 - INFO - gdrive_upload - Found existing folder: relationship\n2024-07-11 13:28:35,565 - INFO - gdrive_upload - File rel-activity_activityType-processed.csv uploaded to Google Drive folder ID: 1w_ea6ETzRcdYz71crLxL9khjLrEfcbuH\n\nAs part of the logging functionality, I created a timing function which tracks and stores various execution and elapsed times with a view to optimising performance or identifying bottlenecks in future development.\n\n\nExample Process Log\n2024-07-11 13:31:24,284 - INFO - process_utils - Processing relationship data\n2024-07-11 13:31:24,284 - INFO - process_utils - Config: {'filename_pattern': 'rel-mod-pos-by-pos-temp_INB112*.csv', 'node1_col': 'modSplusID', 'node2_col': 'posSplusID', 'relationship': 'BELONGS_TO', 'properties': ['modType']}\n2024-07-11 13:31:24,285 - INFO - process_utils - Saving processed relationship file for: module_programme\n2024-07-11 13:31:24,289 - INFO - process_main - Function process_department took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_module took 0.01 seconds\n2024-07-11 13:31:24,292 - INFO - process_main - Function process_room took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_programme took 0.01 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activityType took 0.02 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_staff took 1.82 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_student took 8.76 seconds\n2024-07-11 13:31:24,294 - INFO - process_main - Function process_activity took 0.10 seconds\n\nLogging also summarises loading results as can be seen in the snippet below.\n\n\nExample Load Log\n2024-07-11 16:28:45,220 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:28:45,220 - INFO - load_main - Total execution time: 0.51 seconds\n2024-07-11 16:33:31,166 - INFO - connect_to_neo4j_db - Connected to Neo4j database successfully! Driver: \n2024-07-11 16:33:31,601 - INFO - load_relationships - Found 1 relationship files in Google Drive.\n2024-07-11 16:33:31,604 - INFO - load_relationships - Processing file: rel-activity_room-processed.csv (ID: 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v)\n2024-07-11 16:33:31,605 - INFO - google_drive_utils - Downloading CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v from Google Drive.\n2024-07-11 16:33:35,665 - INFO - google_drive_utils - Downloaded CSV file 1fwUicLoaJ5YJk9tZOpDwyNMFAFWh2Q5v.\n2024-07-11 16:33:45,428 - INFO - load_relationships - Finished loading activity_room relationships:\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Total rows processed: 611\n2024-07-11 16:33:45,430 - INFO - load_relationships -   Relationships created: 1\n2024-07-11 16:33:45,431 - INFO - load_main - Neo4j driver closed.\n2024-07-11 16:33:45,432 - INFO - load_main - Total execution time: 14.34 seconds",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Configuration and Logging"
    ]
  },
  {
    "objectID": "04-timetable-metrics.html",
    "href": "04-timetable-metrics.html",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts",
    "crumbs": [
      "Home",
      "Timetable Metrics"
    ]
  },
  {
    "objectID": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "href": "04-timetable-metrics.html#timetable-quality-metrics-and-insights-1500-2000-words",
    "title": "Timetable Metrics",
    "section": "",
    "text": "Constraint violations (max hours per day, days per week, lunch breaks, etc.)\nDistance-based metrics using room properties\n\n\n\n\n\nStudent-level, programme-level, and other relevant groupings\n\n\n\n\n\nExample queries with explanations\n\n\n\n\n\nBloom visualisations or other relevant charts",
    "crumbs": [
      "Home",
      "Timetable Metrics"
    ]
  },
  {
    "objectID": "06-conclusion.html",
    "href": "06-conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Conclusion"
    ]
  },
  {
    "objectID": "06-conclusion.html#conclusion-500-words",
    "href": "06-conclusion.html#conclusion-500-words",
    "title": "Conclusion",
    "section": "",
    "text": "Summary of key achievements\nReflection on the project‚Äôs impact and potential for timetabling processes\nFuture work and recommendations",
    "crumbs": [
      "Home",
      "Final Thoughts",
      "Conclusion"
    ]
  },
  {
    "objectID": "appendix-supervisor.html",
    "href": "appendix-supervisor.html",
    "title": "Supervisor notes",
    "section": "",
    "text": "TODO\n\n\n\n\nwrite up section - review notes"
  },
  {
    "objectID": "03-01-overview.html",
    "href": "03-01-overview.html",
    "title": "Data Engineering Overview",
    "section": "",
    "text": "TODO\n\n\n\n\nadd references - neo4j, graphviz",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-01-overview.html#overview-of-the-data-pipeline",
    "href": "03-01-overview.html#overview-of-the-data-pipeline",
    "title": "Data Engineering Overview",
    "section": "Overview of the Data Pipeline",
    "text": "Overview of the Data Pipeline\nThe data engineering pipeline is designed to efficiently and securely transfer selected university timetabling data from a relational database (MS SQL) to a graph database (Neo4j), enabling advanced analytics and insights.\nThis section provides an overview of the pipeline architecture, fundamental design principles, implementation approach and key learning takeaways.\n\nHigh-level Architecture\nThe data pipeline consists of these core stages:\n\nExtraction: Data is extracted from the SQL database and saved into CSV files.\nTransformation: The CSV files are processed, cleaned, transformed, merged, and anonymised using Python code.\nIntermediate Storage: Processed CSVs are saved locally and uploaded to Google Drive (required for Neo4j Aura free instance).\nLoading: Clean data is processed and loaded into Neo4j.\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\ncluster_D_E\n\n\n\nA\n\n\nSQL Database\n\n\n\nB\n\nCSV Files\n\n\n\nA-&gt;B\n\n\n 1. ùóòùó´ùóßùó•ùóîùóñùóß: filter\n\n\n\nC\n\nProcessed CSV Files\n\n\n\nB-&gt;C\n\n\n 2. ùóßùó•ùóîùó°ùó¶ùóôùó¢ùó•ùó†: Validate, Process & Anonymise\n\n\n\nD\n\nGoogle Drive\n\n\n\nC-&gt;D\n\n\n 3. ùêîùêèùêãùêéùêÄùêÉ\n\n\n\nE\n\n\nNeo4j Aura DB\n\n\n\n\nD:w-&gt;E\n\n\n 4. Load Schema\n\n\n\nD:e-&gt;E\n\n\n 5. ùóüùó¢ùóîùóó: Load & Validate Data\n\n\n\n\n\n\n\n\n\nData Pipeline Overview\n\n\n\nFigure¬†1\n\n\n\n\n\nDesign Principles\nThis pipeline represents a comprehensive approach to data engineering, incorporating several best practices in data handling, processing, and database management.\n\n\n\nDesign Principles\n\n\nThe data pipeline is built on several core design principles. I started with a strong sense of what I wanted to achieve - a modular, scalable, secure and configurable design - however, what exactly this meant was discovered during the development process.\nGiven that my project bound by time and word-limits and has additional resource and technology constraints, it was important to make the final artefact one which can be built upon after submission, including potential further development within operational contexts.\nHowever, the project is also a proof-of-concept and as such, some design opportunities were eschewed in favour of simplicity and progress.\n\nSecurity and Data Protection\n\n\nSecure access controls\nData anonymisation\nControlled handling of personally identifiable information\n\n\n\nModularity, Scalability and Automation\n\n\nDistinct, interoperable modules (extract, transform, load)\nAbility to handle increased data volume and complexity\nAutomation, where possible\nConfigurable data processing options (e.g., data chunking, row processing)\nOptimised, where possible\n\n\n\nError Handling and Logging\n\n\nRobust error handling\nComprehensive logging for troubleshooting and auditing\n\n\n\nUser configurable\n\n\nFlexible configuration options for data filtering, directory controls, and schema handling\n\n\n\n\nImplementation Approach\nThe pipeline was developed using an iterative approach, allowing for continuous discovery, refinement and improvement.\nCrucial aspects of the implementation include:\n\nTechnology Stack: Python for data processing, MS SQL for source data, Neo4j for the target graph database. See Appendix for more details.\nCloud Integration: Utilisation of Google Drive for intermediate storage, compatible with Neo4j Aura.\nValidation: Implemented at various stages to ensure data integrity and fitness for processing.\nTesting: Continuous simulated unit testing to ensure that componentsare behaving as expected.\n\n\n\nUpcoming Sections\nThe following sections will delve into the specific implementation details of each stage in the pipeline, demonstrating how these principles are put into practice.\nI will explore the iterative development process, configuration management, extraction techniques, transformation processes, loading strategies, and automation workflows.\nFinally, I will reflect on lessons learned and potential future enhancements to the data engineering components.",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Overview"
    ]
  },
  {
    "objectID": "03-06a-load.html",
    "href": "03-06a-load.html",
    "title": "Google Load",
    "section": "",
    "text": "TODO\n\n\n\n\nadd log files?\nadd screenshot?\n\n\n\nAs I am using a free instance of Neo4j‚Äôs graph database - called Neo4j Aura - I needed to overcome some limitations which are not relevant to desktop installations of Neo4j or paid-for cloud instances.\nIn order to load from csv files, Neo4j Aura requires that the csv files are stored in public cloud storage like Google Drive or Dropbox. Therefore, my project requires this intermediary step.\n\n\n\n\n\n\n\ngoogle_drive_storage\n\n\n\nsource_files\n\nLocal Files\n(./{hostkeys}/process)\n\n\n\ngdrive_api\n\n\n\nGoogle Drive API\n\n\n\nsource_files-&gt;gdrive_api\n\n\nConnect via API\n\n\n\nget_folder\n\nGet Folder Details\n\n\n\ngdrive_api-&gt;get_folder\n\n\n\n\n\ncreate_folder\n\nCreate Folder if Needed\n\n\n\nget_folder-&gt;create_folder\n\n\n\n\n\nupload_nodes\n\nUpload Files\nMatching Node Pattern\nto './{hostkeys}/node'\n\n\n\ncreate_folder-&gt;upload_nodes\n\n\n\n\n\nupload_relationships\n\nUpload Files\nMatching Relationship Pattern\nto './{hostkeys}/relationships'\n\n\n\ncreate_folder-&gt;upload_relationships\n\n\n\n\n\nprocess_done\n\nProcess Complete\n\n\n\nupload_nodes-&gt;process_done\n\n\n\n\n\nupload_relationships-&gt;process_done\n\n\n\n\n\n\n\n\n\n\nConfiguration settings determine where processed node and relationship files are stored. I have made one folder in my drive public and all ETL files are stored within this root as follows:\n\nroot Google Drive folder\n\nhostkeys (automatically created, unless override)\n\nnodes\nrelationships\n\n\n\n\n\n\nScreenshot of Google Drive",
    "crumbs": [
      "Home",
      "Data Pipeline",
      "Google Drive Load"
    ]
  },
  {
    "objectID": "appendix-anonymise.html",
    "href": "appendix-anonymise.html",
    "title": "Anonymisation",
    "section": "",
    "text": "TODO\n\n\n\n\nwrtie a summary\nshow before and after\nstaff/student\ncolumns\nconsistent changing - uses random seed to ensure\n\n\n\n\n\nClick to show code\nimport random\nimport hashlib\nfrom faker import Faker\nimport pandas as pd\n\n\ndef anonymise_data(df):\n    \"\"\"\n    Anonymises a DataFrame by generating fake names, emails, and IDs.\n    \"\"\"\n    process_logger.info(\"Starting anonymisation\")\n    process_logger.info(f\"Columns in dataframe: {df.columns.tolist()}\")\n    \n    # Determine if it's staff or student data\n    if 'staffSplusID' in df.columns:\n        process_logger.info(\"Processing staff data\")\n        id_col = 'staffID'\n        prefix = 'staff'\n        columns_to_remove = ['staffFullName', 'staffLastName', 'staffForenames', 'staffID']\n    elif 'stuSplusID' in df.columns:\n        process_logger.info(\"Processing student data\")\n        id_col = 'studentID'\n        prefix = 'stu'\n        columns_to_remove = ['stuFullName', 'stuLastName', 'stuForenames', 'studentID']\n    else:\n        process_logger.error(\"Neither 'staffSplusID' nor 'stuSplusID' found in columns.\")\n        return df  # Return original dataframe if required columns are missing\n\n    # Create a dictionary to store anonymised data\n    anon_data = {}\n    \n    # Generate anonymised data for each unique ID\n    for unique_id in df[id_col].unique():\n        # Create a seed based on the unique_id\n        seed = int(hashlib.md5(str(unique_id).encode()).hexdigest(), 16) & 0xFFFFFFFF\n        fake = Faker()\n        fake.seed_instance(seed)\n        random.seed(seed)\n\n        first_name = fake.first_name()\n        last_name = fake.last_name()\n        full_name = f\"{first_name} {last_name}\"\n        email = f\"{first_name.lower()}.{last_name.lower()}@fakemail.ac.uk\"\n        anon_id = f\"{prefix}-{random.randint(10000000, 99999999):08d}\"\n        \n        anon_data[unique_id] = {\n            f'{prefix}FirstName_anon': first_name,\n            f'{prefix}LastName_anon': last_name,\n            f'{prefix}FullName_anon': full_name,\n            f'{prefix}Email_anon': email,\n            f'{prefix}ID_anon': anon_id\n        }\n    \n    # Create a new DataFrame with anonymised data\n    df_anon = pd.DataFrame.from_dict(anon_data, orient='index')\n    \n    # Reset the index and rename it to match the original ID column\n    df_anon = df_anon.reset_index().rename(columns={'index': id_col})\n    \n    try:\n        # Merge anonymised data with the original DataFrame\n        df_result = pd.merge(df, df_anon, on=id_col)\n        \n        # Remove columns that should be anonymised\n        columns_to_remove = [col for col in columns_to_remove if col in df_result.columns]\n        df_result = df_result.drop(columns=columns_to_remove)\n        \n        process_logger.info(\"Anonymisation completed successfully\")\n        return df_result\n\n    except Exception as e:\n        process_logger.error(f\"Error during anonymisation: {str(e)}\")\n        return df  # Return original dataframe if an error occurs",
    "crumbs": [
      "Home",
      "Appendix & References",
      "Anonymisation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Graph Data Models for Timetabling Insights",
    "section": "",
    "text": "Supervisor: Xiaodong Li\nProgramme: MSc Data Science\nWord Count: [add when finished]\n\n\n\n\n\n\n\nG\n\n\n\n4zj99lz2\n\n\n\n\n666n359b\n\n\n\n\nu0upp5hj\n\n\n\n\n666n359b-&gt;u0upp5hj\n\n\n\n\n\n4s3xzocl\n\n\n\n\nk8yaaeoi\n\n\n\n\n4s3xzocl-&gt;k8yaaeoi\n\n\n\n\n\n9bbhq0e8\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n4s3xzocl-&gt;9bbhq0e8\n\n\n\n\n\n5cymetka\n\n\n\n\ne4qukjnm\n\n\n\n\n5cymetka-&gt;e4qukjnm\n\n\n\n\n\nhx0hp6dr\n\n\n\n\nhx0hp6dr-&gt;hx0hp6dr\n\n\n\n\n\nk8yaaeoi-&gt;k8yaaeoi\n\n\n\n\n\n14ae1pfz\n\n\n\n\nk8yaaeoi-&gt;14ae1pfz\n\n\n\n\n\njrtct42d\n\n\n\n\nglv43ych\n\n\n\n\nxpzdirk1\n\n\n\n\nglv43ych-&gt;xpzdirk1\n\n\n\n\n\n06x4bmzo\n\n\n\n\nglv43ych-&gt;06x4bmzo\n\n\n\n\n\nmis1btmm\n\n\n\n\nglv43ych-&gt;mis1btmm\n\n\n\n\n\nce1516j0\n\n\n\n\nglv43ych-&gt;ce1516j0\n\n\n\n\n\ngzxrlpod\n\n\n\n\nc114j2tw\n\n\n\n\ngzxrlpod-&gt;c114j2tw\n\n\n\n\n\n8bpimk72\n\n\n\n\n8bpimk72-&gt;k8yaaeoi\n\n\n\n\n\n8bpimk72-&gt;14ae1pfz\n\n\n\n\n\nur4yo3tx\n\n\n\n\n8bpimk72-&gt;ur4yo3tx\n\n\n\n\n\nonkctu6x\n\n\n\n\nonkctu6x-&gt;onkctu6x\n\n\n\n\n\n85hi4ca2\n\n\n\n\nonkctu6x-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep\n\n\n\n\nonkctu6x-&gt;x0n5twep\n\n\n\n\n\nc1sdjw8j\n\n\n\n\nc1sdjw8j-&gt;hx0hp6dr\n\n\n\n\n\nc1sdjw8j-&gt;u0upp5hj\n\n\n\n\n\nxh1xin78\n\n\n\n\nxztqibcz\n\n\n\n\nxh1xin78-&gt;xztqibcz\n\n\n\n\n\nvr6qql5z\n\n\n\n\nvr6qql5z-&gt;666n359b\n\n\n\n\n\nwxhhelge\n\n\n\n\nvr6qql5z-&gt;wxhhelge\n\n\n\n\n\n3ytehise\n\n\n\n\nvr6qql5z-&gt;3ytehise\n\n\n\n\n\n2m5wokx2\n\n\n\n\nmfp9is99\n\n\n\n\n2m5wokx2-&gt;mfp9is99\n\n\n\n\n\nv4x5duoh\n\n\n\n\nv4x5duoh-&gt;2m5wokx2\n\n\n\n\n\nfovglxww\n\n\n\n\nv4x5duoh-&gt;fovglxww\n\n\n\n\n\nv4x5duoh-&gt;xztqibcz\n\n\n\n\n\n89w9dxkj\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n89w9dxkj-&gt;glv43ych\n\n\n\n\n\n1chkeldv\n\n\n\n\n89w9dxkj-&gt;1chkeldv\n\n\n\n\n\n89w9dxkj-&gt;mis1btmm\n\n\n\n\n\nxpzdirk1-&gt;gzxrlpod\n\n\n\n\n\nfovglxww-&gt;fovglxww\n\n\n\n\n\nfsp8cdjo\n\n\n\n\nfovglxww-&gt;fsp8cdjo\n\n\n\n\n\nfovglxww-&gt;mfp9is99\n\n\n\n\n\njnazmo3s\n\n\n\n\njnazmo3s-&gt;e4qukjnm\n\n\n\n\n\n72a2d6xd\n\n\n\n\nsj35t4ss\n\n\n\n\n72a2d6xd-&gt;sj35t4ss\n\n\n\n\n\nlfwuczry\n\n\n\n\n35yrnabr\n\n\n\n\nlfwuczry-&gt;35yrnabr\n\n\n\n\n\n8cgoonso\n\n\n\n\nlfwuczry-&gt;8cgoonso\n\n\n\n\n\nhv8k4g84\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nlfwuczry-&gt;hv8k4g84\n\n\n\n\n\nhcqaqm1o\n\n\n\n\nhcqaqm1o-&gt;666n359b\n\n\n\n\n\nqy17mwag\n\n\n\n\nhcqaqm1o-&gt;qy17mwag\n\n\n\n\n\n53bgisfb\n\n\n\n\nhcqaqm1o-&gt;53bgisfb\n\n\n\n\n\n1k9klz8v\n\n\n\n\neacs5e9j\n\n\n\n\n1k9klz8v-&gt;eacs5e9j\n\n\n\n\n\nj812m8am\n\n\n\n\n1k9klz8v-&gt;j812m8am\n\n\n\n\n\nieb7bdce\n\n\n\n\n1k9klz8v-&gt;ieb7bdce\n\n\n\n\n\nbav7hkue\n\n\n\n\nc114j2tw-&gt;bav7hkue\n\n\n\n\n\ne7injpqb\n\n\n\n\nc114j2tw-&gt;e7injpqb\n\n\n\n\n\nhj4rycy9\n\n\n\n\ns3okac0a\n\n\n\n\nhj4rycy9-&gt;s3okac0a\n\n\n\n\n\n85hi4ca2-&gt;hj4rycy9\n\n\n\n\n\n85hi4ca2-&gt;9bbhq0e8\n\n\n\n\n\nakih91pi\n\n\n\n\n85hi4ca2-&gt;akih91pi\n\n\n\n\n\nwxhhelge-&gt;72a2d6xd\n\n\n\n\n\nwxhhelge-&gt;53bgisfb\n\n\n\n\n\niw0pfpmk\n\n\n\n\nwt9rtovp\n\n\n\n\niw0pfpmk-&gt;wt9rtovp\n\n\n\n\n\ndtmww06j\n\n\n\n\niw0pfpmk-&gt;dtmww06j\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;onkctu6x\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nx0n5twep-&gt;85hi4ca2\n\n\n\n\n\nw084j1ko\n\n\n\n\nx0n5twep-&gt;w084j1ko\n\n\n\n\n\ny0sap20o\n\n\n\n\ny0sap20o-&gt;y0sap20o\n\n\n\n\n\ndxppp23s\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\ny0sap20o-&gt;dxppp23s\n\n\n\n\n\npveq6y3l\n\n\n\n\ny0sap20o-&gt;pveq6y3l\n\n\n\n\n\nzefyvzdp\n\n\n\n\npzodwhlw\n\n\n\n\nhxejkaog\n\n\n\n\npzodwhlw-&gt;hxejkaog\n\n\n\n\n\n9bbhq0e8-&gt;dxppp23s\n\n\n\n\n\nm0ggeeei\n\n\n\n\n9bbhq0e8-&gt;m0ggeeei\n\n\n\n\n\n06x4bmzo-&gt;c114j2tw\n\n\n\n\n\nv850hpzb\n\n\n\n\n06x4bmzo-&gt;v850hpzb\n\n\n\n\n\n8bx5ks88\n\n\n\n\n06x4bmzo-&gt;8bx5ks88\n\n\n\n\n\nzyz74rqy\n\n\n\n\nzyz74rqy-&gt;hx0hp6dr\n\n\n\n\n\nzyz74rqy-&gt;c1sdjw8j\n\n\n\n\n\n2cicgpkb\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nzyz74rqy-&gt;2cicgpkb\n\n\n\n\n\nc21k3g41\n\n\n\n\nzyz74rqy-&gt;c21k3g41\n\n\n\n\n\n6e82t0az\n\n\n\n\nsi800de6\n\n\n\n\nsi800de6-&gt;fovglxww\n\n\n\n\n\nsi800de6-&gt;si800de6\n\n\n\n\n\n4rv1fae9\n\n\n\n\n4rv1fae9-&gt;v4x5duoh\n\n\n\n\n\ndogjcfze\n\n\n\n\n4rv1fae9-&gt;dogjcfze\n\n\n\n\n\n4rv1fae9-&gt;dtmww06j\n\n\n\n\n\n4rv1fae9-&gt;3ytehise\n\n\n\n\n\nqvvjbgxm\n\n\n\n\nqy17mwag-&gt;vr6qql5z\n\n\n\n\n\nkjba91s3\n\n\n\n\nqy17mwag-&gt;kjba91s3\n\n\n\n\n\n9ifdi52l\n\n\n\n\ns3tdzgmu\n\n\n\n\n9ifdi52l-&gt;s3tdzgmu\n\n\n\n\n\n14ae1pfz-&gt;eacs5e9j\n\n\n\n\n\nhxejkaog-&gt;s3tdzgmu\n\n\n\n\n\nlijfw7my\n\n\n\n\nlijfw7my-&gt;4zj99lz2\n\n\n\n\n\neacs5e9j-&gt;k8yaaeoi\n\n\n\n\n\nur4yo3tx-&gt;hxejkaog\n\n\n\n\n\n1tdjps87\n\n\n\n\nur4yo3tx-&gt;1tdjps87\n\n\n\n\n\nl9axxrot\n\n\n\n\nur4yo3tx-&gt;l9axxrot\n\n\n\n\n\nfsp8cdjo-&gt;v4x5duoh\n\n\n\n\n\nfsp8cdjo-&gt;fovglxww\n\n\n\n\n\n9xro7toh\n\n\n\n\n4pocwaxo\n\n\n\n\nakih91pi-&gt;4pocwaxo\n\n\n\n\n\nakih91pi-&gt;3ytehise\n\n\n\n\n\nv850hpzb-&gt;89w9dxkj\n\n\n\n\n\nv850hpzb-&gt;qvvjbgxm\n\n\n\n\n\nv850hpzb-&gt;e7injpqb\n\n\n\n\n\ne4qukjnm-&gt;u0upp5hj\n\n\n\n\n\n1bkcwtuf\n\n\n\n\n1bkcwtuf-&gt;c114j2tw\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;06x4bmzo\n\n\n\n\n\n1bkcwtuf-&gt;lijfw7my\n\n\n\n\n\ndvhx7p1e\n\n\n\n\n1bkcwtuf-&gt;dvhx7p1e\n\n\n\n\n\nu0upp5hj-&gt;hcqaqm1o\n\n\n\n\n\nu0upp5hj-&gt;wxhhelge\n\n\n\n\n\nu0upp5hj-&gt;c21k3g41\n\n\n\n\n\nj33a36gy\n\n\n\n\nj33a36gy-&gt;9ifdi52l\n\n\n\n\n\nj33a36gy-&gt;j812m8am\n\n\n\n\n\newthkxdz\n\n\n\n\nefvvduxt\n\n\n\n\nefvvduxt-&gt;onkctu6x\n\n\n\n\n\nefvvduxt-&gt;4pocwaxo\n\n\n\n\n\ntwne0skc\n\n\n\n\ntwne0skc-&gt;xpzdirk1\n\n\n\n\n\ntwne0skc-&gt;1bkcwtuf\n\n\n\n\n\n2cicgpkb-&gt;l9axxrot\n\n\n\n\n\nozdu79aw\n\n\n\n\nozdu79aw-&gt;hx0hp6dr\n\n\n\n\n\nozdu79aw-&gt;1tdjps87\n\n\n\n\n\nj812m8am-&gt;j33a36gy\n\n\n\n\n\nsj35t4ss-&gt;72a2d6xd\n\n\n\n\n\nsj35t4ss-&gt;53bgisfb\n\n\n\n\n\n1tdjps87-&gt;8bpimk72\n\n\n\n\n\n1tdjps87-&gt;8cgoonso\n\n\n\n\n\nqcsg1epp\n\n\n\n\nqcsg1epp-&gt;k8yaaeoi\n\n\n\n\n\nkjba91s3-&gt;53bgisfb\n\n\n\n\n\nmis1btmm-&gt;c114j2tw\n\n\n\n\n\nmis1btmm-&gt;efvvduxt\n\n\n\n\n\n35yrnabr-&gt;4zj99lz2\n\n\n\n\n\n35yrnabr-&gt;y0sap20o\n\n\n\n\n\n35yrnabr-&gt;dvhx7p1e\n\n\n\n\n\nwmnsxqhi\n\n\n\n\nwmnsxqhi-&gt;wxhhelge\n\n\n\n\n\ntyoit3iq\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;v4x5duoh\n\n\n\n\n\ntyoit3iq-&gt;fovglxww\n\n\n\n\n\ntyoit3iq-&gt;1chkeldv\n\n\n\n\n\n8cgoonso-&gt;j33a36gy\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\n8bx5ks88-&gt;mis1btmm\n\n\n\n\n\nez7k5sb5\n\n\n\n\nez7k5sb5-&gt;72a2d6xd\n\n\n\n\n\nez7k5sb5-&gt;9ifdi52l\n\n\n\n\n\nieb7bdce-&gt;xh1xin78\n\n\n\n\n\n4pocwaxo-&gt;85hi4ca2\n\n\n\n\n\n4pocwaxo-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz\n\n\n\n\n4pocwaxo-&gt;qez5iiiz\n\n\n\n\n\nmfp9is99-&gt;wt9rtovp\n\n\n\n\n\nmfp9is99-&gt;1chkeldv\n\n\n\n\n\nyezs4jbg\n\n\n\n\nyezs4jbg-&gt;tyoit3iq\n\n\n\n\n\nyezs4jbg-&gt;w084j1ko\n\n\n\n\n\nqez5iiiz-&gt;onkctu6x\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\nqez5iiiz-&gt;4pocwaxo\n\n\n\n\n\ns3okac0a-&gt;5cymetka\n\n\n\n\n\ns3okac0a-&gt;vr6qql5z\n\n\n\n\n\ns3okac0a-&gt;72a2d6xd\n\n\n\n\n\n1pk6uryz\n\n\n\n\n1pk6uryz-&gt;xh1xin78\n\n\n\n\n\n1pk6uryz-&gt;2m5wokx2\n\n\n\n\n\n1pk6uryz-&gt;v4x5duoh\n\n\n\n\n\n1pk6uryz-&gt;fovglxww\n\n\n\n\n\n1pk6uryz-&gt;iw0pfpmk\n\n\n\n\n\n1pk6uryz-&gt;1chkeldv\n\n\n\n\n\nhv8k4g84-&gt;89w9dxkj\n\n\n\n\n\n83ayee0w\n\n\n\n\n83ayee0w-&gt;y0sap20o\n\n\n\n\n\n83ayee0w-&gt;bav7hkue\n\n\n\n\n\n83ayee0w-&gt;lijfw7my\n\n\n\n\n\ndxppp23s-&gt;glv43ych\n\n\n\n\n\ndxppp23s-&gt;m0ggeeei\n\n\n\n\n\ndogjcfze-&gt;1chkeldv\n\n\n\n\n\ndogjcfze-&gt;dtmww06j\n\n\n\n\n\n53bgisfb-&gt;666n359b\n\n\n\n\n\n53bgisfb-&gt;72a2d6xd\n\n\n\n\n\n53bgisfb-&gt;9ifdi52l\n\n\n\n\n\n53bgisfb-&gt;xztqibcz\n\n\n\n\n\nb463lw81\n\n\n\n\n53bgisfb-&gt;b463lw81\n\n\n\n\n\ne7injpqb-&gt;bav7hkue\n\n\n\n\n\ndvhx7p1e-&gt;5cymetka\n\n\n\n\n\ndvhx7p1e-&gt;8bx5ks88\n\n\n\n\n\npveq6y3l-&gt;4s3xzocl\n\n\n\n\n\npveq6y3l-&gt;pveq6y3l\n\n\n\n\n\nm0ggeeei-&gt;4s3xzocl\n\n\n\n\n\nm0ggeeei-&gt;qez5iiiz\n\n\n\n\n\nl9axxrot-&gt;pzodwhlw\n\n\n\n\n\nl9axxrot-&gt;akih91pi\n\n\n\n\n\nl9axxrot-&gt;c21k3g41\n\n\n\n\n\nc21k3g41-&gt;c1sdjw8j\n\n\n\n\n\nc21k3g41-&gt;ozdu79aw\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nc21k3g41-&gt;ieb7bdce\n\n\n\n\n\nb463lw81-&gt;hcqaqm1o\n\n\n\n\n\nb463lw81-&gt;wxhhelge\n\n\n\n\n\nce1516j0-&gt;4s3xzocl\n\n\n\n\n\nce1516j0-&gt;83ayee0w\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nce1516j0-&gt;ce1516j0\n\n\n\n\n\nw084j1ko-&gt;onkctu6x\n\n\n\n\n\nw084j1ko-&gt;dxppp23s\n\n\n\n\n\ns3tdzgmu-&gt;vr6qql5z\n\n\n\n\n\ns3tdzgmu-&gt;ez7k5sb5\n\n\n\n\n\ns3tdzgmu-&gt;b463lw81\n\n\n\n\n\n\n\n\n\n\nThis is a randomly generated graph for visual purposes only.\nSee Appendix for graph generator code",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "02-02b-expand.html",
    "href": "02-02b-expand.html",
    "title": "Early Insights and Future Expansion",
    "section": "",
    "text": "TODO\n\n\n\n\nadd cypher output results when reload data",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights and Model Expansion"
    ]
  },
  {
    "objectID": "02-02b-expand.html#unveiling-basic-patterns",
    "href": "02-02b-expand.html#unveiling-basic-patterns",
    "title": "Early Insights and Future Expansion",
    "section": "Unveiling Basic Patterns",
    "text": "Unveiling Basic Patterns\nEven with this basic model, we can easily extract valuable insights, for example:\n\nActivity Load: Identify staff with the highest number of teaching activities or total teaching hours.\nStudent Timetable Profiles: Calculate average hours per student or per programme to understand workload distribution.\nResource utilisation: Determine the busiest locations or times on campus based on activity scheduling.\nAnomaly detection: Identify students who have unexpected profiles or unusual combinations\n\n\nExample code\n\nBusiest location overall\nMATCH (r:Room)&lt;-[:TAKES_PLACE_IN]-(a:Activity)\nWITH r, sum(a.duration) AS totalDuration\nRETURN r.name AS Room, totalDuration\nORDER BY totalDuration DESC\nLIMIT 1\n\n\nBusiest location for a specific time\nMATCH (r:Room)&lt;-[:TAKES_PLACE_IN]-(a:Activity)\nWHERE a.startTime = \"11:00\" \nWITH r, count(a) AS activityCount\nRETURN r.name AS Room, activityCount\nORDER BY activityCount DESC\nLIMIT 1 \n\n\nStudents with below/above average hours\n// Calculate program averages and standard deviations\nMATCH (s:Student)-[:IS_ALLOCATED_TO]-&gt;(a:Activity)\nWITH s.prog AS programme, AVG(a.duration) AS avgDuration, STDEV(a.duration) AS stdDev\nGROUP BY programme\n\n// Identify students outside the 10% margin \nMATCH (s:Student)-[:IS_ALLOCATED_TO]-&gt;(a:Activity)\nWITH s.studentID AS studentID, s.prog AS program, SUM(a.duration) AS totalDuration\nMATCH (avgData)\nWHERE avgData.programme = programme\nWITH studentID, programme, totalDuration, avgData.avgDuration AS avgDuration, avgData.stdDev AS stdDev\nWHERE totalDuration &lt; avgDuration - (0.1 * avgDuration) OR totalDuration &gt; avgDuration + (0.1 * avgDuration)\nRETURN studentID, programme, totalDuration, avgDuration, stdDev \nORDER BY programme, totalDuration DESC",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights and Model Expansion"
    ]
  },
  {
    "objectID": "02-02b-expand.html#expanding-the-model-towards-richer-insights",
    "href": "02-02b-expand.html#expanding-the-model-towards-richer-insights",
    "title": "Early Insights and Future Expansion",
    "section": "Expanding the Model: Towards Richer Insights",
    "text": "Expanding the Model: Towards Richer Insights\nThe true power of the graph model lies in its extensibility. Introducing additional nodes and properties allows for a more comprehensive representation and enables more sophisticated analysis.\n\nPotential Expansions:\n\nOrganisational Units: Include departments, colleges, or schools to analyse timetabling within organisational structures.\nCurriculum Data: Incorporate modules and programmes to understand the interconnectedness of courses and student enrolment patterns.\nActivity Types: Differentiate between lectures, seminars, labs, etc., for a more granular analysis of teaching and learning activities.\nActivity Delivery: Understand teaching delivery (virtual, in-person, hybrid, drop-in).\nStudent Attributes: Add properties like ‚Äúinternational student‚Äù or ‚Äúfirst-year student‚Äù to explore potential student clusters.\n\n\n\n\nExample of Expanded Timetable Graph Model",
    "crumbs": [
      "Home",
      "Graph Data Model",
      "Early Insights and Model Expansion"
    ]
  }
]